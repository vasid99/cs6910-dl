{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Assignment02.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CANQJlDqn4i"
      },
      "source": [
        "# 1. Imports and Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ddKj89qn5i"
      },
      "source": [
        "## Library Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUPL8q9Hqn5w"
      },
      "source": [
        "!pip install -q wandb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fccMM4mPqn57"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slDPyWggqn6D"
      },
      "source": [
        "# generic libs\n",
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# ML-specific libs\n",
        "import tensorflow as tf\n",
        "ks = tf.keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# WandB\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback as WandbCallback"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8jf3VVhqn6I"
      },
      "source": [
        "## mount Google Drive (for iNaturalist 12K dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bScHwPcXqn6P"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-p-cWqFqn6S"
      },
      "source": [
        "# 2. Harness Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b51er49Mqn6W"
      },
      "source": [
        "# initialize model from JSON-style config object\n",
        "def initModel(config):\n",
        "  # create layer list\n",
        "  layers = []\n",
        "\n",
        "  for lconf in config[\"layers\"]:\n",
        "    ltype = lconf[\"type\"]\n",
        "    if   ltype==\"input\":\n",
        "      layers.append(ks.Input( \n",
        "        shape = lconf[\"size\"]\n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"conv\":\n",
        "      layers.append( ks.layers.Conv2D( \n",
        "        lconf[\"filter_num\"],\n",
        "        lconf[\"filter_size\"], \n",
        "        strides = lconf[\"stride\"],\n",
        "        activation = lconf[\"activation\"] \n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"pool\":\n",
        "      layers.append( ks.layers.MaxPooling2D( \n",
        "        pool_size=lconf[\"size\"] \n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"fc\":\n",
        "      layers.append( ks.layers.Dense( \n",
        "        lconf[\"size\"], \n",
        "        activation = lconf[\"activation\"]\n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"flatten\":\n",
        "      layers.append( ks.layers.Flatten() )\n",
        "    \n",
        "    elif ltype==\"dropout\":\n",
        "      layers.append( ks.layers.Dropout(\n",
        "        lconf[\"fraction\"]\n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"batchnorm\":\n",
        "      if \"perform\" in lconf:\n",
        "        if lconf[\"perform\"]==True:\n",
        "          layers.append( ks.layers.BatchNormalization() )\n",
        "      else:\n",
        "        layers.append( ks.layers.BatchNormalization() )\n",
        "    \n",
        "    elif ltype==\"exec\":\n",
        "      exec(\"lexec=ks.layers.\"+lconf[\"expr\"])\n",
        "      layers.append(locals()[\"lexec\"])\n",
        "\n",
        "  model = ks.Sequential(layers)\n",
        "\n",
        "  # compile and return model for given hyperparameters\n",
        "  exec( \"optexec=ks.optimizers.\" + config[\"optimizer\"]) #* revise based on final storage pattern used\n",
        "  opt = locals()[\"optexec\"]\n",
        "  model.compile( optimizer = opt, loss = config[\"loss\"], metrics = [\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "def trainingRun(config, **kwargs):\n",
        "  model = initModel(config)\n",
        "  tp = config[\"trainparams\"]\n",
        "\n",
        "  # set up WandB callback to train function\n",
        "  callbacks=[]\n",
        "  \n",
        "  try:\n",
        "    if config[\"wandb\"]:\n",
        "      callbacks.append(\n",
        "        WandbCallback(\n",
        "          monitor=\"val_loss\"\n",
        "        )\n",
        "      )\n",
        "    else: print(\"Warning: not using WandB\")\n",
        "  except KeyError: print(\"Warning: not using WandB\")\n",
        "  \n",
        "  # run training loop for given number of epochs\n",
        "  if all([_ in kwargs for _ in [\"xdata\",\"ydata\"]]):\n",
        "    model.fit(kwargs[\"xdata\"], kwargs[\"ydata\"], epochs = tp[\"epochs\"], batch_size = tp[\"batch_size\"], validation_split = tp[\"val_split\"], callbacks = callbacks)\n",
        "  elif all([_ in kwargs for _ in [\"train_data\",\"val_data\"]]):\n",
        "    model.fit(kwargs[\"train_data\"], validation_data = kwargs[\"val_data\"], epochs = tp[\"epochs\"], batch_size = tp[\"batch_size\"], callbacks = callbacks)\n",
        "  else: raise AssertionError(\"improper arguments given\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dwa3ZeQqn6e"
      },
      "source": [
        "# 3. Dataset Initialization and Reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWMEKNgFqn6j"
      },
      "source": [
        "## MNIST digit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2V1kPgFqn6r"
      },
      "source": [
        "# import MNIST dataset\n",
        "((x_train, y_train), (x_test, y_test)) = ks.datasets.mnist.load_data()\n",
        "\n",
        "# pad with zeros and convert to 32x32 LeNet-5 input\n",
        "x_train = np.pad(x_train,((0,0),(2,2),(2,2))).reshape((len(x_train),32,32,1))\n",
        "x_test = np.pad(x_test,((0,0),(2,2),(2,2))).reshape((len(x_test),32,32,1))\n",
        "\n",
        "# convert to float and normalize\n",
        "x_train = x_train.astype('float32')/255.0\n",
        "x_test = x_test.astype('float32')/255.0\n",
        "\n",
        "# one hot encode target values\n",
        "y_train = ks.utils.to_categorical(y_train)\n",
        "y_test = ks.utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhlFmZwYqn6s"
      },
      "source": [
        "## iNaturalist 12K dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhqgypppqn6u"
      },
      "source": [
        "inaturalist_train_root = \"/content/drive/MyDrive/inaturalist_12K/train\"\n",
        "\n",
        "'''labels = [_.split(\"/\")[-1] for _ in glob.glob(inaturalist_root + \"/*\")]\n",
        "def oneHotLabels(i,l):\n",
        "  ohl = tf.one_hot(l,len(labels))\n",
        "  print(ohl)\n",
        "  return i, tf.cast(ohl,tf.float32)'''\n",
        "\n",
        "def augmentImage(image, label):\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  image = tf.image.random_flip_up_down(image)\n",
        "  return image, label\n",
        "\n",
        "def I12kDatasets(config):\n",
        "  img_dims = (800,800,3)\n",
        "  numLabels = config[\"layers\"][-1][\"size\"]\n",
        "\n",
        "  train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    inaturalist_train_root,\n",
        "    validation_split=config[\"trainparams\"][\"val_split\"],\n",
        "    subset=\"training\",\n",
        "    label_mode=\"categorical\",\n",
        "    seed=123,\n",
        "    image_size=img_dims[:2],\n",
        "    batch_size=config[\"trainparams\"][\"batch_size\"])\n",
        "\n",
        "  val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    inaturalist_train_root,\n",
        "    validation_split=config[\"trainparams\"][\"val_split\"],\n",
        "    subset=\"validation\",\n",
        "    label_mode=\"categorical\",\n",
        "    seed=123,\n",
        "    image_size=img_dims[:2],\n",
        "    batch_size=config[\"trainparams\"][\"batch_size\"])\n",
        "\n",
        "  if \"dsAugment\" in config[\"trainparams\"] and config[\"trainparams\"][\"dsAugment\"]==True:\n",
        "    train_ds = train_ds.map(augmentImage,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    val_ds   = val_ds.map(augmentImage,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "  train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  val_ds   = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  \n",
        "  numLabels = None\n",
        "  return train_ds, val_ds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdxsWgWWqn6z"
      },
      "source": [
        "# 4. Running A Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHMgsAOmqn62"
      },
      "source": [
        "## Run-helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N59czmsoqn63"
      },
      "source": [
        "# create config object from JSON file\n",
        "def configFromJSON(arg):\n",
        "  # input checks\n",
        "  argtype = type(arg).__name__\n",
        "  config = None\n",
        "  if   argtype==\"str\":\n",
        "    config = json.load(open(arg,'r')) #* add failsafe\n",
        "  elif argtype==\"TextIOWrapper\":\n",
        "    config = json.load(arg)\n",
        "  elif argtype==\"dict\":\n",
        "    config = arg\n",
        "  return config"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RywlctCOqn68"
      },
      "source": [
        "## Sample runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfDeRSc0qn6-"
      },
      "source": [
        "wandb.init(project=\"CNN MNIST Test Runs\")\n",
        "run_config = configFromJSON(\"lenet5.json\")\n",
        "trainingRun(run_config, xdata=x_train, ydata=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkTKrXahqn7B"
      },
      "source": [
        "# 5. Sweeps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpa44bH-qn7D"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kz2oj5Nqn7H"
      },
      "source": [
        "# create config object for WandB sweep (Assignment 2 architecture)\n",
        "def configForA2WandbSweep():\n",
        "  wcfg = wandb.config\n",
        "  return { \"layers\": [\n",
        "    {\n",
        "      \"type\": \"exec\",\n",
        "      \"expr\": \"experimental.preprocessing.Rescaling(1./255)\"\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_1_filter_num,\n",
        "      \"filter_size\": wcfg.layer_1_filter_size,\n",
        "      \"stride\": wcfg.layer_1_stride,\n",
        "      \"activation\": wcfg.layer_1_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"batchnorm\",\n",
        "      \"perform\": wcfg.bn_perform\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_1_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_2_filter_num,\n",
        "      \"filter_size\": wcfg.layer_2_filter_size,\n",
        "      \"stride\": wcfg.layer_2_stride,\n",
        "      \"activation\": wcfg.layer_2_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"batchnorm\",\n",
        "      \"perform\": wcfg.bn_perform\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_2_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_3_filter_num,\n",
        "      \"filter_size\": wcfg.layer_3_filter_size,\n",
        "      \"stride\": wcfg.layer_3_stride,\n",
        "      \"activation\": wcfg.layer_3_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"batchnorm\",\n",
        "      \"perform\": wcfg.bn_perform\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_3_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_4_filter_num,\n",
        "      \"filter_size\": wcfg.layer_4_filter_size,\n",
        "      \"stride\": wcfg.layer_4_stride,\n",
        "      \"activation\": wcfg.layer_4_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"batchnorm\",\n",
        "      \"perform\": wcfg.bn_perform\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_4_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_5_filter_num,\n",
        "      \"filter_size\": wcfg.layer_5_filter_size,\n",
        "      \"stride\": wcfg.layer_5_stride,\n",
        "      \"activation\": wcfg.layer_5_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"batchnorm\",\n",
        "      \"perform\": wcfg.bn_perform\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_5_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"flatten\"\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"dropout\",\n",
        "      \"fraction\": wcfg.layer_6_dropout\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"fc\",\n",
        "      \"size\": wcfg.layer_6_size,\n",
        "      \"activation\": wcfg.layer_6_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"dropout\",\n",
        "      \"fraction\": wcfg.layer_7_dropout\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"fc\",\n",
        "      \"size\": wcfg.layer_7_size,\n",
        "      \"activation\": wcfg.layer_7_activation\n",
        "    }\n",
        "    #* add dropout at strategic location\n",
        "    ],\n",
        "    \"optimizer\": wcfg.optimizer,\n",
        "    \"loss\": wcfg.loss,\n",
        "    \"trainparams\": {\n",
        "      \"epochs\": wcfg.epochs,\n",
        "      \"batch_size\": wcfg.batch_size,\n",
        "      \"val_split\": wcfg.val_split,\n",
        "    \"dsAugment\": wcfg.dsAugment\n",
        "    },\n",
        "    \"wandb\": True\n",
        "  }\n",
        "\n",
        "def runWandbSweep():\n",
        "  wandb.init()\n",
        "  # obtain config and base(i.e. un-augmented) datasets\n",
        "  run_config = configForA2WandbSweep()\n",
        "  train_dataset, val_dataset = I12kDatasets(run_config)\n",
        "  \n",
        "  # perform training run\n",
        "  trainingRun(run_config, train_data=train_dataset, val_data=val_dataset)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PaIjJru7V_9"
      },
      "source": [
        "r_config = configFromJSON(\"A2arch.json\")\n",
        "train_dataset, val_dataset = I12kDatasets(r_config)\n",
        "\n",
        "# perform training run\n",
        "trainingRun(r_config, train_data=train_dataset, val_data=val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Fg-Dbkqn7L"
      },
      "source": [
        "## Configure and run sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUdkRYrJqn7N"
      },
      "source": [
        "wandbSweepCfg = {\n",
        "  \"name\":\"iNaturalist Parameter Sweep\", \n",
        "  \"metric\":{\n",
        "    \"name\":\"val_loss\",\n",
        "    \"goal\":\"minimize\"\n",
        "  }, \n",
        "  \"method\": \"bayes\", \n",
        "  \"parameters\":{\n",
        "    # layer parameters\n",
        "    \"layer_1_filter_num\": { \"values\": [8,16,32] },\n",
        "    \"layer_1_filter_size\": { \"values\": [3,7] },\n",
        "    \"layer_1_stride\": { \"values\": [1] },\n",
        "    \"layer_1_activation\": { \"values\": [\"relu\"] },\n",
        "    #\"layer_1_bn_perform\": { \"values\": [False] },\n",
        "    \"layer_1_pool_size\": { \"values\": [2] },\n",
        "\n",
        "    \"layer_2_filter_num\": { \"values\": [8,16,32] },\n",
        "    \"layer_2_filter_size\": { \"values\": [3,7] },\n",
        "    \"layer_2_stride\": { \"values\": [1] },\n",
        "    \"layer_2_activation\": { \"values\": [\"relu\"] },\n",
        "    #\"layer_2_bn_perform\": { \"values\": [False] },\n",
        "    \"layer_2_pool_size\": { \"values\": [2] },\n",
        "\n",
        "    \"layer_3_filter_num\": { \"values\": [8,16,32] },\n",
        "    \"layer_3_filter_size\": { \"values\": [3,7] },\n",
        "    \"layer_3_stride\": { \"values\": [1] },\n",
        "    \"layer_3_activation\": { \"values\": [\"relu\"] },\n",
        "    #\"layer_3_bn_perform\": { \"values\": [False] },\n",
        "    \"layer_3_pool_size\": { \"values\": [2] },\n",
        "\n",
        "    \"layer_4_filter_num\": { \"values\": [8,16,32] },\n",
        "    \"layer_4_filter_size\": { \"values\": [3,7] },\n",
        "    \"layer_4_stride\": { \"values\": [1] },\n",
        "    \"layer_4_activation\": { \"values\": [\"relu\"] },\n",
        "    #\"layer_4_bn_perform\": { \"values\": [False] },\n",
        "    \"layer_4_pool_size\": { \"values\": [2] },\n",
        "\n",
        "    \"layer_5_filter_num\": { \"values\": [8,16,32] },\n",
        "    \"layer_5_filter_size\": { \"values\": [3,7] },\n",
        "    \"layer_5_stride\": { \"values\": [1] },\n",
        "    \"layer_5_activation\": { \"values\": [\"relu\"] },\n",
        "    #\"layer_5_bn_perform\": { \"values\": [False] },\n",
        "    \"layer_5_pool_size\": { \"values\": [2] },\n",
        "    \n",
        "    \"layer_6_dropout\": { \"values\": [0.2,0.3] },\n",
        "    \"layer_6_size\": { \"values\": [64, 96, 128] },\n",
        "    \"layer_6_activation\": { \"values\": [\"tanh\"] },\n",
        "\n",
        "    \"layer_7_dropout\": { \"values\": [0,0.2,0.3] },\n",
        "    \"layer_7_size\": { \"values\": [10] },\n",
        "    \"layer_7_activation\": { \"values\": [\"softmax\"] },\n",
        "\n",
        "    \"bn_perform\": { \"values\": [False, True] },\n",
        "\n",
        "    # optimizer and loss\n",
        "    \"optimizer\": { \"values\": [\"Adam()\"] },\n",
        "    \"loss\": { \"values\": [\"categorical_crossentropy\"] },\n",
        "\n",
        "    # training parameters\n",
        "    \"epochs\": { \"values\":[5] },\n",
        "    \"batch_size\": { \"values\":[32] },\n",
        "    \"val_split\": { \"values\": [0.1] },\n",
        "    \"dsAugment\": { \"values\": [False, True]}\n",
        "  }\n",
        "}\n",
        "\n",
        "sweepId = wandb.sweep(wandbSweepCfg)\n",
        "wandb.agent(sweepId, function = runWandbSweep)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
