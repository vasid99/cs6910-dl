{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Imports and Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Library Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q keras wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generic functions\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Keras imports\n",
        "from keras import (Input as ksInp, Sequential as ksSeq, layers as ksl, optimizers as kso, utils as ksu)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import mnist # dry runs for code checking\n",
        "\n",
        "# WandB imports\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback as WandbCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Harness Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create config object for WandB sweep (general architecture)\n",
        "def configForWandbSweep():\n",
        "  wcfg = wandb.config\n",
        "  config = { \"layers\": [{\n",
        "    \"type\": \"input\", \n",
        "    \"size\": wcfg.input_size\n",
        "  }] }\n",
        "\n",
        "  #i=1\n",
        "  while True:\n",
        "    try:\n",
        "      config[\"layers\"].append({})\n",
        "      # need string-based execution for general architecture\n",
        "      # proceeding with specific architecture suggested for A2\n",
        "    except KeyError: break\n",
        "\n",
        "# create config object for WandB sweep (Assignment 2 architecture)\n",
        "def configForA2WandbSweep():\n",
        "  wcfg = wandb.config\n",
        "  return { \"layers\": [\n",
        "    {\n",
        "      \"type\": \"input\", \n",
        "      \"size\": wcfg.input_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_1_filter_num,\n",
        "      \"filter_size\": wcfg.layer_1_filter_size,\n",
        "      \"activation\": wcfg.layer_1_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_1_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_2_filter_num,\n",
        "      \"filter_size\": wcfg.layer_2_filter_size,\n",
        "      \"activation\": wcfg.layer_2_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_2_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_3_filter_num,\n",
        "      \"filter_size\": wcfg.layer_3_filter_size,\n",
        "      \"activation\": wcfg.layer_3_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_3_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_4_filter_num,\n",
        "      \"filter_size\": wcfg.layer_4_filter_size,\n",
        "      \"activation\": wcfg.layer_4_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_4_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"conv\",\n",
        "      \"filter_num\": wcfg.layer_5_filter_num,\n",
        "      \"filter_size\": wcfg.layer_5_filter_size,\n",
        "      \"activation\": wcfg.layer_5_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"pool\",\n",
        "      \"size\": wcfg.layer_5_pool_size\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"flatten\"\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"fc\",\n",
        "      \"size\": wcfg.layer_6_size,\n",
        "      \"activation\": wcfg.layer_6_activation\n",
        "    },\n",
        "    {\n",
        "      \"type\": \"fc\",\n",
        "      \"size\": wcfg.layer_7_size,\n",
        "      \"activation\": wcfg.layer_7_activation\n",
        "    }\n",
        "    #* add dropout at strategic location\n",
        "    ],\n",
        "    \"opt\": wcfg.optimizer,\n",
        "    \"loss\": wcfg.loss,\n",
        "    \"trainparams\": {\n",
        "      \"epochs\": wcfg.epochs,\n",
        "      \"batch_size\": wcfg.btch_size,\n",
        "      \"val_split\": wcfg.val_split\n",
        "    },\n",
        "    \"wandb\": 1\n",
        "  }\n",
        "\n",
        "# create config object from JSON file\n",
        "def configFromJSON(arg):\n",
        "  # input checks\n",
        "  argtype = type(arg).__name__\n",
        "  config = None\n",
        "  if   argtype==\"str\":\n",
        "    config = json.load(open(arg,'r')) #* add failsafe\n",
        "  elif argtype==\"TextIOWrapper\":\n",
        "    config = json.load(arg)\n",
        "  elif argtype==\"dict\":\n",
        "    config = arg\n",
        "  return config\n",
        "\n",
        "# initialize model from JSON-style config object\n",
        "def initModel(config):\n",
        "  # create layer list\n",
        "  layers = []\n",
        "\n",
        "  for lconf in config[\"layers\"]:\n",
        "    ltype = lconf[\"type\"]\n",
        "    if   ltype==\"input\":\n",
        "      layers.append(ksInp( \n",
        "        shape = lconf[\"size\"]\n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"conv\":\n",
        "      layers.append( ksl.Conv2D( \n",
        "        lconf[\"filter_num\"],\n",
        "        lconf[\"filter_size\"], \n",
        "        activation = lconf[\"activation\"] \n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"pool\":\n",
        "      layers.append( ksl.MaxPooling2D( \n",
        "        pool_size=lconf[\"size\"] \n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"fc\":\n",
        "      layers.append( ksl.Dense( \n",
        "        lconf[\"size\"], \n",
        "        activation = lconf[\"activation\"]\n",
        "      ) )\n",
        "    \n",
        "    elif ltype==\"flatten\":\n",
        "      layers.append( ksl.Flatten() )\n",
        "    \n",
        "    elif ltype==\"dropout\":\n",
        "      layers.append( ksl.Dropout(\n",
        "        lconf[\"fraction\"]\n",
        "      ) )\n",
        "\n",
        "  model = ksSeq(layers)\n",
        "\n",
        "  # compile and return model for given hyperparameters\n",
        "  exec( \"optexec=kso.\" + config[\"optimizer\"]) #* revise based on final storage pattern used\n",
        "  opt = locals()[\"optexec\"]\n",
        "  model.compile( optimizer = opt, loss = config[\"loss\"], metrics = [\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "def trainingRun(config, xdata=xdata, ydata=ydata, train_data=train_data, val_data=val_data, input_mode=1):\n",
        "  model = initModel(config)\n",
        "  tp = config[\"trainparams\"]\n",
        "\n",
        "  # set up WandB callback to train function\n",
        "  callbacks=[]\n",
        "  \n",
        "  try:\n",
        "    if config[\"wandb\"]:\n",
        "      callbacks.append(\n",
        "        WandbCallback(\n",
        "          monitor=\"val_loss\" #* add generator for dataset-aug. and batch normalization\n",
        "        )\n",
        "      )\n",
        "    else: print(\"Warning: not using WandB\")\n",
        "  except KeyError: print(\"Warning: not using WandB\")\n",
        "  \n",
        "  # run training loop for given number of epochs\n",
        "  if input_mode == 1:\n",
        "    model.fit(xdata, ydata, batch_size = tp[\"batch_size\"], epochs = tp[\"epochs\"], validation_split = tp[\"val_split\"], callbacks = callbacks)\n",
        "  elif input_mode == 2:\n",
        "    model.fit(train_data, epochs = tp[\"epochs\"], val_data, callbacks = callbacks)\n",
        "\n",
        "\n",
        "def get_train_val_data(path, config):\n",
        "  tp = config[\"trainparams\"]\n",
        "\n",
        "  data_generator = ImageDataGenerator(rescale = 1/255, horizontal_flip = True, validation_split = tp[\"val_split\"])\n",
        "\n",
        "  train_dataset = data_generator.flow_from_directory(\n",
        "    path,\n",
        "    target_size = (800, 800),\n",
        "    batch_size = tp[\"batch_size\"],\n",
        "    subset = \"training\"\n",
        "  )\n",
        "\n",
        "  val_dataset = data_generator.flow_from_directory(\n",
        "    path,\n",
        "    target_size = (800, 800),\n",
        "    batch_size = tp[\"batch_size\"],\n",
        "    subset = \"validation\"\n",
        "  )\n",
        "\n",
        "  return train_dataset, val_dataset\n",
        "\n",
        "def runWandbSweep():\n",
        "  run_config = configForA2WandbSweep()\n",
        "  train_dataset, val_dataset = get_train_val_data(train_path, run_config)\n",
        "  trainingRun(run_config, train_data=train_dataset, val_data=val_dataset, input_mode=2)\n",
        "#* add data flattening/reshaping functions\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Dataset Initialization and Reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST digit dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import MNIST dataset\n",
        "((x_train, y_train), (x_test, y_test)) = mnist.load_data()\n",
        "\n",
        "# pad with zeros and convert to 32x32 LeNet-5 input\n",
        "x_train = np.pad(x_train,((0,0),(2,2),(2,2))).reshape((len(x_train),32,32,1))\n",
        "x_test = np.pad(x_test,((0,0),(2,2),(2,2))).reshape((len(x_test),32,32,1))\n",
        "\n",
        "# convert to float and normalize\n",
        "x_train = x_train.astype('float32')/255.0\n",
        "x_test = x_test.astype('float32')/255.0\n",
        "\n",
        "# one hot encode target values\n",
        "y_train = ksu.to_categorical(y_train)\n",
        "y_test = ksu.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Running A Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.init(project=\"CNN MNIST Test Runs\")\n",
        "run_config = configFromJSON(\"lenet5.json\")\n",
        "trainingRun(run_config, xdata=x_train, ydata=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandbSweepCfg = {\n",
        "  \"name\":\"iNaturalist Parameter Sweep\", \n",
        "  \"metric\":{\n",
        "    \"name\":\"val_loss\",\n",
        "    \"goal\":\"minimize\"\n",
        "  }, \n",
        "  \"method\": \"bayes\", \n",
        "  \"parameters\":{\n",
        "    # input parameters\n",
        "    \"input_size\": { \"values\" : [[217, 217]] },\n",
        "\n",
        "    # layer parameters\n",
        "    \"layer_1_filter_num\": { \"values\": [5,10,20] },\n",
        "    \"layer_1_filter_size\": { \"values\": [3,5,7] },\n",
        "    \"layer_1_activation\": { \"values\": [\"relu\"] },\n",
        "    \"layer_1_pool_size\": { \"values\": [2,3,4] },\n",
        "\n",
        "    \"layer_2_filter_num\": { \"values\": [5,10,20] },\n",
        "    \"layer_2_filter_size\": { \"values\": [3,5,7] },\n",
        "    \"layer_2_activation\": { \"values\": [\"relu\"] },\n",
        "    \"layer_2_pool_size\": { \"values\": [2,3,4] },\n",
        "\n",
        "    \"layer_3_filter_num\": { \"values\": [5,10,20] },\n",
        "    \"layer_3_filter_size\": { \"values\": [3,5,7] },\n",
        "    \"layer_3_activation\": { \"values\": [\"relu\"] },\n",
        "    \"layer_3_pool_size\": { \"values\": [2,3,4] },\n",
        "\n",
        "    \"layer_4_filter_num\": { \"values\": [5,10,20] },\n",
        "    \"layer_4_filter_size\": { \"values\": [3,5,7] },\n",
        "    \"layer_4_activation\": { \"values\": [\"relu\"] },\n",
        "    \"layer_4_pool_size\": { \"values\": [2,3,4] },\n",
        "\n",
        "    \"layer_5_filter_num\": { \"values\": [5,10,20] },\n",
        "    \"layer_5_filter_size\": { \"values\": [3,5,7] },\n",
        "    \"layer_5_activation\": { \"values\": [\"relu\"] },\n",
        "    \"layer_5_pool_size\": { \"values\": [2,3,4] },\n",
        "\n",
        "    \"layer_6_size\": { \"values\": [256, 512, 1024] },\n",
        "    \"layer_6_activation\": { \"values\": [\"tanh\"] },\n",
        "\n",
        "    \"layer_7_size\": { \"values\": [10] },\n",
        "    \"layer_7_activation\": { \"values\": [\"softmax\"] },\n",
        "\n",
        "    # optimizer and loss\n",
        "    \"optimizer\": { \"values\": [\"SGD(lr=0.01, momentum=0.9)\"] },\n",
        "    \"loss\": { \"values\": [\"categorical_crossentropy\"] },\n",
        "\n",
        "    # training parameters\n",
        "    \"epochs\": { \"values\":[25] },\n",
        "    \"batch_size\": { \"values\":[32, 64] },\n",
        "    \"val_split\": { \"values\": [0.1] },\n",
        "  }\n",
        "}\n",
        "\n",
        "sweepId = wandb.sweep(wandbSweepCfg)\n",
        "wandb.agent(sweepId, function = runWandbSweep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using inaturalist_12k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_path = \"/content/drive/MyDrive/Sem 8/DL/inaturalist_12K/train\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}