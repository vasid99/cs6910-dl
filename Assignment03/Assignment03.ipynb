{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_assgn_3_Srijan_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PATZzAYvjFKg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "ks = tf.keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi5rn-e7m25n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3fjeYakm8cl"
      },
      "source": [
        "basepath = \"/content/drive/MyDrive/Sem_8/dl-datasets/dakshina_dataset_v1.0/hi/lexicons\"\n",
        "\n",
        "col_names = ['Dev.','Roman','att.']\n",
        "STARTCHAR = '\\t'\n",
        "ENDCHAR   = '\\n'\n",
        "\n",
        "def read_as_array(path):\n",
        "  data = pd.read_csv(path, sep='\\t', names=col_names).drop_duplicates(subset=\"Dev.\").dropna()\n",
        "  data['Dev.'] = STARTCHAR + data['Dev.'] + ENDCHAR\n",
        "  return np.array(data)[:,:2]\n",
        "\n",
        "train_data = read_as_array(basepath+\"/hi.translit.sampled.train.tsv\")\n",
        "val_data   = read_as_array(basepath+\"/hi.translit.sampled.dev.tsv\")\n",
        "test_data  = read_as_array(basepath+\"/hi.translit.sampled.test.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBFXE2gbv9tw"
      },
      "source": [
        "input_vocab = set()\n",
        "target_vocab = set()\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  for char in train_data[i,1]:\n",
        "    input_vocab.add(char)\n",
        "  for char in train_data[i,0]:\n",
        "    target_vocab.add(char)\n",
        "\n",
        "input_vocab  = [''] + sorted(list(input_vocab))\n",
        "target_vocab = [''] + sorted(list(target_vocab))\n",
        "\n",
        "len_input_vocab  = len(input_vocab)\n",
        "len_target_vocab = len(target_vocab)\n",
        "\n",
        "input_dict  = dict([ (char, i) for i, char in enumerate(input_vocab)])\n",
        "target_dict = dict([ (char, i) for i, char in enumerate(target_vocab)])\n",
        "\n",
        "max_len_input  = max([ len(word) for data in [train_data[:,1], val_data[:,1], test_data[:,1]] for word in data ])\n",
        "max_len_target = max([ len(word) for data in [train_data[:,0], val_data[:,0], test_data[:,0]] for word in data ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsHJ1lWzQhWZ"
      },
      "source": [
        "input_dict_id_to_ch  = dict([(value, key) for key, value in input_dict.items()])\n",
        "target_dict_id_to_ch = dict([(value, key) for key, value in target_dict.items()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mi7OBsT8hDA"
      },
      "source": [
        "def encode_char_to_num(data):\n",
        "  enc_inp = np.zeros((data.shape[0], max_len_input), dtype=\"float32\")\n",
        "  dec_inp = np.zeros((data.shape[0], max_len_target), dtype=\"float32\")\n",
        "  dec_tgt = np.zeros((data.shape[0], max_len_target), dtype=\"float32\")\n",
        "  dec_tgt_onehot = np.zeros((data.shape[0], max_len_target, len_target_vocab), dtype=\"float32\")\n",
        "\n",
        "  for i, (target_word, input_word) in enumerate(data):\n",
        "    for j, ch in enumerate(input_word):  enc_inp[i,j] = input_dict[ch]\n",
        "    for j, ch in enumerate(target_word): dec_inp[i,j] = target_dict[ch]\n",
        "  dec_tgt[:,:-1] = dec_inp[:,1:]\n",
        "  for i in range(len_target_vocab): dec_tgt_onehot[:,:,i] = dec_tgt[:,:]==i\n",
        "\n",
        "  return enc_inp, dec_inp, dec_tgt_onehot\n",
        "\n",
        "enc_in_train_idxd, dec_in_train_idxd, dec_targ_train_idxd = encode_char_to_num(train_data)\n",
        "enc_in_val_idxd, dec_in_val_idxd, dec_targ_val_idxd       = encode_char_to_num(val_data)\n",
        "enc_in_test_idxd, dec_in_test_idxd, dec_targ_test_idxd    = encode_char_to_num(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjhsuXPfDS4m"
      },
      "source": [
        "epochs = 100\n",
        "latent_dim = 256\n",
        "batch_size = 32\n",
        "embed_size = 16\n",
        "\n",
        "#Building the model\n",
        "\n",
        "# Encoder\n",
        "encoder_input = ks.Input(shape=(1,),dtype='string')\n",
        "vectorization_layer = ks.experimental.preprocessing.TextVectorization()\n",
        "encoder_embedded = ks.layers.Embedding(len_input_vocab+1, embed_size, mask_zero=True)(encoder_input)\n",
        "encoder_output, state_h, state_c = ks.layers.LSTM(latent_dim, return_state=True)(encoder_embedded)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_input = ks.Input(shape=(None,))\n",
        "decoder_embedded = ks.layers.Embedding(len_target_vocab+1, embed_size, mask_zero=True)(decoder_input)\n",
        "decoder_lstm, _, _ = ks.layers.LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
        "    decoder_embedded, initial_state=encoder_states\n",
        ")\n",
        "decoder_output = ks.layers.Dense(len_target_vocab, activation=\"softmax\")(decoder_lstm)\n",
        "\n",
        "# The Model\n",
        "model = ks.Model([encoder_input, decoder_input], decoder_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyJ49BAKXDQu"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET4ji9GkYz0W"
      },
      "source": [
        "#Training the model\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    [enc_in_train_idxd, dec_in_train_idxd],\n",
        "    dec_targ_train_idxd,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=( [enc_in_val_idxd, dec_in_val_idxd], dec_targ_val_idxd )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skFP9OyVWPQ"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/Sem_8/s2s.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hgSfEIsfoTd"
      },
      "source": [
        "print(dec_in_train_idxd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEVq5fQQWb0w"
      },
      "source": [
        "## add text-vectorization layer\n",
        "inputs = np.array(['bharat','mata','ki','jay','ho'])\n",
        "STARTCHAR_intencoded = np.array([target_dict['\\t']==_ for _ in range(len_target_vocab)], ndmin=2, dtype='float32')\n",
        "\n",
        "for i, input_word in enumerate(inputs):\n",
        "  input_numencoded = np.zeros((1, max_len_input), dtype=\"float32\")\n",
        "  for j, ch in enumerate(input_word):  input_numencoded[0,j] = input_dict[ch]\n",
        "\n",
        "  inputs_embedded = model.layers[2](input_numencoded)\n",
        "  encoder_out, encoder_h, encoder_c = model.layers[4](inputs_embedded)\n",
        "\n",
        "  first_iteration = True\n",
        "  while ch!=ENDCHAR:\n",
        "    target_char_embedded = model.layers[3](STARTCHAR_intencoded if first_iteration else decoder_rnn_out)\n",
        "    decoder_rnn_out, decoder_s, decoder_c = model.layers[5](target_char_embedded, initial_state=[encoder_h, encoder_c] if first_iteration else [decoder_s, decoder_c])\n",
        "    decoder_out = model.layers[6](decoder_rnn_out)\n",
        "    print(np.argmax(decoder_out.numpy()))\n",
        "    ch = target_vocab[np.argmax(decoder_out.numpy())]\n",
        "    print(ch)\n",
        "    if first_iteration: first_iteration = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PovOQRfw-j_"
      },
      "source": [
        "# Adding Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWEqYeuwxDBI"
      },
      "source": [
        "epochs = 100\n",
        "latent_dim = 256\n",
        "batch_size = 32\n",
        "embed_size = 16\n",
        "\n",
        "#Building the model\n",
        "\n",
        "# Encoder\n",
        "encoder_input = ks.Input(shape=(max_len_input,))\n",
        "encoder_embedded = ks.layers.Embedding(len_input_vocab+1, embed_size, mask_zero=True)(encoder_input)\n",
        "encoder_output, state_h, state_c = ks.layers.LSTM(latent_dim, return_sequences=True, return_state=True)(encoder_embedded)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_input = ks.Input(shape=(max_len_target,))\n",
        "decoder_embedded = ks.layers.Embedding(len_target_vocab+1, embed_size, mask_zero=True)(decoder_input)\n",
        "decoder_lstm, _, _ = ks.layers.LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
        "    decoder_embedded, initial_state=encoder_states\n",
        ")\n",
        "\n",
        "#Attention\n",
        "context_vec, attntn_weights = ks.layers.AdditiveAttention()([decoder_lstm, encoder_output], return_attention_scores=True)\n",
        "decoder_concat_input = ks.layers.Concatenate()([decoder_lstm, context_vec])\n",
        "\n",
        "#Final Dense layer\n",
        "decoder_output = ks.layers.Dense(len_target_vocab, activation=\"softmax\")(decoder_concat_input)\n",
        "\n",
        "# The Model\n",
        "model = ks.Model([encoder_input, decoder_input], decoder_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igooBnLjEFaU"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emg6egUqE_-M"
      },
      "source": [
        "#Training the model\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    [enc_in_train_idxd, dec_in_train_idxd],\n",
        "    dec_targ_train_idxd,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=( [enc_in_val_idxd, dec_in_val_idxd], dec_targ_val_idxd )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0zcX07rFPU2"
      },
      "source": [
        "pred_onehot = model.predict([tf.expand_dims(enc_in_test_idxd[0], axis=0),\n",
        "                             tf.expand_dims(dec_in_test_idxd[0], axis=0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3h-7VhvPhYU"
      },
      "source": [
        "pred = ''.join([ target_dict_id_to_ch[id] for id in np.argmax(pred_onehot, axis=2)[0]])\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmSAT9ndRxU1"
      },
      "source": [
        "test_data[0,0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}