{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_assgn_3_Srijan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HT0-WpHe_XRxpXqZmq_QsC_HtU_y3FbR",
      "authorship_tag": "ABX9TyPCjKaomrOj7QScTPtS6hX2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PATZzAYvjFKg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "ks = tf.keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi5rn-e7m25n",
        "outputId": "1b5a909d-197d-4794-c32b-663f0f76b9f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3fjeYakm8cl"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Sem_8/dl-datasets/dakshina_dataset_v1.0/hi/lexicons\"\n",
        "\n",
        "col_names = ['Dev.','Roman','att.']\n",
        "train_data = np.array(pd.read_csv(path+\"/hi.translit.sampled.train.tsv\", sep='\\t', names=col_names).drop_duplicates(subset=\"Dev.\").dropna())[:,:2]\n",
        "val_data = np.array(pd.read_csv(path+\"/hi.translit.sampled.dev.tsv\", sep='\\t', names=col_names).drop_duplicates(subset=\"Dev.\").dropna())[:,:2]\n",
        "test_data = np.array(pd.read_csv(path+\"/hi.translit.sampled.test.tsv\", sep='\\t', names=col_names).drop_duplicates(subset=\"Dev.\").dropna())[:,:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBFXE2gbv9tw"
      },
      "source": [
        "input_vocab = set();\n",
        "target_vocab = set();\n",
        "\n",
        "for i in range(train_data.shape[0]):\n",
        "    for char in train_data[i,0]:\n",
        "        if char not in input_vocab:\n",
        "            input_vocab.add(char)\n",
        "    for char in train_data[i,1]:\n",
        "        if char not in target_vocab:\n",
        "            target_vocab.add(char)\n",
        "\n",
        "input_vocab = sorted(list(input_vocab))\n",
        "target_vocab = sorted(list(target_vocab))\n",
        "\n",
        "len_input_vocab = len(input_vocab)\n",
        "len_target_vocab = len(target_vocab)\n",
        "\n",
        "input_dict = dict([ (char, i) for i, char in enumerate(input_vocab)])\n",
        "target_dict = dict([ (char, i) for i, char in enumerate(target_vocab)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHVAm_kE70Rq"
      },
      "source": [
        "X_train_idd = tf.ragged.constant([ [ input_dict[ch] for ch in word ] for word in train_data[:,0] ])\n",
        "Y_train_idd = tf.ragged.constant([ [ target_dict[ch] for ch in word ] for word in train_data[:,1] ])\n",
        "\n",
        "X_val_idd = tf.ragged.constant([ [ input_dict[ch] for ch in word ] for word in val_data[:,0] ])\n",
        "Y_val_idd = tf.ragged.constant([ [ target_dict[ch] for ch in word ] for word in val_data[:,1] ])\n",
        "\n",
        "X_test_idd = tf.ragged.constant([ [ input_dict[ch] for ch in word ] for word in test_data[:,0] ])\n",
        "Y_test_idd = tf.ragged.constant([ [ target_dict[ch] for ch in word ] for word in test_data[:,1] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyltq2xH_ucC"
      },
      "source": [
        "batch_id = 0\n",
        "batch_size = 32\n",
        "embed_size = 16\n",
        "\n",
        "model_embed_input = ks.Sequential()\n",
        "model_embed_input.add(ks.layers.Embedding(len_input_vocab, embed_size))\n",
        "model_embed_input.compile('rmsprop','mse')\n",
        "\n",
        "X_train_embed = model_embed_input.predict( X_train_idd[ batch_id*batch_size : (batch_id+1)*batch_size ] )\n",
        "\n",
        "model_embed_target = ks.Sequential()\n",
        "model_embed_target.add(ks.layers.Embedding(len_target_vocab, embed_size))\n",
        "model_embed_target.compile('rmsprop','mse')\n",
        "\n",
        "Y_train_embed = model_embed_target.predict( Y_train_idd[ batch_id*batch_size : (batch_id+1)*batch_size ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjhsuXPfDS4m"
      },
      "source": [
        "epochs = 100\n",
        "latent_dim = 256\n",
        "\n",
        "#Building the model\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = ks.Input(shape=(None, embed_size))\n",
        "encoder = ks.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = ks.Input(shape=(None, embed_size))\n",
        "decoder_lstm = ks.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = ks.layers.Dense(embed_size, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# The Model\n",
        "model = ks.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET4ji9GkYz0W"
      },
      "source": [
        "#Training the model\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}