{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment03_Krish.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3DhFtI8H5AIC",
        "3-NFLIXk51NL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0idENJUy464K"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOuJdDvYDbv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea31db2-e209-430c-8801-c337dc22df1b"
      },
      "source": [
        "!pip install -q wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 23.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 48.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 44.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.7MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PATZzAYvjFKg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "ks = tf.keras\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback as WandbCallback\n",
        "\n",
        "import datetime\n",
        "timestr = lambda fmt:datetime.datetime.now().strftime(fmt)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi5rn-e7m25n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68513533-cb15-467e-909c-3c22e5f5f699"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DhFtI8H5AIC"
      },
      "source": [
        "# Dataset Loading, Characteristic Extraction and Reshaping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3fjeYakm8cl"
      },
      "source": [
        "basepath = \"/content/drive/MyDrive/Sem_8/dl-datasets/dakshina_dataset_v1.0/hi/lexicons\"\n",
        "\n",
        "col_names = ['Dev.','Roman','att.']\n",
        "STARTCHAR = '\\t'\n",
        "ENDCHAR   = '\\n'\n",
        "\n",
        "def read_as_array(path):\n",
        "  data = pd.read_csv(path, sep='\\t', names=col_names).drop_duplicates(subset=\"Dev.\").dropna()\n",
        "  data['Dev.'] = STARTCHAR + data['Dev.'] + ENDCHAR\n",
        "  return np.array(data)[:,:2]\n",
        "\n",
        "train_data = read_as_array(basepath+\"/hi.translit.sampled.train.tsv\")\n",
        "val_data   = read_as_array(basepath+\"/hi.translit.sampled.dev.tsv\")\n",
        "test_data  = read_as_array(basepath+\"/hi.translit.sampled.test.tsv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBFXE2gbv9tw"
      },
      "source": [
        "input_vocab = set()\n",
        "target_vocab = set()\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  for char in train_data[i,1]:\n",
        "    input_vocab.add(char)\n",
        "  for char in train_data[i,0]:\n",
        "    target_vocab.add(char)\n",
        "\n",
        "input_vocab  = [''] + sorted(list(input_vocab))\n",
        "target_vocab = [''] + sorted(list(target_vocab))\n",
        "\n",
        "len_input_vocab  = len(input_vocab)\n",
        "len_target_vocab = len(target_vocab)\n",
        "\n",
        "input_dict  = dict([ (char, i) for i, char in enumerate(input_vocab)])\n",
        "target_dict = dict([ (char, i) for i, char in enumerate(target_vocab)])\n",
        "\n",
        "max_len_input  = max([ len(word) for data in [train_data[:,1], val_data[:,1], test_data[:,1]] for word in data ])\n",
        "max_len_target = max([ len(word) for data in [train_data[:,0], val_data[:,0], test_data[:,0]] for word in data ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mi7OBsT8hDA"
      },
      "source": [
        "def str_to_numarray(strs, output_size, charmap):\n",
        "  ret = np.zeros((len(strs), output_size), dtype=\"float32\")\n",
        "  for i, s in enumerate(strs):\n",
        "    for j, ch in enumerate(s): ret[i,j] = charmap[ch]\n",
        "  return ret\n",
        "\n",
        "def vectorize_dataset(data):\n",
        "  # inputs\n",
        "  enc_inp = str_to_numarray(data[:,1], max_len_input, input_dict)\n",
        "  dec_inp = str_to_numarray(data[:,0], max_len_target, target_dict)\n",
        "  \n",
        "  # targets\n",
        "  dec_tgt = np.pad(dec_inp[:,1:],((0,0),(0,1)))\n",
        "  dec_tgt_onehot = np.zeros((data.shape[0], max_len_target, len_target_vocab), dtype=\"float32\")\n",
        "  for i in range(len_target_vocab): dec_tgt_onehot[:,:,i] = dec_tgt[:,:]==i\n",
        "  \n",
        "  return enc_inp, dec_inp, dec_tgt_onehot\n",
        "\n",
        "enc_inp_train, dec_inp_train, dec_tgt_train_onehot = vectorize_dataset(train_data)\n",
        "enc_inp_val,   dec_inp_val,   dec_tgt_val_onehot   = vectorize_dataset(val_data)\n",
        "enc_inp_test,  dec_inp_test,  dec_tgt_test_onehot  = vectorize_dataset(test_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-NFLIXk51NL"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYGzxCwa5asf"
      },
      "source": [
        "## Model Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKGB3v7LqKn0"
      },
      "source": [
        "DEFAULT_NETPARAMS = {\n",
        "    \"embed_size\": 16, \n",
        "    \"latent_dim\": 256, \n",
        "    \"enc_layers\": 1, \n",
        "    \"dec_layers\": 1, \n",
        "    \"recurrent_cell\": \"SimpleRNN\",\n",
        "    \"dec_attention\": False,\n",
        "    \"dropout\": 0, \n",
        "    \"beam_size\": 1, \n",
        "    \"enc_state_dep\": \"first\"\n",
        "}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjhsuXPfDS4m"
      },
      "source": [
        "def fresh_training_model(netparams):\n",
        "  # unpack network parameters\n",
        "  embed_size = netparams[\"embed_size\"]\n",
        "  latent_dim = netparams[\"latent_dim\"]\n",
        "  enc_layers = netparams[\"enc_layers\"]\n",
        "  dec_layers = netparams[\"dec_layers\"]\n",
        "  exec(\"rlexec=ks.layers.\"+netparams[\"recurrent_cell\"]); recurrent_cell = locals()[\"rlexec\"]\n",
        "  dec_attention = netparams[\"dec_attention\"]\n",
        "  dropout    = netparams[\"dropout\"]\n",
        "  ## add beam search\n",
        "  enc_state_dep = netparams[\"enc_state_dep\"]\n",
        "\n",
        "  # encoder layers\n",
        "  encoder_input = ks.Input(shape=(None,), name=\"encoder_input\")\n",
        "  encoder_embedding = ks.layers.Embedding(len_input_vocab+1, embed_size, mask_zero=True, name=\"encoder_embedding\")\n",
        "  encoder_rnns = [recurrent_cell(latent_dim, return_sequences=True, return_state=True, dropout=dropout, name=\"encoder_rnn_\"+str(i)) for i in range(1,enc_layers+1)]\n",
        "\n",
        "  # encoder feedforward path\n",
        "  encoder_output = encoder_embedding(encoder_input)\n",
        "  for encoder_rnn in encoder_rnns:\n",
        "    encoder_ret = encoder_rnn(encoder_output)\n",
        "    encoder_output, encoder_state = encoder_ret[0], list(encoder_ret[1:])\n",
        "\n",
        "  # decoder layers\n",
        "  decoder_input = ks.Input(shape=(None,), name=\"decoder_input\")\n",
        "  decoder_embedding = ks.layers.Embedding(len_target_vocab+1, embed_size, mask_zero=True, name=\"decoder_embedding\")\n",
        "  decoder_rnns = [recurrent_cell(latent_dim, return_sequences=True, return_state=True, dropout=dropout, name=\"decoder_rnn_\"+str(i)) for i in range(1,dec_layers+1)]\n",
        "  if dec_attention:\n",
        "    decoder_attention = ks.layers.AdditiveAttention(name=\"decoder_attention\")\n",
        "    decoder_concat    = ks.layers.Concatenate(name=\"decoder_concat\")\n",
        "  decoder_dropout = ks.layers.Dropout(dropout, name=\"decoder_dropout\")\n",
        "  decoder_dense = ks.layers.Dense(len_target_vocab, activation=\"softmax\", name=\"decoder_dense\")\n",
        "\n",
        "  # decoder feedforward path\n",
        "  decoder_rnn_out = decoder_embedding(decoder_input)\n",
        "  for i,decoder_rnn in enumerate(decoder_rnns):\n",
        "    if enc_state_dep=='first':\n",
        "      decoder_state_input = encoder_state if i==0 else None\n",
        "    elif enc_state_dep=='all':\n",
        "      decoder_state_input = encoder_state\n",
        "    else:\n",
        "      decoder_state_input = None\n",
        "    decoder_ret = decoder_rnn(decoder_rnn_out, initial_state=decoder_state_input)\n",
        "    decoder_rnn_out = decoder_ret[0]\n",
        "  if dec_attention:\n",
        "    context_vec, attn_weights = decoder_attention([decoder_rnn_out, encoder_output], return_attention_scores=True)\n",
        "    decoder_dense_input = decoder_concat([decoder_rnn_out, context_vec])\n",
        "  else:\n",
        "    decoder_dense_input = decoder_rnn_out\n",
        "  decoder_dense_input = decoder_dropout(decoder_dense_input, training=True)\n",
        "  decoder_output = decoder_dense(decoder_dense_input)\n",
        "\n",
        "  model = ks.Model([encoder_input, decoder_input], decoder_output, name=\"training_model\")\n",
        "  model.netparams = netparams\n",
        "  return model\n",
        "\n",
        "def gen_enc_dec_models(model):\n",
        "  # get layer from name\n",
        "  layer_idxs = dict([(l.name,i) for i,l in enumerate(model.layers)])\n",
        "  layer_from_name = lambda s: model.layers[layer_idxs[s]]\n",
        "  \n",
        "  # unpack network parameters\n",
        "  netparams  = model.netparams\n",
        "  latent_dim = netparams[\"latent_dim\"]\n",
        "  enc_layers = netparams[\"enc_layers\"]\n",
        "  dec_layers = netparams[\"dec_layers\"]\n",
        "  dec_attention = netparams[\"dec_attention\"]\n",
        "  recurrent_cell_name = netparams[\"recurrent_cell\"]\n",
        "\n",
        "  # reconstructing encoder model\n",
        "    # inputs\n",
        "  encoder_model_input = layer_from_name(\"encoder_input\").input\n",
        "    # outputs\n",
        "  encoder_model_ret = layer_from_name(\"encoder_rnn_\"+str(enc_layers)).output\n",
        "  encoder_model_output, encoder_model_state_output = encoder_model_ret[0], list(encoder_model_ret[1:])\n",
        "    # model reconstruction\n",
        "  encoder_model = ks.Model(\n",
        "    encoder_model_input, \n",
        "    [encoder_model_output, encoder_model_state_output], \n",
        "    name=\"encoder_model\"\n",
        "  )\n",
        "\n",
        "  # reconstructing decoder model\n",
        "    # inputs\n",
        "  decoder_model_input  = layer_from_name(\"decoder_input\").input\n",
        "  decoder_model_encoder_output = ks.Input(shape=(max_len_input,latent_dim,), name=\"decoder_encoder_output\")\n",
        "  decoder_model_state_input  = []\n",
        "    # outputs\n",
        "  decoder_model_output = layer_from_name(\"decoder_embedding\")(decoder_model_input)\n",
        "  decoder_model_state_output = []\n",
        "    # model reconstruction\n",
        "  for dec_layer in range(1,dec_layers+1):\n",
        "    decoder_model_state_input.append(\n",
        "        [ks.Input(shape=(latent_dim,), name=\"decoder_state_h_\"+str(dec_layer)), ks.Input(shape=(latent_dim,), name=\"decoder_state_c_\"+str(dec_layer))] \n",
        "        if recurrent_cell_name==\"LSTM\" else [ks.Input(shape=(latent_dim,), name=\"decoder_state_\"+str(dec_layer))]\n",
        "    )\n",
        "    decoder_ret = layer_from_name(\"decoder_rnn_\"+str(dec_layer))(decoder_model_output, initial_state = decoder_model_state_input[-1])\n",
        "    decoder_model_output, decoder_model_state = decoder_ret[0], list(decoder_ret[1:])\n",
        "    decoder_model_state_output.append(decoder_model_state)\n",
        "    dec_layer += 1\n",
        "  if dec_attention:\n",
        "    decoder_model_context_vec, decoder_model_attention_weights = layer_from_name(\"decoder_attention\")([decoder_model_output, decoder_model_encoder_output], return_attention_scores=True)\n",
        "    decoder_model_output = layer_from_name(\"decoder_concat\")([decoder_model_output, decoder_model_context_vec])\n",
        "  decoder_model_output = layer_from_name(\"decoder_dropout\")(decoder_model_output, training=False)\n",
        "  decoder_model_output = layer_from_name(\"decoder_dense\")(decoder_model_output)\n",
        "  if dec_attention:\n",
        "    decoder_model = ks.Model(\n",
        "      [decoder_model_input, decoder_model_encoder_output, decoder_model_state_input], \n",
        "      [decoder_model_output, decoder_model_state_output, decoder_model_attention_weights], \n",
        "      name=\"decoder_model\"\n",
        "    )\n",
        "  else:\n",
        "    decoder_model = ks.Model(\n",
        "      [decoder_model_input, decoder_model_state_input], \n",
        "      [decoder_model_output, decoder_model_state_output], \n",
        "      name=\"decoder_model\"\n",
        "    )\n",
        "\n",
        "  encoder_model.netparams = netparams\n",
        "  decoder_model.netparams = netparams\n",
        "\n",
        "  return encoder_model, decoder_model"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jUg86VR6CFA"
      },
      "source": [
        "## Decoding Input String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vdSmTDeI-XB"
      },
      "source": [
        "def decode_sequence(input_seq, models):\n",
        "  # Convert string input to numerical array\n",
        "  encoded_input_seq = str_to_numarray([input_seq], max_len_input, input_dict)\n",
        "\n",
        "  # Unpack testing model and its network parameters\n",
        "  encoder_model, decoder_model = models\n",
        "  netparams = decoder_model.netparams\n",
        "  latent_dim = netparams[\"latent_dim\"]\n",
        "  recurrent_cell_name = netparams[\"recurrent_cell\"]\n",
        "  dec_attention = netparams[\"dec_attention\"]\n",
        "  dec_layers = netparams[\"dec_layers\"]\n",
        "  enc_state_dep = netparams[\"enc_state_dep\"]\n",
        "  \n",
        "  # Run encoder model and create initial state for decoder\n",
        "  encoder_layer_out, encoder_states_out = encoder_model.predict(encoded_input_seq)\n",
        "  default_initial_state = [np.zeros((1,latent_dim))]*(2 if recurrent_cell_name==\"LSTM\" else 1)\n",
        "  if enc_state_dep=='first':\n",
        "    decoder_state = [encoder_states_out] + [default_initial_state]*(dec_layers-1)\n",
        "  elif enc_state_dep=='all':\n",
        "    decoder_state = [encoder_states_out] * dec_layers\n",
        "  else:\n",
        "    decoder_state = [default_initial_state] * dec_layers\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.array(target_dict[STARTCHAR],ndmin=2)\n",
        "\n",
        "  # Sampling loop for a batch of sequences\n",
        "  # (to simplify, here we assume a batch of size 1).\n",
        "  stop_condition = False\n",
        "  decoded_sentence = \"\"\n",
        "  while not stop_condition:\n",
        "    if dec_attention:\n",
        "      output_tokens, decoder_state, attn_weights = decoder_model.predict([target_seq, encoder_layer_out, decoder_state])\n",
        "    else:\n",
        "      output_tokens, decoder_state = decoder_model.predict([target_seq, decoder_state])\n",
        "\n",
        "    # Sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = target_vocab[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        "\n",
        "    if dec_attention:\n",
        "      # attention weights: my time to shine!\n",
        "      pass\n",
        "    \n",
        "    # Exit condition: either hit max length\n",
        "    # or find stop character.\n",
        "    if sampled_char == ENDCHAR or len(decoded_sentence) > max_len_target:\n",
        "      stop_condition = True\n",
        "\n",
        "    # Update the target sequence (of length 1).\n",
        "    target_seq[0,0] = sampled_token_index\n",
        "  return decoded_sentence"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TITfjs6n5kh_"
      },
      "source": [
        "# Sample Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR2_cz6vUimT"
      },
      "source": [
        "## Training a model for a particular network configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OEEPxLutm-Q"
      },
      "source": [
        "# initialize training model\n",
        "netparams = DEFAULT_NETPARAMS.copy()\n",
        "netparams.update({\n",
        "  \"embed_size\": 16, \n",
        "  \"latent_dim\": 256, \n",
        "  \"enc_layers\": 3, \n",
        "  \"dec_layers\": 2, \n",
        "  \"recurrent_cell\": \"LSTM\",\n",
        "  \"dec_attention\": True,\n",
        "  \"dropout\": 0.05, \n",
        "  \"beam_size\": 1, \n",
        "  \"enc_state_dep\": \"first\"\n",
        "})\n",
        "training_model = fresh_training_model(netparams) # ks.models.load_model(savedModelPath)\n",
        "training_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET4ji9GkYz0W"
      },
      "source": [
        "# training parameters\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "# compile training model\n",
        "training_model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#wandb.init(project=\"Dakshina HI Test 1\")\n",
        "# train seq2seq model\n",
        "training_model.fit(\n",
        "    [enc_inp_train, dec_inp_train],\n",
        "    dec_tgt_train_onehot,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=( [enc_inp_val, dec_inp_val], dec_tgt_val_onehot )\n",
        "    #,callbacks=[WandbCallback(monitor=\"val_accuracy\")]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohl7khBRXUTc"
      },
      "source": [
        "training_model.evaluate(x=[enc_inp_test, dec_inp_test], y=dec_tgt_test_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skFP9OyVWPQ"
      },
      "source": [
        "training_model.save(\"/content/drive/MyDrive/Sem_8/trained_models/s2s_samplerun.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6NseynqUvH1"
      },
      "source": [
        "## Testing trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHwFkyuoIfd7"
      },
      "source": [
        "test_model = training_model # ks.models.load_model(\"/content/drive/MyDrive/Sem_8/s2s_samplerun.h5\")\n",
        "encoder_model, decoder_model = gen_enc_dec_models(test_model)\n",
        "encoder_model.summary()\n",
        "decoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNdOBVzwTYM1"
      },
      "source": [
        "print(decode_sequence(\"intel\", [encoder_model, decoder_model]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8_vhwvv7W4y"
      },
      "source": [
        "# Training Sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vtz3Qo0Ca7v"
      },
      "source": [
        "sweeplog_path = \"/content/drive/MyDrive/Sem_8/sweeplog.txt\"\n",
        "savemodel_path = \"/content/drive/MyDrive/Sem_8/trained-models\"\n",
        "run_sep = '-='*30+'-'\n",
        "\n",
        "!touch $sweeplog_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOWMrRtOFF8-"
      },
      "source": [
        "def runWandbSweep():\n",
        "  wandb.init(project=\"dakshina_hi_no_attention_1\")\n",
        "  \n",
        "  tcr_wandb_format     = timestr(\"%b %d' %H:%M:%S\")\n",
        "  tcr_savemodel_format = timestr(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "  cfg = wandb.config\n",
        "  netparams = DEFAULT_NETPARAMS.copy()\n",
        "  netparams.update({\n",
        "    \"embed_size\": cfg.embedding_size, \n",
        "    \"latent_dim\": cfg.hidden_layer_size,\n",
        "    \"enc_layers\": cfg.num_encoder_layers,\n",
        "    \"dec_layers\": cfg.num_decoder_layers,\n",
        "    \"recurrent_cell\": cfg.recurrent_cell,\n",
        "    \"dec_attention\": cfg.decoder_attention,\n",
        "    \"dropout\": cfg.dropout,\n",
        "    \"enc_state_dep\": cfg.encoder_state_dependencies\n",
        "  })\n",
        "  \n",
        "  log_output = \"Sweep run created on \"+tcr_wandb_format+\" (\"+tcr_savemodel_format+\") with following sweep parameters:\\n\"\n",
        "  for i,k in enumerate(netparams):\n",
        "    log_output += str(i+1)+\". \"+k+\" = \"+str(netparams[k])+'\\n'\n",
        "  log_output += \"\\n\\n\"\n",
        "  open(sweeplog_path,'a').write(log_output)\n",
        "  \n",
        "  model = fresh_training_model(netparams)\n",
        "  model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  \n",
        "  model.fit(\n",
        "      [enc_inp_train, dec_inp_train],\n",
        "      dec_tgt_train_onehot,\n",
        "      batch_size=cfg.batch_size,\n",
        "      epochs=cfg.epochs,\n",
        "      validation_data=( [enc_inp_val, dec_inp_val], dec_tgt_val_onehot ),\n",
        "      callbacks=[WandbCallback(monitor=\"val_accuracy\")]\n",
        "  )\n",
        "  model.save(savemodel_path+\"/s2s_sweep_\"+tcr_savemodel_format+\".h5\")\n",
        "  open(sweeplog_path,'a').write(\"Sweep run completed, model saved with timestamp: \"+tcr_savemodel_format+\"\\n\\n\"+run_sep+\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71HRcdIeY5uU"
      },
      "source": [
        "wandbSweepCfg = {\n",
        "  \"name\":\"Dakshina HI Parameter Sweep\", \n",
        "  \"metric\":{\n",
        "    \"name\":\"val_accuracy\",\n",
        "    \"goal\":\"maximize\"\n",
        "  }, \n",
        "  \"method\": \"bayes\", \n",
        "  \"parameters\":{\n",
        "    # network parameters\n",
        "    \"embedding_size\": {\"values\":[16, 32, 64, 256]},\n",
        "    \"hidden_layer_size\": {\"values\":[16, 32, 64, 256]},\n",
        "    \"num_encoder_layers\": {\"values\":[1, 2, 3]},\n",
        "    \"num_decoder_layers\": {\"values\":[1, 2, 3]},\n",
        "    \"recurrent_cell\": {\"values\":[\"SimpleRNN\", \"GRU\", \"LSTM\"]},\n",
        "    \"decoder_attention\": {\"values\":[False, True]},\n",
        "    \"dropout\": {\"values\":[0,0.1,0.2]},\n",
        "    \"encoder_state_dependencies\":{\"values\":[\"first\"]},\n",
        "    \n",
        "    # training parameters\n",
        "    \"epochs\": { \"values\":[30] },\n",
        "    \"batch_size\": { \"values\":[32] }\n",
        "  }\n",
        "}\n",
        "\n",
        "open(sweeplog_path,'a').write(run_sep+\"\\n\\nStarted sweep at \"+timestr(\"%H:%M:%S on %b %d, %Y\")+'\\n\\n')\n",
        "\n",
        "sweepId = wandb.sweep(wandbSweepCfg)#\"vasid99/uncategorized/uklnmska\"\n",
        "wandb.agent(sweepId, function = runWandbSweep)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}