{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of SG_DL_assign01_3",
      "provenance": [],
      "collapsed_sections": [
        "SK1UeoJdYdHP",
        "A_-pg1irI4Ow"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasid99/cs6910-dl/blob/main/Assignment01/Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsh0vUPPl1gm"
      },
      "source": [
        "- change backpropagation for l2 reg\n",
        "\n",
        "- do loss calculation for validation set, not training set\n",
        "\n",
        "- change error calculation\n",
        "\n",
        "- we are assuming that 0-1 error is to be reported\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK1UeoJdYdHP"
      },
      "source": [
        "# Library+Dataset Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYppBN-iFAB2"
      },
      "source": [
        "!pip install -q wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRAVTE1O9Sn"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b4N8bBKd8qf"
      },
      "source": [
        "((x_train, y_train), (x_test, y_test)) = fashion_mnist.load_data()\r\n",
        "classes, idx_sample_class = np.unique(y_train, return_index=True)\r\n",
        "sample_images = x_train[ idx_sample_class ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUeJZNT6f7NF"
      },
      "source": [
        "'''wandb.init()\n",
        "wandb.log({\"Sample_Images\": \\\n",
        "[ wandb.Image(sample_images[i], caption=\"Label:\"+str(classes[i])) \\\n",
        "for i in range(len(idx_sample_class)) ] })'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxO0RopTVSMv"
      },
      "source": [
        "trgt = np.array([[1,0,0,0],[0,1,0,1],[0,0,1,0]]).astype(float)\r\n",
        "out = np.arange(1,13).reshape((3,4))\r\n",
        "print('out=',out)\r\n",
        "print('out[trgt==1]=',out[trgt==1])\r\n",
        "out2 = trgt.copy()\r\n",
        "print(out2)\r\n",
        "print('out2[trgt==1] = ',out2[trgt==1])\r\n",
        "print('1/out[trgt==1] = ',1/out[trgt==1])\r\n",
        "out2[trgt==1] = 1/out[trgt==1]\r\n",
        "out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_st426MeIyo6"
      },
      "source": [
        "# Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRn-J2MUwg_"
      },
      "source": [
        "# function constants\n",
        "ACTIVATION_SIGMOID   = 'sigmoid'\n",
        "ACTIVATION_SOFTMAX   = 'softmax'\n",
        "ACTIVATION_THRESHOLD = 'threshold'\n",
        "ACTIVATION_RELU      = 'reLU'\n",
        "ACTIVATION_TANH      = 'tanh'\n",
        "\n",
        "LOSS_SQERROR         = 'sq_error'\n",
        "LOSS_CROSSENTROPY    = 'cross_entropy'\n",
        "\n",
        "GDOPT_NONE           = 'vanilla'\n",
        "GDOPT_MOMENTUM       = 'momentum'\n",
        "GDOPT_NESTEROV       = 'NAG'\n",
        "GDOPT_ADAGRAD        = 'adagrad'\n",
        "GDOPT_RMSPROP        = 'rmsprop'\n",
        "GDOPT_ADAM           = 'adam'\n",
        "GDOPT_NADAM          = 'nadam'\n",
        "\n",
        "WINIT_RANDOM         = 'random'\n",
        "WINIT_XAVIER         = 'xavier'\n",
        "\n",
        "# numerical limits\n",
        "EXP_INPUT_TOL = 100\n",
        "'''\n",
        "def expinbound(x):\n",
        "  return np.minimum(np.maximum(x,-EXP_INPUT_TOL),EXP_INPUT_TOL) # return np.maximum(x - np.max(x), -300) # \n",
        "\n",
        "def expinbound(x):\n",
        "  return 200*(x - np.mean(x, axis=0))/(np.amax(x, axis=0) - np.amin(x, axis=0)) # return np.maximum(x - np.max(x), -300) # \n",
        "'''\n",
        "def expinbound(x):\n",
        "  return np.maximum(x - np.amax(x, axis=0) + 300, -300) # \n",
        "\n",
        "class neuralNetwork:\n",
        "  \"\"\"\n",
        "  Class for a neural network made up of multiple layers of perceptrons\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters and hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # assign basic hyperparameters to neural network and math functions object\n",
        "    self.hyperparams = {}\n",
        "    self.setHyperparameters(hyperparams)\n",
        "\n",
        "    # initialize the weight and bias matrices of the NN\n",
        "    self.initModel(hyperparams)\n",
        "  \n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # change values of only the hyperparameters specified in the input variable\n",
        "    self.hyperparams.update(hp)\n",
        "    \n",
        "    # use member variables for commonly used hyperparameters\n",
        "    self.layerSizes       = self.hyperparams[\"layerSizes\"]\n",
        "    self.batchSize        = self.hyperparams[\"batchSize\"]\n",
        "    self.learningRate     = self.hyperparams[\"learningRate\"]\n",
        "    self.epochs           = self.hyperparams[\"epochs\"]\n",
        "    self.numLayers        = len(self.layerSizes) - 1\n",
        "    \n",
        "    # set math functions object hyperparameters\n",
        "    assert len(self.hyperparams[\"activations\"])==self.numLayers, \"number of layers (%d) and number of activations (%d) don't match\"%(self.numLayers,len(hp[\"activations\"]))\n",
        "    self.activations = self.hyperparams[\"activations\"]\n",
        "    self.lossFn = self.hyperparams[\"lossFn\"]\n",
        "    self.regparam = self.hyperparams[\"regparam\"]\n",
        "\n",
        "  def initModel(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters (weight and bias matrices) of neural network\n",
        "    \"\"\"\n",
        "    # checking bounds arg\n",
        "    bounds = (0,1)\n",
        "    if \"initWeightBounds\" in hyperparams.keys():\n",
        "      assert len(hyperparams[\"initWeightBounds\"])==2, \"bounds arg has to be a list/tuple of 2 numbers\"\n",
        "      bounds = hyperparams[\"initWeightBounds\"]\n",
        "\n",
        "    # create list of weight matrices and bias vectors\n",
        "    # the goal is to make the indexing same as that in lecture derivation, hence the dummy values\n",
        "    self.wmat = [np.array([1],ndmin=2)]\n",
        "    self.bias = [np.array([1],ndmin=2)]\n",
        "    \n",
        "    # create random initial parameters and append them to the above initialized lists\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      if self.hyperparams[\"initWeightMethod\"]==WINIT_XAVIER:\n",
        "        bounds = (-1/(self.layerSizes[i-1])**0.5,1/(self.layerSizes[i-1])**0.5)\n",
        "      self.wmat.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],self.layerSizes[i-1])+bounds[0])\n",
        "      self.bias.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],1)+bounds[0])\n",
        "\n",
        "  def activation(self,layerNum,x):\n",
        "    \"\"\"\n",
        "    Compute and return activation values for a given layer and its sum values\n",
        "    \"\"\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "      out = np.zeros(np.shape(x))\n",
        "      out[x>100] = 1\n",
        "      out[(x<=100)&(x>=-100)] = 1/(1 + np.exp(x[(x<=100)&(x>=-100)]))\n",
        "      return out\n",
        "    elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "      z = np.exp(expinbound(x))\n",
        "      return z/np.sum(z,axis=0)\n",
        "    elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "      return (x>=0)+0\n",
        "    elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "      return np.maximum(x,0)\n",
        "    elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "      return np.tanh(x)\n",
        "  \n",
        "  def activationDerivative(self,layerNum,**kwargs):\n",
        "    \"\"\"\n",
        "    Compute and return activation derivative values for a given layer and its sum or output values depending on the given argument\n",
        "    \"\"\"\n",
        "    assert ( len(kwargs.keys())==1 and np.any([_ in kwargs.keys() for _ in [\"x\",\"y\"]]) ), \"activationDerivative argument malformed. \\\n",
        "    Use activationDerivative(layerNum,x=x_val) or activationDerivative(layerNum,y=y_val)\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    \n",
        "    if \"y\" in kwargs.keys():\n",
        "      y = kwargs[\"y\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (y>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "        return 1-y**2\n",
        "    else:\n",
        "      x = kwargs[\"x\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        z = np.exp(x)\n",
        "        s = np.sum(z)\n",
        "        return z*(s-z)/(s**2)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (x>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return 1-np.tanh(x)**2\n",
        "  \n",
        "  def lossOutputDerivative(self,outputData,targetData):\n",
        "    \"\"\"\n",
        "    Compute and return loss derivatives for given output and target data\n",
        "    \"\"\"\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return (outputData-targetData) #/len(targetData[0])\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      outputData = np.maximum(outputData/np.sum(outputData, axis=0), 1e-100)\n",
        "      ret = targetData.copy()\n",
        "      ret[targetData==1] = -(1 / outputData[targetData==1]) #/len(targetData[0])\n",
        "      return ret\n",
        "  \n",
        "  def forwardPass(self, inputData):\n",
        "    \"\"\"\n",
        "    Compute output activations of all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                              # --- PSEUDOCODE ---\n",
        "    h     = inputData                              # h[0] = x\n",
        "    hData = [h]                                    #\n",
        "    datasetSize = np.shape(inputData)[1]           #\n",
        "    #                                              #\n",
        "    for i in range(1,self.numLayers+1):            # for i from 1 to L:\n",
        "      a   = self.wmat[i] @ h + self.bias[i]        #     a[i] = w[i] @ h[i-1] + b[i]\n",
        "      h   = self.activation(i,a)               #     h[i] = f(a[i])\n",
        "      hData.append(h)\n",
        "    \n",
        "    return hData\n",
        "  \n",
        "  def backwardPass(self, layerwiseOutputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute weight and bias gradients for all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                                                                        # --- PSEUDOCODE ---\n",
        "    lossData    = self.lossOutputDerivative(layerwiseOutputData[-1], targetData)             # loss_derivative = d(loss)/dh[L]\n",
        "    Delta       = lossData                                                                   # Delta[L] = loss_derivative\n",
        "    datasetSize = np.shape(targetData)[1]                                                    #\n",
        "    biasInputs  = np.array(np.ones(datasetSize),ndmin=2).T                                   #\n",
        "    gradW       = []                                                                         #\n",
        "    gradB       = []                                                                         #\n",
        "    #                                                                                        #\n",
        "    for iFwd in range(self.numLayers):                                                       # for i from L to 1:\n",
        "      i            = self.numLayers - iFwd                                                   #     // index correction\n",
        "      stocBiasCorr = self.activationDerivative(i,y=layerwiseOutputData[i]) * Delta           #     stochastic_bias_corrections = f'(a[i]) * Delta[i]\n",
        "      gW           = stocBiasCorr @ layerwiseOutputData[i-1].T + self.regparam*self.wmat[i] #/len(targetData[0])  #     grad(W[i]) = stochastic_bias_corrections x (h[i-1]).T\n",
        "      gB           = stocBiasCorr @ biasInputs + self.regparam*self.bias[i] #/len(targetData[0])                              #     grad(b[i]) = sum(stochastic_bias_corrections)\n",
        "      Delta        = self.wmat[i].T @ stocBiasCorr                                           #     Delta[i-1] = W[i] x stochastic_bias_corrections\n",
        "      \n",
        "      gradW.append(gW)\n",
        "      '''\n",
        "      f=open(\"wout.txt\",\"a\")\n",
        "      f.write(str(gW))\n",
        "      f.write(str(gB))\n",
        "      f.close()\n",
        "      '''\n",
        "      gradB.append(gB)\n",
        "    \n",
        "    # dummy element and order handling\n",
        "    gradW.append(np.array([0],ndmin=2))\n",
        "    gradW.reverse()\n",
        "    gradB.append(np.array([0],ndmin=2))\n",
        "    gradB.reverse()\n",
        "    \n",
        "    return (gradW,gradB)\n",
        "  \n",
        "  def infer(self,inputData,**kwargs):\n",
        "    \"\"\"\n",
        "    Perform inference on input dataset using the neural network\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    \n",
        "    # perform forward pass and return last-layer outputs\n",
        "    return self.forwardPass(inputData)[-1]\n",
        "\n",
        "  def gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex):##\n",
        "    # create data batches\n",
        "    startIndex  = batchSize * batchIndex\n",
        "    endIndex    = min(startIndex + batchSize, datasetSize)\n",
        "    inputBatch  = inputData[:,startIndex:endIndex]\n",
        "    targetBatch = targetData[:,startIndex:endIndex]\n",
        "    # perform forward and backward passes to compute gradients\n",
        "    layerwiseOutputData = self.forwardPass(inputBatch)\n",
        "    (gradW, gradB)      = self.backwardPass(layerwiseOutputData,targetBatch)\n",
        "    return gradW, gradB ##\n",
        "\n",
        "  def update_val_train_loss_and_acc(self, inputData, targetData, x_val, y_val, epoch):\n",
        "    TOL = 1e-300\n",
        "    y_pred_train = self.infer(inputData, colwiseData =True)\n",
        "    y_pred_val   = self.infer(x_val, colwiseData =True)\n",
        "    #print(y_pred_train[:,:4]-targetData[:,:4])\n",
        "    modW_sq = np.sum( np.array( [ np.linalg.norm(W)**2 for W in self.wmat ] ) )\n",
        "    modB_sq = np.sum( np.array( [ np.linalg.norm(B)**2 for B in self.bias ] ) )\n",
        "    modtheta_sq = (modW_sq + modB_sq)\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      loss_train = (0.5*np.sum(np.linalg.norm(y_pred_train - targetData, axis=0)**2) + 0.5*self.regparam*modtheta_sq) #/len(targetData[0])\n",
        "      loss_val   = (0.5*np.sum(np.linalg.norm(  y_pred_val - y_val     , axis=0)**2) + 0.5*self.regparam*modtheta_sq) #/len(y_val[0])\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      loss_train = - np.sum(targetData * np.log( np.maximum(y_pred_train, TOL)))/len(targetData[0]) + 0.5*self.regparam*modtheta_sq\n",
        "      loss_val   = - np.sum(     y_val * np.log( np.maximum(y_pred_val, TOL)  ))/len(y_val[0]) + 0.5*self.regparam*modtheta_sq\n",
        "    acc_train = np.count_nonzero( np.argmax(targetData, axis=0) == np.argmax(y_pred_train, axis=0) )/len(targetData[0])\n",
        "    acc_val   = np.count_nonzero( np.argmax(     y_val, axis=0) == np.argmax(  y_pred_val, axis=0) )/len(y_val[0])\n",
        "    self.loss_train.append(loss_train)\n",
        "    self.acc_train.append(acc_train)\n",
        "    self.loss_val.append(loss_val)\n",
        "    self.acc_val.append(acc_val)\n",
        "    wandb.log({\"loss_train\":self.loss_train[-1],\"acc_train\":self.acc_train[-1],\n",
        "               \"loss_val\":self.loss_val[-1],\"acc_val\":self.acc_val[-1], \"epoch\": epoch}) #,\"weightDump\":wandb.Html(str(self.wmat))\n",
        "\n",
        "  def sgd(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch)\n",
        "      for batchIndex in range(numBatches):       \n",
        "        #Get grad theta\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -self.learningRate * gradW[i]\n",
        "          self.bias[i] += -self.learningRate * gradB[i]\n",
        "    self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch+1)\n",
        "\n",
        "  def momentumGD(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    self.loss_lst_train = [] ##\n",
        "    self.acc_lst_train = [] ##\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch)\n",
        "      for batchIndex in range(numBatches):       \n",
        "        #Get grad theta\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -update_w[i]\n",
        "          self.bias[i] += -update_b[i]\n",
        "    self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch+1) \n",
        "\n",
        "  def NAG(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch)\n",
        "      for batchIndex in range(numBatches):\n",
        "        # perform look ahead parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -gamma*update_w[i]\n",
        "          self.bias[i] += -gamma*update_b[i]\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -eta*gradW[i]\n",
        "          self.bias[i] += -eta*gradB[i]\n",
        "    self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch+1) \n",
        "\n",
        "  def rmsprop(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    beta = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #Initialise\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch)\n",
        "      for batchIndex in range(numBatches):        \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          v_w[i] = beta*v_w[i] + (1-beta)*gradW[i]**2\n",
        "          v_b[i] = beta*v_b[i] + (1-beta)*gradB[i]**2 \n",
        "          self.wmat[i] += -eta * (v_w[i] + epsilon)**-0.5 * gradW[i]\n",
        "          self.bias[i] += -eta * (v_b[i] + epsilon)**-0.5 * gradB[i]\n",
        "    self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch+1) \n",
        "  \n",
        "  def adam(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch)\n",
        "      for batchIndex in range(numBatches): \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = m_w[i]/(1-beta_1**t)\n",
        "          m_b_hat = m_b[i]/(1-beta_1**t)\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "    self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch+1) \n",
        "\n",
        "  def nadam(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch)\n",
        "      for batchIndex in range(numBatches): \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = (beta_1/(1-beta_1**(t+1)))*m_w[i] + ((1-beta_1)/(1-beta_1**t))*gradW[i]\n",
        "          m_b_hat = (beta_1/(1-beta_1**(t+1)))*m_b[i] + ((1-beta_1)/(1-beta_1**t))*gradB[i]\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "    self.update_val_train_loss_and_acc(inputData, targetData, x_val, y_val, epoch+1) \n",
        "\n",
        "\n",
        "  def train(self, inputData, targetData, x_val, y_val, **kwargs):\n",
        "    \"\"\"\n",
        "    Train the network on the given input and target datasets\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input and target dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    targetData = np.array(targetData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "      targetData = targetData.T\n",
        "      x_val = x_val.T\n",
        "      y_val = y_val.T\n",
        "    assert np.shape(inputData)[1]==np.shape(targetData)[1], \"input and target datasets have different dataset sizes\"\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    assert np.shape(targetData)[0]==self.layerSizes[-1], \"size of target datapoint differs from size of target vector given as hyperparameter\"\n",
        "    datasetSize = np.shape(targetData)[1]\n",
        "\n",
        "    # calculate batch parameters\n",
        "    batchSize = datasetSize if self.batchSize==-1 else self.batchSize\n",
        "    numBatches = int(np.ceil(datasetSize / batchSize))\n",
        "\n",
        "    #Initialise loss and accuracy lists\n",
        "    self.loss_train = []\n",
        "    self.acc_train = []\n",
        "    self.loss_val = []\n",
        "    self.acc_val = []\n",
        "\n",
        "    if self.hyperparams[\"optimizer\"] == GDOPT_NONE:\n",
        "      self.sgd(inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_MOMENTUM:\n",
        "      self.momentumGD(inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NESTEROV:\n",
        "      self.NAG(inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_RMSPROP:\n",
        "      self.rmsprop(inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_ADAM:\n",
        "      self.adam(inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NADAM:\n",
        "      self.nadam(inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val)\n",
        "    \n",
        "    self.loss_train = np.array(self.loss_train)\n",
        "    self.acc_train = np.array(self.acc_train)\n",
        "    self.loss_val = np.array(self.loss_val)\n",
        "    self.acc_val = np.array(self.acc_val)\n",
        "\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_-pg1irI4Ow"
      },
      "source": [
        "# Data Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pswfmu-9fdLW"
      },
      "source": [
        "#Reshaping the 'x' data:\r\n",
        "len_1D = x_train.shape[1]*x_train.shape[2]\r\n",
        "x_train_1D = np.array( [x.reshape(len_1D) for x in x_train] )\r\n",
        "x_test_1D  = np.array( [x.reshape(len_1D) for x in x_test] )\r\n",
        "\r\n",
        "#Transforming 'y' data - changing scalar i to vector e(i)\r\n",
        "y_train_1D = np.zeros( (len(y_train), len(classes)) )\r\n",
        "for i in range(len(y_train)):\r\n",
        "  y_train_1D[i, y_train[i]] = 1\r\n",
        "y_test_1D = np.zeros( (len(y_test), len(classes)) )\r\n",
        "for i in range(len(y_test)):\r\n",
        "  y_test_1D[i, y_test[i]] = 1\r\n",
        "\r\n",
        "y_train_1D = y_train_1D.astype(float)\r\n",
        "y_test_1D = y_test_1D.astype(float)\r\n",
        "\r\n",
        "frac_val = 0.1\r\n",
        "all_idx = np.arange(len(x_train))\r\n",
        "val_idx = np.random.choice(all_idx, int(frac_val*len(x_train)), replace=False)\r\n",
        "tr2_idx = np.array([i for i in all_idx if i not in val_idx])\r\n",
        "\r\n",
        "x_train2 = x_train_1D[tr2_idx]\r\n",
        "y_train2 = y_train_1D[tr2_idx]\r\n",
        "x_val = x_train_1D[val_idx]\r\n",
        "y_val = y_train_1D[val_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld-ZEKD-YvKg"
      },
      "source": [
        "# Runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnAKPohu-0T7"
      },
      "source": [
        "'''\r\n",
        "plt.plot( NN.loss_train, label='train loss')\r\n",
        "plt.plot( NN.loss_val, label='validation loss')\r\n",
        "plt.plot( NN.acc_train, label='train acc')\r\n",
        "plt.plot( NN.acc_val, label='validation acc')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yToPXmOKBtC-"
      },
      "source": [
        "'''\n",
        "f=open(\"wout.txt\",\"w\")\n",
        "f.write(\"\")\n",
        "f.close()\n",
        "'''\n",
        "wandb.init(project=\"Overtraining Test - 1\")\n",
        "hyp = {\n",
        "    \"layerSizes\": [len(x_train_1D[0]),100,len(y_train_1D[0])],\n",
        "    \"batchSize\": 128,\n",
        "    \"learningRate\": 1e-6,\n",
        "    \"epochs\": 10000,\n",
        "    \"activations\": [ ACTIVATION_RELU, ACTIVATION_SOFTMAX ],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY, #LOSS_CROSSENTROPY LOSS_SQERROR\n",
        "    \"initWeightBounds\": (-1,1),\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-3,  # eta scaling hyperparam\n",
        "    \"regparam\": 0\n",
        "}\n",
        "\n",
        "#wandb.config.update(hyp)\n",
        "\n",
        "NN = neuralNetwork(hyp)\n",
        "NN.train(x_train2[:101]/(255), y_train2[:101], x_val[0:2]/(255), y_val[0:2])\n",
        "# Note that x_val, y_val are given as an input just to calculate the loss and error at each epoch.\n",
        "# They are not used anywhere to train the neural network\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlySAhxrY35S"
      },
      "source": [
        "# Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUACGeFN-Oyi"
      },
      "source": [
        "#wandb.init(project = \"NN Fashion MNIST\")\n",
        "\n",
        "def runSweep():\n",
        "  wandb.init() #project = \"NN Fashion MNIST\"\n",
        "  \n",
        "  hyp = {}\n",
        "  cfg = wandb.config\n",
        "\n",
        "  hyp[\"epochs\"] = cfg.epochs\n",
        "\n",
        "  layersHidden = []\n",
        "  for i in range(cfg.numHiddenLayers):\n",
        "    layersHidden.append(cfg.hiddenLayerSize)\n",
        "  hyp[\"layerSizes\"] = [784] + layersHidden + [10]\n",
        "\n",
        "  hyp[\"regparam\"] = cfg.L2Reg\n",
        "\n",
        "  hyp[\"learningRate\"] = cfg.learningRate\n",
        "\n",
        "  hyp[\"optimizer\"] = cfg.optimizer\n",
        "\n",
        "  hyp[\"batchSize\"] = cfg.batchSize\n",
        "\n",
        "  hyp[\"initWeightMethod\"] = cfg.initWeightMethod\n",
        "\n",
        "  hyp[\"activations\"] = []\n",
        "  for i in range(cfg.numHiddenLayers):\n",
        "    hyp[\"activations\"].append(cfg.activationFn)\n",
        "  hyp[\"activations\"].append(ACTIVATION_SOFTMAX)\n",
        "\n",
        "  hyp[\"lossFn\"] = cfg.loss\n",
        "\n",
        "  hyp.update({\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    })\n",
        "\n",
        "  run_name = wandb.run.name\n",
        "  wandb.run.name = run_name+'ep_'+str(hyp[\"epochs\"])+'_numhl_'+str(cfg.numHiddenLayers)\\\n",
        "  +'_hlsize_'+str(cfg.hiddenLayerSize)+'_l2reg_'+str(hyp[\"regparam\"])\\\n",
        "  +'_eta_'+str(hyp[\"learningRate\"])+'_'+str(hyp[\"optimizer\"])\\\n",
        "  +'_bs_'+str(hyp[\"batchSize\"])+'_'+str(hyp[\"initWeightMethod\"])+'_'+str(hyp[\"lossFn\"]) \n",
        "  wandb.run.save()\n",
        "\n",
        "  ## run train functions. Also, put wandb.log statements inside after loss/err calculation\n",
        "  nn = neuralNetwork(hyp)\n",
        "  nn.train(x_train2/255, y_train2, x_val/255, y_val)\n",
        "\n",
        "sweepCfg = {\n",
        "    \"name\":\"NN Fashion MNIST - Test Sweep9\", \n",
        "    \"metric\":{\n",
        "        \"name\":\"acc_val\", ##\n",
        "        \"goal\":\"maximize\"\n",
        "    }, \n",
        "    \"method\": \"bayes\", \n",
        "    \"parameters\":{\n",
        "        \"epochs\":{\n",
        "          \"values\":[25]\n",
        "        },\n",
        "        \"numHiddenLayers\":{\n",
        "          \"values\":[3,4,5]\n",
        "        },\n",
        "        \"hiddenLayerSize\":{\n",
        "          \"values\":[32,64,128]\n",
        "        },\n",
        "        \"L2Reg\":{\n",
        "          \"values\":[0,5e-4,0.5]\n",
        "        },\n",
        "        \"learningRate\":{\n",
        "          \"values\":[1e-3, 1e-5, 1e-7]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "          \"values\":[GDOPT_NONE, GDOPT_MOMENTUM, GDOPT_NESTEROV, GDOPT_RMSPROP, GDOPT_ADAM, GDOPT_NADAM]\n",
        "        },\n",
        "        \"batchSize\":{\n",
        "          \"values\":[32,64,128]\n",
        "        },\n",
        "        \"initWeightMethod\":{\n",
        "          \"values\":[WINIT_RANDOM, WINIT_XAVIER]\n",
        "        },\n",
        "        \"activationFn\":{\n",
        "          \"values\":[ACTIVATION_SIGMOID, ACTIVATION_RELU, ACTIVATION_TANH]#[ACTIVATION_SIGMOID, ACTIVATION_RELU, ACTIVATION_TANH]\n",
        "        },\n",
        "        \"loss\":{\n",
        "          \"values\": [LOSS_CROSSENTROPY] #[LOSS_CROSSENTROPY, LOSS_SQERROR]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweepId = wandb.sweep(sweepCfg)\n",
        "wandb.agent(sweepId, function = runSweep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51n0yc-CoFo-"
      },
      "source": [
        "# wandb log checks\n",
        "'''wandb.log(\n",
        "    {\"Parameter 1\": \n",
        "     [\"test string\",\"another test string\",\"yet another test\"], \n",
        "     \"Test text\": wandb.Html(\"<b><i>Working</i></b>\"), \n",
        "     \"Test table\": wandb.Table(columns=[\"I like this column\", \"Well, I like this one\"], data=[[\"The header above likes me\", \"Same here\"],[\"No idiot, he likes the whole column\", \"Good point there, neighbour\"]])})\n",
        "\n",
        "\n",
        "# wandb sweep checks\n",
        "def fsweeptest():\n",
        "  run = wandb.init(config={\"daIndex\":10})\n",
        "  for i in range(10):\n",
        "    wandb.log({\"x\": i, \"daMetric\": np.cos(np.pi*i*wandb.config.daIndex/80)})\n",
        "\n",
        "sweepCfg = {\"name\":\"Test sweep\", \n",
        "            \"metric\":{\n",
        "                \"name\":\"daMetric\",\n",
        "                \"goal\":\"maximise\"\n",
        "            },\n",
        "            \"method\":\"grid\", \n",
        "            \"parameters\":{\"daIndex\":{\"values\":[10,20,40,80,160]}}}\n",
        "sweepId = wandb.sweep(sweepCfg)\n",
        "wandb.agent(sweepId, function = fsweeptest)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8W7xnoYOGeI"
      },
      "source": [
        "# Performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzVXDYHEOMjI"
      },
      "source": [
        "'''\r\n",
        "f=open(\"wout.txt\",\"w\")\r\n",
        "f.write(\"\")\r\n",
        "f.close()\r\n",
        "'''\r\n",
        "wandb.init(project=\"Test run - 1\")\r\n",
        "hyp = {\r\n",
        "    \"layerSizes\": [len(x_train_1D[0]),32, 32, 32, 32, 32,len(y_train_1D[0])],\r\n",
        "    \"batchSize\": 128,\r\n",
        "    \"learningRate\": 1e-3,\r\n",
        "    \"epochs\": 30,\r\n",
        "    \"activations\": [ ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_SOFTMAX ],\r\n",
        "    \"lossFn\": LOSS_SQERROR, #LOSS_CROSSENTROPY LOSS_SQERROR\r\n",
        "    \"initWeightBounds\": (-1,1),\r\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\r\n",
        "    \"optimizer\": GDOPT_ADAM,\r\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\r\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\r\n",
        "    \"epsilon\": 1e-3,  # eta scaling hyperparam\r\n",
        "    \"regparam\": 5e-4\r\n",
        "}\r\n",
        "\r\n",
        "#wandb.config.update(hyp)\r\n",
        "\r\n",
        "NN = neuralNetwork(hyp)\r\n",
        "NN.train(x_train_1D/(255), y_train_1D, x_test_1D/(255), y_test_1D)\r\n",
        "# Note that x_val, y_val are given as an input just to calculate the loss and error at each epoch.\r\n",
        "# They are not used anywhere to train the neural network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4NvnzOURnNv"
      },
      "source": [
        "y_pred_1D = NN.infer(x_test_1D/(255))\r\n",
        "y_pred = np.argmax(y_pred_1D, axis=0)\r\n",
        "conf_mat = confusion_matrix(y_test, y_pred, normalize='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZnFVeUoW22B"
      },
      "source": [
        "X, Y = np.meshgrid( np.arange(len(classes))-0.2, np.arange(len(classes))-0.2 )\r\n",
        "x_vec = X.ravel()\r\n",
        "y_vec = Y.ravel()\r\n",
        "z_vec = 0\r\n",
        "\r\n",
        "dx = 0.4*np.ones_like(x_vec)\r\n",
        "dy = 0.4*np.ones_like(y_vec)\r\n",
        "dz = conf_mat.ravel()\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(15,10))\r\n",
        "ax = fig.add_subplot(111, projection='3d')\r\n",
        "\r\n",
        "minima = np.amin(conf_mat)\r\n",
        "maxima = np.amax(conf_mat)\r\n",
        "\r\n",
        "norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\r\n",
        "mapper = cm.ScalarMappable(norm=norm, cmap=cm.viridis)\r\n",
        "\r\n",
        "color_vec = [ mapper.to_rgba(v) for v_vec in conf_mat for v in v_vec]\r\n",
        "\r\n",
        "bar3d = ax.bar3d(x_vec, y_vec, z_vec, dx, dy, dz,\r\n",
        "         zsort='average', color=color_vec, shade=True,alpha=0.4, edgecolor='black')\r\n",
        "ax.set_xlabel('Predicted Class')\r\n",
        "ax.set_ylabel('True Class')\r\n",
        "ax.view_init(50,-100)\r\n",
        "\r\n",
        "fig.colorbar(bar3d, boundaries=np.linspace(np.amin(dz), np.amax(dz), 10 ))\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}