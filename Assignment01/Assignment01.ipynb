{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasid99/cs6910-dl/blob/main/Assignment01/Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsh0vUPPl1gm"
      },
      "source": [
        "- change backpropagation for l2 reg\n",
        "\n",
        "- do loss calculation for validation set, not training set\n",
        "\n",
        "- change error calculation\n",
        "\n",
        "- we are assuming that 0-1 error is to be reported\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYppBN-iFAB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2869b33-42e1-4bee-d47a-51385c324ec1"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 40.1MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.5MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=eac0dcc3384069c8fa05c16baf9b810dc7d6a29b893551b9476a6ae4cd00020a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=1a167dd0a655bd4979b866d31d8b5997feebbec174fb7df85bb32a8629ddbc10\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: pathtools, sentry-sdk, subprocess32, shortuuid, configparser, docker-pycreds, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRAVTE1O9Sn"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrW6NV3sY1CG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "6765768a-2ec0-420f-efb5-da9db13eaa52"
      },
      "source": [
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">wild-bush-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/vasid99/uncategorized\" target=\"_blank\">https://wandb.ai/vasid99/uncategorized</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/vasid99/uncategorized/runs/1sz97062\" target=\"_blank\">https://wandb.ai/vasid99/uncategorized/runs/1sz97062</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210310_101420-1sz97062</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fa48267c210>"
            ],
            "text/html": [
              "<h1>Run(1sz97062)</h1><iframe src=\"https://wandb.ai/vasid99/uncategorized/runs/1sz97062\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b4N8bBKd8qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1501c94-d908-4f77-8f56-b19c6d1618cf"
      },
      "source": [
        "((x_train, y_train), (x_test, y_test)) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19DhbVbQehm4"
      },
      "source": [
        "classes, idx_sample_class = np.unique(y_train, return_index=True)\r\n",
        "sample_images = x_train[ idx_sample_class ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUeJZNT6f7NF"
      },
      "source": [
        "wandb.log({\"Sample_Images\": \\\n",
        "[ wandb.Image(sample_images[i], caption=\"Label:\"+str(classes[i])) \\\n",
        "for i in range(len(idx_sample_class)) ] })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRn-J2MUwg_"
      },
      "source": [
        "# function constants\n",
        "ACTIVATION_SIGMOID   = 0\n",
        "ACTIVATION_SOFTMAX   = 1\n",
        "ACTIVATION_THRESHOLD = 2\n",
        "ACTIVATION_RELU      = 3\n",
        "ACTIVATION_TANH      = 4\n",
        "\n",
        "LOSS_SQERROR         = 0\n",
        "LOSS_CROSSENTROPY    = 1\n",
        "\n",
        "GDOPT_NONE           = 0\n",
        "GDOPT_MOMENTUM       = 1\n",
        "GDOPT_NESTEROV       = 2\n",
        "GDOPT_ADAGRAD        = 3\n",
        "GDOPT_RMSPROP        = 4\n",
        "GDOPT_ADAM           = 5\n",
        "GDOPT_NADAM          = 6\n",
        "\n",
        "WINIT_RANDOM         = 0\n",
        "WINIT_XAVIER         = 1\n",
        "\n",
        "# math functions class\n",
        "class neuralNetworkMathFunctions:\n",
        "  \"\"\"\n",
        "  Helper class to handle mathematical operations of neural network passes, specifically activations and loss calculation\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize math helper class for a neural network. Hyperparameter object can be the same as given to neural network\n",
        "    \"\"\"\n",
        "    # Hyperparameters are initialized separately by the parent neural network\n",
        "\n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set mathematical hyperparameters of neural network\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "class neuralNetwork:\n",
        "  \"\"\"\n",
        "  Class for a neural network made up of multiple layers of perceptrons\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters and hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # create empty math functions object\n",
        "    self.fns = neuralNetworkMathFunctions()\n",
        "\n",
        "    # assign basic hyperparameters to neural network and math functions object\n",
        "    self.hyperparams = {}\n",
        "    self.setHyperparameters(hyperparams)\n",
        "\n",
        "    # initialize the weight and bias matrices of the NN\n",
        "    self.initModel(hyperparams)\n",
        "  \n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # change values of only the hyperparameters specified in the input variable\n",
        "    self.hyperparams.update(hp)\n",
        "    \n",
        "    # use member variables for commonly used hyperparameters\n",
        "    self.layerSizes       = self.hyperparams[\"layerSizes\"]\n",
        "    self.batchSize        = self.hyperparams[\"batchSize\"]\n",
        "    self.learningRate     = self.hyperparams[\"learningRate\"]\n",
        "    self.epochs           = self.hyperparams[\"epochs\"]\n",
        "    self.numLayers        = len(self.layerSizes) - 1\n",
        "    \n",
        "    # set math functions object hyperparameters\n",
        "    assert len(self.hyperparams[\"activations\"])==self.numLayers, \"number of layers (%d) and number of activations (%d) don't match\"%(self.numLayers,len(hp[\"activations\"]))\n",
        "    self.activations = self.hyperparams[\"activations\"]\n",
        "    self.lossFn = self.hyperparams[\"lossFn\"]\n",
        "    self.regparam = self.hyperparams[\"regparam\"]\n",
        "\n",
        "  def initModel(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters (weight and bias matrices) of neural network\n",
        "    \"\"\"\n",
        "    # checking bounds arg\n",
        "    bounds = (0,1)\n",
        "    if \"initWeightBounds\" in hyperparams.keys():\n",
        "      assert len(hyperparams[\"initWeightBounds\"])==2, \"bounds arg has to be a list/tuple of 2 numbers\"\n",
        "      bounds = hyperparams[\"initWeightBounds\"]\n",
        "\n",
        "    # create list of weight matrices and bias vectors\n",
        "    # the goal is to make the indexing same as that in lecture derivation, hence the dummy values\n",
        "    self.wmat = [np.array([1],ndmin=2)]\n",
        "    self.bias = [np.array([1],ndmin=2)]\n",
        "    \n",
        "    # create random initial parameters and append them to the above initialized lists\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      self.wmat.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],self.layerSizes[i-1])+bounds[0])\n",
        "      self.bias.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],1)+bounds[0])\n",
        "\n",
        "  def activation(self,layerNum,x):\n",
        "    \"\"\"\n",
        "    Compute and return activation values for a given layer and its sum values\n",
        "    \"\"\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "      return 1/(1+np.exp(-x))\n",
        "    elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "      z = np.exp(x)\n",
        "      return z/np.sum(z)\n",
        "    elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "      return (x>=0)+0\n",
        "    elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "      return np.maximum(x,0)\n",
        "    elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "      return np.tanh(x)\n",
        "  \n",
        "  def activationDerivative(self,layerNum,**kwargs):\n",
        "    \"\"\"\n",
        "    Compute and return activation derivative values for a given layer and its sum or output values depending on the given argument\n",
        "    \"\"\"\n",
        "    assert ( len(kwargs.keys())==1 and np.any([_ in kwargs.keys() for _ in [\"x\",\"y\"]]) ), \"activationDerivative argument malformed. \\\n",
        "    Use activationDerivative(layerNum,x=x_val) or activationDerivative(layerNum,y=y_val)\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    \n",
        "    if \"y\" in kwargs.keys():\n",
        "      y = kwargs[\"y\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (y>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "        return 1/y\n",
        "    else:\n",
        "      x = kwargs[\"x\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        z = np.exp(x)\n",
        "        s = np.sum(z)\n",
        "        return z*(s-z)/(s**2)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (x>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return 1/np.tanh(x)\n",
        "\n",
        "  def loss(self,outputData,targetData):\n",
        "    modW = np.sum( np.array( [ np.linalg.norm(W) for W in self.wmat ] ) )\n",
        "    modB = np.sum( np.array( [ np.linalg.norm(B) for B in self.bias ] ) )\n",
        "    modtheta_sq = (modW + modB)**2\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return 0.5*np.sum(np.linalg.norm(outputData-targetData, axis=0)**2) + 0.5*self.regparam*modtheta_sq\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      return - np.sum(targetData * np.log(outputData)) + 0.5*self.regparam*modtheta_sq\n",
        "  \n",
        "  def lossOutputDerivative(self,outputData,targetData):\n",
        "    \"\"\"\n",
        "    Compute and return loss derivatives for given output and target data\n",
        "    \"\"\"\n",
        "    print('outputData = ',outputData)\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return outputData-targetData\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      return -targetData / outputData \n",
        "  \n",
        "  def forwardPass(self, inputData):\n",
        "    \"\"\"\n",
        "    Compute output activations of all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                              # --- PSEUDOCODE ---\n",
        "    h     = inputData                              # h[0] = x\n",
        "    hData = [h]                                    #\n",
        "    datasetSize = np.shape(inputData)[1]           #\n",
        "    #                                              #\n",
        "    for i in range(1,self.numLayers+1):            # for i from 1 to L:\n",
        "      a   = self.wmat[i] @ h + self.bias[i]        #     a[i] = w[i] @ h[i-1] + b[i]\n",
        "      h   = self.activation(i,a)               #     h[i] = f(a[i])\n",
        "      hData.append(h)\n",
        "    \n",
        "    return hData\n",
        "  \n",
        "  def backwardPass(self, layerwiseOutputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute weight and bias gradients for all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                                                                        # --- PSEUDOCODE ---\n",
        "    lossData    = self.lossOutputDerivative(layerwiseOutputData[-1], targetData)             # loss_derivative = d(loss)/dh[L]\n",
        "    Delta       = lossData                                                                   # Delta[L] = loss_derivative\n",
        "    datasetSize = np.shape(targetData)[1]                                                    #\n",
        "    biasInputs  = np.array(np.ones(datasetSize),ndmin=2).T                                   #\n",
        "    gradW       = []                                                                         #\n",
        "    gradB       = []                                                                         #\n",
        "    #                                                                                        #\n",
        "    for iFwd in range(self.numLayers):                                                       # for i from L to 1:\n",
        "      i            = self.numLayers - iFwd                                                   #     // index correction\n",
        "      stocBiasCorr = self.activationDerivative(i,y=layerwiseOutputData[i]) * Delta       #     stochastic_bias_corrections = f'(a[i]) * Delta[i]\n",
        "      gW           = stocBiasCorr @ layerwiseOutputData[i-1].T                               #     grad(W[i]) = stochastic_bias_corrections x (h[i-1]).T\n",
        "      gB           = stocBiasCorr @ biasInputs                                               #     grad(b[i]) = sum(stochastic_bias_corrections)\n",
        "      Delta        = self.wmat[i].T @ stocBiasCorr                                           #     Delta[i-1] = W[i] x stochastic_bias_corrections\n",
        "      \n",
        "      gradW.append(gW)\n",
        "      gradB.append(gB)\n",
        "    \n",
        "    # dummy element and order handling\n",
        "    gradW.append(np.array([0],ndmin=2))\n",
        "    gradW.reverse()\n",
        "    gradB.append(np.array([0],ndmin=2))\n",
        "    gradB.reverse()\n",
        "    \n",
        "    return (gradW,gradB)\n",
        "  \n",
        "  def infer(self,inputData,**kwargs):\n",
        "    \"\"\"\n",
        "    Perform inference on input dataset using the neural network\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    \n",
        "    # perform forward pass and return last-layer outputs\n",
        "    return self.forwardPass(inputData)[-1]\n",
        "\n",
        "  def gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex):##\n",
        "    # create data batches\n",
        "    startIndex  = batchSize * batchIndex\n",
        "    endIndex    = min(startIndex + batchSize, datasetSize)\n",
        "    inputBatch  = inputData[:,startIndex:endIndex]\n",
        "    targetBatch = targetData[:,startIndex:endIndex]\n",
        "    # perform forward and backward passes to compute gradients\n",
        "    layerwiseOutputData = self.forwardPass(inputBatch)\n",
        "    print(\"fwd_done\")\n",
        "    (gradW, gradB)      = self.backwardPass(layerwiseOutputData,targetBatch)\n",
        "    return gradW, gradB ##\n",
        "\n",
        "  def sgd(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    # Initialise loss and error lists\n",
        "    loss = [] ##\n",
        "    error = [] ##\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):       \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        y_out = self.infer(inputData, colwiseData =True)\n",
        "        loss.append(self.loss(y_out, targetData)) ##\n",
        "        error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -self.learningRate * gradW[i]\n",
        "          self.bias[i] += -self.learningRate * gradB[i]\n",
        "    y_out = self.infer(inputData, colwiseData =True)\n",
        "    loss.append(self.loss(y_out, targetData)) ##\n",
        "    error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "    return loss, error ##\n",
        "\n",
        "  def momentumGD(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    loss = [] ##\n",
        "    error = [] ##\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        y_out = self.infer(inputData, colwiseData =True)\n",
        "        loss.append(self.loss(y_out, targetData)) ##\n",
        "        error.append(1 - np.count_nonzero( np.all(targetData[np.argmax(y_out)]==1, axis=0) )/len(targetData)) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -update_w[i]\n",
        "          self.bias[i] += -update_b[i]\n",
        "    y_out = self.infer(inputData, colwiseData =True)\n",
        "    loss.append(self.loss(y_out, targetData)) ##\n",
        "    error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "    return loss, error ##\n",
        "\n",
        "  def NAG(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    loss = [] ##\n",
        "    error = [] ##\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        # perform look ahead parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -gamma*update_w[i]\n",
        "          self.bias[i] += -gamma*update_b[i]\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        y_out = self.infer(inputData, colwiseData =True)\n",
        "        loss.append(self.loss(y_out, targetData)) ##\n",
        "        error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -eta*gradW[i]\n",
        "          self.bias[i] += -eta*gradW[i]\n",
        "    y_out = self.infer(inputData, colwiseData =True)\n",
        "    loss.append(self.loss(y_out, targetData)) ##\n",
        "    error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "    return loss, error ##\n",
        "\n",
        "  def rmsprop(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    beta = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #Initialise\n",
        "    loss = [] ##\n",
        "    error = [] ##\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):       \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        y_out = self.infer(inputData, colwiseData =True)\n",
        "        loss.append(self.loss(y_out, targetData)) ##\n",
        "        error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          v_w[i] = beta*v_w[i] + (1-beta)*gradW[i]**2\n",
        "          v_b[i] = beta*v_b[i] + (1-beta)*gradB[i]**2 \n",
        "          self.wmat[i] += -eta * (v_w[i] + epsilon)**-0.5 * gradW[i]\n",
        "          self.bias[i] += -eta * (v_b[i] + epsilon)**-0.5 * gradB[i]\n",
        "    y_out = self.infer(inputData, colwiseData =True)\n",
        "    loss.append(self.loss(y_out, targetData)) ##\n",
        "    error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "    return loss, error ##\n",
        "  \n",
        "  def adam(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    loss = [] ##\n",
        "    error = [] ##\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        y_out = self.infer(inputData, colwiseData =True)\n",
        "        loss.append(self.loss(y_out, targetData)) ##\n",
        "        error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = m_w[i]/(1-beta_1**t)\n",
        "          m_b_hat = m_b[i]/(1-beta_1**t)\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "    y_out = self.infer(inputData, colwiseData =True)\n",
        "    loss.append(self.loss(y_out, targetData)) ##\n",
        "    error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "    return loss, error ##\n",
        "\n",
        "  def nadam(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    loss = [] ##\n",
        "    error = [] ##\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        y_out = self.infer(inputData, colwiseData =True)\n",
        "        loss.append(self.loss(y_out, targetData)) ##\n",
        "        error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = (beta_1/(1-beta_1**(t+1)))*m_w[i] + ((1-beta_1)/(1-beta_1**t))*gradW[i]\n",
        "          m_b_hat = (beta_1/(1-beta_1**(t+1)))*m_b[i] + ((1-beta_1)/(1-beta_1**t))*gradB[i]\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "    y_out = self.infer(inputData, colwiseData =True)\n",
        "    loss.append(self.loss(y_out, targetData)) ##\n",
        "    error.append(1 - np.count_nonzero( np.all(y_out == targetData, axis=0) )/len(targetData)) ##\n",
        "    return loss, error ##\n",
        "\n",
        "\n",
        "  def train(self, inputData, targetData, **kwargs):\n",
        "    \"\"\"\n",
        "    Train the network on the given input and target datasets\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input and target dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    targetData = np.array(targetData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "      targetData = targetData.T\n",
        "    assert np.shape(inputData)[1]==np.shape(targetData)[1], \"input and target datasets have different dataset sizes\"\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    assert np.shape(targetData)[0]==self.layerSizes[-1], \"size of target datapoint differs from size of target vector given as hyperparameter\"\n",
        "    datasetSize = np.shape(targetData)[1]\n",
        "\n",
        "    # calculate batch parameters\n",
        "    batchSize = datasetSize if self.batchSize==-1 else self.batchSize\n",
        "    numBatches = int(np.ceil(datasetSize / batchSize))\n",
        "\n",
        "    if self.hyperparams[\"optimizer\"] == GDOPT_NONE:\n",
        "      (loss, error) = self.sgd(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_MOMENTUM:\n",
        "      (loss, error) = self.momentumGD(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NESTEROV:\n",
        "      (loss, error) = self.NAG(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_RMSPROP:\n",
        "      (loss, error) = self.rmsprop(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_ADAM:\n",
        "      (loss, error) = self.adam(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NADAM:\n",
        "      (loss, error) = self.nadam(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    \n",
        "    return np.array(loss), np.array(error)\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8x1hv7hiXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ee70fd-48bf-4920-f649-090ccdd44ecb"
      },
      "source": [
        "hyp = {\n",
        "    \"layerSizes\": [2,4,1],\n",
        "    \"batchSize\": 1,\n",
        "    \"learningRate\": 1,\n",
        "    \"epochs\": 500,\n",
        "    \"activations\": [ACTIVATION_SIGMOID, ACTIVATION_SIGMOID],\n",
        "    \"lossFn\": LOSS_SQERROR,\n",
        "    \"initWeightBounds\": (-0.1,0.1),\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    \"reg param\":0.1\n",
        "}\n",
        "\n",
        "x = neuralNetwork(hyp)\n",
        "inp = np.array([[1,0.5],[-0.5,0.25],[1,2]])\n",
        "tar = np.array([[0.5],[0.75],[0.67]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target data:\n",
            "[[0.5  0.75 0.67]]\n",
            "Output before training:\n",
            "[[0.52630128 0.52657921 0.52605094]]\n",
            "Performing training now\n",
            "Output after training for 500 epochs with learning rate of 1.00:\n",
            "[[0.59370377 0.7462625  0.5936988 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nLeUVg2YFbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd7a9ae-bea1-4483-9a6d-40c34aa6a85a"
      },
      "source": [
        "x.setHyperparameters({\n",
        "    \"learningRate\":0.2,\n",
        "    \"epochs\":5000,\n",
        "    \"activations\":[ACTIVATION_RELU, ACTIVATION_RELU]\n",
        "    # square error loss will work as Hamming distance in this case\n",
        "})\n",
        "inp = np.array([[0,0],[0,1],[1,0],[1,1],[0,0],[0,1],[1,0],[1,1]])\n",
        "tar = np.array([[0],[0],[0],[1],[0],[0],[0],[1]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target data:\n",
            "[[0 0 0 1 0 0 0 1]]\n",
            "Output before training:\n",
            "[[0. 0. 0. 1. 0. 0. 0. 1.]]\n",
            "Performing training now\n",
            "Output after training for 5000 epochs with learning rate of 0.20:\n",
            "[[0. 0. 0. 1. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pswfmu-9fdLW"
      },
      "source": [
        "#Reshaping the 'x' data:\r\n",
        "len_1D = x_train.shape[1]*x_train.shape[2]\r\n",
        "x_train_1D = np.array( [x.reshape(len_1D) for x in x_train] )\r\n",
        "x_test_1D  = np.array( [x.reshape(len_1D) for x in x_test] )\r\n",
        "\r\n",
        "#Transforming 'y' data - changing scalar i to vector e(i)\r\n",
        "y_train_1D = np.zeros( (len(y_train), len(classes)) )\r\n",
        "for i in range(len(y_train)):\r\n",
        "  y_train_1D[i, y_train[i]] = 1\r\n",
        "y_test_1D = np.zeros( (len(y_test), len(classes)) )\r\n",
        "for i in range(len(y_test)):\r\n",
        "  y_test_1D[i, y_train[i]] = 1\r\n",
        "\r\n",
        "frac_val = 0.1\r\n",
        "all_idx = np.arange(len(x_train))\r\n",
        "val_idx = np.random.choice(all_idx, int(frac_val*len(x_train)), replace=False)\r\n",
        "tr2_idx = np.array([i for i in all_idx if i not in val_idx])\r\n",
        "\r\n",
        "x_train2 = x_train_1D[tr2_idx]\r\n",
        "y_train2 = y_train_1D[tr2_idx]\r\n",
        "x_val = x_train_1D[val_idx]\r\n",
        "y_val = y_train_1D[val_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnAlQs3YyE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afadf0e3-3d6e-41d3-f46b-3dc86b3f7fe8"
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwamUIcFrJ40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a002a412-aa86-4dce-bfcb-5afef9f92c35"
      },
      "source": [
        "hyp = {\r\n",
        "    \"layerSizes\": [len(x_train_1D[0]),len(y_train_1D[0])],\r\n",
        "    \"batchSize\": 32,\r\n",
        "    \"learningRate\": 1e-3,\r\n",
        "    \"epochs\": 1,\r\n",
        "    \"activations\": [ ACTIVATION_SOFTMAX],\r\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\r\n",
        "    \"initWeightBounds\": (-1,1),\r\n",
        "    \"optimizer\": GDOPT_ADAM,\r\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\r\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\r\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\r\n",
        "    \"regparam\": 0.1\r\n",
        "}\r\n",
        "\r\n",
        "#wandb.config.update(hyp)\r\n",
        "\r\n",
        "NN = neuralNetwork(hyp)\r\n",
        "(val_loss, val_error) = NN.train(x_train2[:256]/255, y_train2[:256]/255)\r\n",
        "#(loss_lst, error_lst) = NN.train(x_train_1D, y_train_1D)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fwd_done\n",
            "outputData =  [[5.20431386e-13 5.61976245e-15 8.95957884e-14 1.08270726e-14\n",
            "  1.41145478e-15 7.43604018e-22 7.61884184e-09 1.77008515e-10\n",
            "  2.19728599e-15 4.01667791e-18 5.23558335e-11 3.26917215e-12\n",
            "  4.30163885e-13 6.41721083e-14 4.77727915e-15 2.33503694e-17\n",
            "  1.24123910e-13 3.78814268e-18 1.21765977e-16 2.93440900e-18\n",
            "  2.96952798e-12 1.86869260e-19 2.09665892e-16 5.78286734e-14\n",
            "  2.97934298e-20 1.43803597e-17 4.72554865e-21 3.06118498e-13\n",
            "  7.39710824e-15 2.17409927e-15 4.86737414e-13 2.48267638e-13]\n",
            " [1.58282884e-08 1.22576206e-09 7.19073104e-11 2.52936170e-09\n",
            "  6.94520486e-13 3.31107459e-12 1.86757013e-11 1.15531198e-10\n",
            "  1.12830649e-09 6.45779795e-08 9.55279893e-12 1.10031642e-13\n",
            "  3.01680868e-11 2.82017590e-11 2.79175808e-10 1.11682405e-10\n",
            "  1.63727022e-12 3.03663859e-10 6.25444584e-10 1.98605226e-08\n",
            "  8.94089538e-11 2.21704286e-09 1.17265234e-10 1.40323843e-07\n",
            "  8.74556554e-12 9.44758277e-11 3.65604447e-10 1.33591387e-13\n",
            "  3.44074813e-11 7.62607857e-10 4.08378275e-11 5.41482069e-11]\n",
            " [8.77261991e-15 4.78508851e-17 2.39549000e-14 4.46241548e-15\n",
            "  9.54846312e-13 1.02006240e-14 8.62229233e-14 5.32490730e-15\n",
            "  2.53433215e-17 2.49713804e-17 1.56245788e-10 1.92585770e-11\n",
            "  4.19284426e-11 3.19376164e-17 7.85029542e-17 2.40066015e-15\n",
            "  7.82558438e-12 6.58564739e-18 8.02062865e-15 9.17259102e-15\n",
            "  1.47494304e-15 2.23496985e-15 7.07413849e-17 1.18263490e-14\n",
            "  3.49507455e-16 8.68222570e-13 6.48345521e-17 1.84139151e-12\n",
            "  7.61973435e-15 1.69558871e-16 9.62670298e-13 2.50433198e-13]\n",
            " [6.14283828e-10 9.81642871e-01 1.56368054e-07 1.05067331e-06\n",
            "  7.10668440e-11 1.49004665e-05 4.09747818e-10 3.96629773e-13\n",
            "  1.79870337e-03 8.64065613e-10 2.93939679e-14 1.54587171e-09\n",
            "  6.14087059e-12 6.90805018e-05 8.08581759e-04 1.18950299e-07\n",
            "  9.99076158e-08 5.64835329e-05 3.91959491e-04 1.18302195e-06\n",
            "  5.24747270e-08 1.19952898e-04 1.35718141e-03 6.47540009e-07\n",
            "  1.33401699e-02 2.33507884e-07 2.84482055e-05 3.91270244e-10\n",
            "  2.06528388e-08 3.60305864e-07 7.39234563e-10 3.35055317e-09]\n",
            " [2.33586669e-11 3.18334403e-13 5.96024257e-11 6.16217819e-12\n",
            "  6.85915617e-11 1.30633635e-18 7.91146738e-12 3.02279206e-11\n",
            "  1.64409299e-11 3.51879052e-15 3.50821621e-12 2.16837121e-10\n",
            "  2.68690255e-12 1.23418196e-11 8.33970563e-11 4.34336575e-14\n",
            "  4.45947571e-12 1.79111634e-12 1.66100423e-09 2.27746174e-10\n",
            "  2.79707054e-12 2.09911290e-14 5.23875781e-10 2.29945269e-10\n",
            "  1.30165242e-15 2.26958687e-13 2.08287936e-14 2.01454018e-11\n",
            "  1.36089402e-10 2.62974563e-12 2.47875121e-12 1.91712953e-11]\n",
            " [1.18412745e-11 1.12187453e-13 4.21474632e-11 2.73972690e-11\n",
            "  4.37712544e-12 1.28707718e-11 4.79881660e-10 5.07152876e-10\n",
            "  1.10652577e-12 2.35838141e-14 7.70078281e-11 4.15421396e-13\n",
            "  1.79917088e-11 1.80250072e-09 1.09329148e-11 8.38759288e-10\n",
            "  1.83360690e-11 1.60956486e-11 3.00282218e-11 3.29055824e-11\n",
            "  4.56518972e-13 3.24015286e-11 1.21691573e-13 2.30647211e-15\n",
            "  1.53241877e-13 4.80174902e-12 1.30829030e-11 3.00016501e-11\n",
            "  2.51161271e-11 1.77199962e-10 4.94201299e-12 7.18355220e-12]\n",
            " [6.47354379e-14 4.60412280e-15 3.75994663e-14 2.52591427e-14\n",
            "  1.08943008e-14 8.79339466e-16 6.67479537e-09 1.94793506e-07\n",
            "  3.03095449e-14 2.41280152e-15 2.27383748e-12 2.56340003e-12\n",
            "  1.57229896e-13 6.50373991e-18 2.82546416e-15 4.48603753e-15\n",
            "  1.44105697e-13 1.27610287e-16 1.41462876e-16 1.15307654e-16\n",
            "  9.46040685e-14 1.22684232e-15 1.51283225e-16 3.30884186e-13\n",
            "  7.95723702e-18 6.05547913e-16 4.69204564e-15 3.67443422e-13\n",
            "  3.21838400e-14 7.84625746e-14 6.92529617e-11 9.01978173e-12]\n",
            " [1.99519134e-14 3.98929655e-13 1.53461347e-11 2.42266265e-11\n",
            "  2.19499413e-11 4.29330969e-13 4.01848223e-13 4.21266322e-12\n",
            "  1.13737248e-10 3.70643845e-12 1.12962025e-13 1.13352182e-10\n",
            "  1.30201502e-16 1.24204159e-13 7.81619294e-13 7.68927744e-13\n",
            "  1.72770793e-12 1.99185603e-12 7.39579071e-13 1.62231264e-12\n",
            "  2.48395949e-13 2.54656170e-15 1.17042677e-13 3.46109235e-11\n",
            "  6.52656905e-13 3.16487115e-14 2.06041874e-13 5.57142435e-11\n",
            "  4.89870790e-12 2.11563199e-14 7.85134945e-12 1.04010166e-11]\n",
            " [3.44460623e-09 5.94666537e-14 2.66194885e-09 2.15643220e-10\n",
            "  2.96167521e-11 4.90604210e-10 8.18225563e-10 2.39253078e-12\n",
            "  2.10704372e-09 3.66658100e-04 2.89568874e-12 6.14734549e-10\n",
            "  1.00644364e-10 1.47161939e-09 1.04001612e-09 2.76654158e-10\n",
            "  3.26302822e-10 3.39977249e-07 1.43263601e-07 2.61664045e-08\n",
            "  1.11045138e-10 5.29913512e-11 1.27405648e-09 2.46201552e-10\n",
            "  6.63803625e-09 2.71252174e-10 1.73354936e-08 2.95803313e-09\n",
            "  2.06175352e-08 1.97106249e-08 1.39109873e-11 3.04678660e-12]\n",
            " [2.33927119e-15 5.71977907e-13 4.18972438e-10 6.29423410e-12\n",
            "  2.11761548e-13 2.93012512e-09 2.85294747e-14 2.26276954e-14\n",
            "  7.11067606e-11 5.46763173e-14 1.22108154e-13 2.42349172e-12\n",
            "  1.42251793e-13 7.67889523e-09 7.87742168e-11 3.00352799e-12\n",
            "  6.82249297e-11 3.50353649e-10 4.69650966e-09 7.77613525e-11\n",
            "  7.37181537e-11 5.40723311e-12 4.01702870e-10 4.93538525e-13\n",
            "  3.09972292e-13 4.61465617e-11 4.71659422e-12 1.34275887e-11\n",
            "  7.24437736e-10 2.78912384e-12 1.59942626e-11 1.77143490e-11]]\n",
            "fwd_done\n",
            "outputData =  [[5.85241227e-14 6.27552394e-11 6.99006141e-15 1.43118308e-18\n",
            "  9.20477267e-17 2.42692102e-10 1.50255346e-14 1.03411498e-11\n",
            "  1.08829534e-15 2.41378920e-13 1.39126903e-15 1.46149350e-14\n",
            "  1.67649322e-16 1.78135102e-14 9.25269274e-13 2.06209963e-18\n",
            "  1.65384485e-13 1.86784650e-14 3.10379190e-15 2.42457010e-13\n",
            "  1.06815428e-16 1.01150206e-15 2.88599910e-11 2.04583552e-09\n",
            "  5.20817118e-11 2.00335846e-15 4.70069997e-17 3.60826140e-14\n",
            "  5.81890581e-14 1.08723625e-13 4.27760422e-14 1.11719777e-16]\n",
            " [9.95673526e-10 6.94043580e-07 3.33926863e-09 3.65962960e-08\n",
            "  7.22277736e-09 5.71891149e-11 7.40558464e-10 1.19059896e-09\n",
            "  2.03726925e-09 3.44005439e-12 2.82745929e-07 5.97411481e-08\n",
            "  7.41865602e-09 1.05934996e-08 8.66396819e-12 6.25538220e-10\n",
            "  5.33461407e-09 9.98918006e-06 1.80599875e-08 3.31658901e-09\n",
            "  1.56360176e-08 3.76113199e-09 4.64480525e-11 5.08224448e-09\n",
            "  6.63573502e-10 3.88435138e-06 2.67789654e-08 1.10428706e-06\n",
            "  8.21179415e-06 4.60272310e-09 4.59569211e-10 8.50553796e-07]\n",
            " [1.15124009e-13 3.65178064e-13 3.66579618e-13 1.06597870e-14\n",
            "  2.32133792e-13 8.81396163e-11 1.94223237e-09 1.51191544e-11\n",
            "  2.52870176e-12 1.12945137e-10 3.89935770e-15 8.49204651e-15\n",
            "  1.12223362e-14 1.42783999e-14 5.29636780e-09 4.85276279e-14\n",
            "  2.00426191e-12 2.06103375e-14 3.37971193e-12 5.81475134e-15\n",
            "  1.71742245e-14 8.70399589e-12 2.39729336e-13 2.07229674e-09\n",
            "  2.75847156e-11 3.10929573e-16 3.92241547e-14 1.66586688e-15\n",
            "  3.25360581e-15 1.11058629e-15 4.66254043e-13 4.42609361e-15]\n",
            " [2.58911073e-07 2.80503460e-08 6.76975157e-03 8.83035037e-03\n",
            "  3.85043726e-02 1.83419391e-08 6.26175143e-09 4.50633844e-07\n",
            "  1.52425151e-04 1.40329495e-08 8.08842707e-03 5.41678477e-03\n",
            "  6.62461298e-03 7.21545113e-03 8.71581481e-11 1.45934187e-01\n",
            "  1.18725007e-06 9.05670354e-02 3.84010458e-04 2.85500998e-05\n",
            "  2.34229433e-03 1.33536533e-05 5.22712315e-09 5.79622243e-13\n",
            "  4.41053545e-09 1.94472741e-03 1.52984109e-03 5.74234130e-01\n",
            "  4.32581053e-06 2.90259516e-04 6.81518018e-05 1.00721141e-01]\n",
            " [4.47078298e-10 1.96479310e-10 1.56046332e-06 3.00573738e-13\n",
            "  2.65618712e-13 7.72373004e-11 1.33192023e-10 9.19636141e-09\n",
            "  2.34649122e-12 1.33268297e-10 1.29424429e-09 1.74552544e-08\n",
            "  1.41232647e-09 4.78147432e-11 3.10337623e-10 7.81821554e-14\n",
            "  9.68145391e-10 2.65222113e-10 1.15852524e-11 1.01202484e-09\n",
            "  1.82491862e-08 1.41684601e-06 5.48908204e-08 9.55202046e-11\n",
            "  6.17794223e-10 2.15493973e-09 1.79361427e-11 2.89397102e-09\n",
            "  6.18402744e-08 9.24607735e-12 2.52808173e-08 3.20617591e-12]\n",
            " [6.50661956e-11 1.16798500e-11 4.83433924e-09 1.73424722e-11\n",
            "  3.50634319e-10 1.11193578e-10 2.93408988e-09 2.71408340e-09\n",
            "  5.70348451e-10 3.91543222e-10 3.06905760e-10 1.36099553e-09\n",
            "  7.75076713e-11 3.97935600e-10 1.77315062e-10 3.95790708e-08\n",
            "  2.02477787e-09 1.73902321e-11 1.27897285e-09 1.66807359e-11\n",
            "  8.04000880e-11 1.46878513e-11 1.84359095e-10 4.09451147e-09\n",
            "  3.06812218e-11 2.89655431e-12 1.01853401e-10 8.65230981e-13\n",
            "  6.85943755e-14 5.76475809e-11 4.33095414e-10 3.44158243e-10]\n",
            " [6.35201835e-12 7.02330841e-11 2.20250798e-14 8.17214861e-15\n",
            "  3.34511547e-14 1.83309662e-11 8.42943528e-14 6.45923472e-09\n",
            "  1.65557734e-14 1.47706318e-12 3.16142797e-15 3.34358548e-17\n",
            "  7.86677446e-15 7.17975416e-16 4.62533899e-13 3.77369531e-17\n",
            "  1.47117813e-11 3.39263574e-13 2.09251515e-14 7.37408170e-14\n",
            "  4.48126627e-16 2.18560827e-14 2.92809834e-09 1.83669008e-11\n",
            "  3.88280871e-12 8.40515979e-13 1.43064981e-13 2.38373847e-12\n",
            "  2.74110403e-11 1.56986788e-13 1.65902166e-14 3.37352940e-15]\n",
            " [7.36128405e-08 2.73241473e-08 2.74013505e-10 2.55770590e-14\n",
            "  8.16839498e-13 1.68227976e-10 5.91730508e-13 3.04851626e-13\n",
            "  1.65020585e-11 3.59537523e-10 1.05520546e-11 3.78656793e-15\n",
            "  1.82925692e-10 2.15104554e-09 2.27764075e-11 2.64521487e-15\n",
            "  4.41860803e-11 2.23287095e-11 4.08091981e-12 1.56196529e-10\n",
            "  9.10667416e-11 3.94248541e-10 4.46830262e-09 1.06849596e-12\n",
            "  1.53945752e-09 3.81029695e-09 5.38390205e-10 3.80332225e-11\n",
            "  9.58236643e-12 1.86842758e-10 4.59776459e-11 4.16828100e-12]\n",
            " [2.56328356e-07 1.81584237e-06 4.06698624e-08 1.91220367e-09\n",
            "  1.98928092e-08 2.01432633e-10 5.29330651e-12 1.63769737e-04\n",
            "  1.38519996e-08 6.71089196e-09 3.99356132e-08 1.22418036e-08\n",
            "  6.62344793e-06 4.31559652e-05 2.79803102e-09 8.18492103e-08\n",
            "  2.50735616e-10 3.63626181e-09 1.56057391e-09 8.28689960e-08\n",
            "  5.89970073e-05 2.78252568e-06 1.53503298e-09 1.71313429e-11\n",
            "  6.54375124e-10 1.51922055e-07 4.72810245e-08 4.04789271e-10\n",
            "  7.75111404e-07 2.30062011e-05 1.46316038e-07 2.37621925e-06]\n",
            " [5.43006001e-11 3.46184533e-11 6.86176462e-08 1.04039210e-09\n",
            "  2.24132283e-08 3.75239857e-12 8.99785085e-15 5.65079513e-11\n",
            "  7.00468767e-11 7.38078821e-12 3.45922038e-08 6.71663647e-10\n",
            "  5.84176977e-07 6.70927315e-09 7.46671744e-12 2.32352931e-10\n",
            "  2.27469213e-09 4.52659865e-10 1.11007081e-08 2.59148293e-11\n",
            "  7.48642915e-08 7.15868834e-09 9.37461961e-16 4.10852511e-12\n",
            "  9.10336399e-10 4.50914451e-10 2.02128364e-10 2.61508420e-09\n",
            "  2.37449191e-08 2.10436804e-11 1.79351135e-07 1.92794415e-11]]\n",
            "fwd_done\n",
            "outputData =  [[9.79280328e-14 7.34793947e-14 6.19149509e-16 2.62539308e-14\n",
            "  1.09667275e-17 2.04508627e-19 7.67438905e-12 1.29280662e-14\n",
            "  7.65875056e-12 2.03756371e-14 6.79497388e-13 7.01614259e-09\n",
            "  1.69794104e-13 8.97419136e-10 1.60328899e-14 9.41373784e-13\n",
            "  3.96506151e-13 2.57784376e-13 9.81932475e-11 1.27843717e-12\n",
            "  1.71151147e-12 6.39116047e-15 4.28846026e-16 5.93674589e-12\n",
            "  4.02382182e-13 1.66934063e-12 2.94713077e-14 3.92026756e-13\n",
            "  3.29336035e-11 3.16526611e-15 5.37398307e-12 2.70770665e-18]\n",
            " [3.55009534e-10 1.63639900e-09 8.75174575e-08 1.73168773e-08\n",
            "  3.74855933e-06 1.46905262e-08 5.94456331e-08 1.22176288e-10\n",
            "  2.15775332e-08 2.96222288e-09 1.04323365e-09 6.57583431e-10\n",
            "  3.67796913e-11 1.24564639e-06 1.31663287e-08 2.85707101e-11\n",
            "  2.07625968e-09 4.54065250e-11 4.48201660e-09 7.54797468e-11\n",
            "  3.27499941e-12 1.27017459e-06 2.95892388e-08 8.35140281e-10\n",
            "  9.94292520e-09 3.99478406e-09 5.02107110e-08 2.60500141e-09\n",
            "  8.41924927e-07 4.15655397e-08 2.77264753e-09 2.61650157e-13]\n",
            " [9.22064542e-13 2.55473813e-14 8.43204401e-15 1.56345974e-14\n",
            "  7.49782420e-15 1.45017083e-13 9.42740936e-12 5.82251255e-12\n",
            "  9.55314025e-10 6.11239783e-13 2.07879389e-13 4.08801387e-10\n",
            "  3.75680622e-09 2.48521226e-13 4.01145390e-14 7.73951113e-10\n",
            "  7.70299842e-15 6.38609919e-09 9.87927327e-13 3.66600769e-13\n",
            "  6.01143491e-10 3.40918296e-15 5.13911699e-13 1.37793703e-11\n",
            "  1.44924651e-12 1.84348223e-14 4.80690006e-10 2.96013266e-13\n",
            "  1.99731565e-14 9.89770024e-13 1.62545226e-13 3.48285793e-08]\n",
            " [5.43738838e-04 2.68072015e-04 2.32623619e-03 6.05495087e-03\n",
            "  9.18154559e-01 4.90417089e-05 1.65296650e-03 7.81150792e-06\n",
            "  9.04797315e-07 4.66501140e-02 9.40944979e-06 1.39601790e-09\n",
            "  1.00819265e-09 3.64768819e-08 1.60752641e-02 1.00464649e-08\n",
            "  4.49185490e-06 2.30328237e-09 1.95039385e-07 1.05037272e-08\n",
            "  2.91729465e-10 4.23805123e-03 7.97940204e-07 5.10752944e-08\n",
            "  2.63225270e-04 8.55139771e-05 1.08444672e-07 1.53714104e-09\n",
            "  1.41049429e-06 4.09191483e-05 3.32344169e-06 3.12250069e-03]\n",
            " [5.30580362e-08 3.02708297e-10 3.13332994e-08 2.84707794e-06\n",
            "  5.99290162e-10 4.50399327e-11 2.07978180e-10 1.47250710e-07\n",
            "  2.53076991e-11 5.81317295e-09 4.72020355e-10 6.18906042e-11\n",
            "  1.21706137e-11 1.36833390e-11 9.29234320e-08 7.97941808e-10\n",
            "  4.00778467e-11 8.11325836e-12 1.77959788e-12 7.67857328e-11\n",
            "  1.43754398e-10 9.48509445e-10 1.68378716e-11 9.56022254e-11\n",
            "  2.11995490e-05 5.47988071e-10 1.08399071e-12 7.27237673e-12\n",
            "  2.57530928e-09 5.23547019e-06 4.81558518e-13 1.25951813e-11]\n",
            " [1.34567003e-09 2.56777712e-12 9.82586381e-11 9.32559263e-10\n",
            "  2.21735071e-10 1.00276371e-11 1.40722669e-13 6.35303352e-11\n",
            "  2.93349174e-12 2.72371366e-08 1.80151290e-10 2.34051638e-09\n",
            "  1.65385637e-10 3.80733347e-12 5.31689247e-10 1.50791873e-10\n",
            "  3.36450269e-10 6.82666978e-10 4.12671803e-09 1.77283790e-10\n",
            "  1.45388615e-10 2.26436365e-11 3.25673887e-10 3.81334297e-09\n",
            "  3.23599488e-09 1.26151691e-07 1.35387377e-12 6.31676976e-12\n",
            "  1.88122692e-11 3.45520402e-10 2.05786625e-09 2.92322306e-11]\n",
            " [9.59784853e-14 7.84630411e-11 3.52191148e-16 5.32040317e-14\n",
            "  1.13548385e-14 2.70430160e-13 4.49746927e-14 5.19978308e-14\n",
            "  2.86242974e-09 5.10411736e-14 5.61094706e-12 2.35404594e-08\n",
            "  8.61922878e-14 1.59090739e-09 3.15823304e-14 4.94465551e-12\n",
            "  2.42217998e-12 3.23110209e-13 1.12638057e-08 1.74630089e-11\n",
            "  2.77587084e-15 3.08251407e-15 2.05474991e-11 3.86296166e-12\n",
            "  3.57450604e-15 5.88682026e-14 2.73004761e-12 5.92103091e-12\n",
            "  5.42265603e-13 3.67648590e-15 5.64766867e-11 3.91146967e-15]\n",
            " [2.86346144e-10 1.20319666e-10 1.00843453e-10 3.11295589e-11\n",
            "  2.28398014e-11 2.16133788e-12 5.84182802e-11 3.22241188e-10\n",
            "  8.21666765e-13 1.30454975e-11 5.55888029e-11 6.36404668e-12\n",
            "  7.39368543e-12 1.53829017e-11 2.06898566e-11 2.76826487e-10\n",
            "  5.61929147e-11 5.68523520e-13 2.43338569e-11 4.21837802e-11\n",
            "  3.13581377e-14 1.93104150e-11 5.32045017e-13 1.04336592e-11\n",
            "  1.01559986e-09 3.35322351e-13 2.70372985e-12 4.22700981e-09\n",
            "  2.44823039e-11 1.02863207e-10 1.93786725e-12 3.82882751e-12]\n",
            " [7.18949179e-09 1.39446220e-08 3.30093806e-04 2.97807392e-08\n",
            "  8.44523341e-08 1.55472438e-07 4.46271286e-11 1.56226980e-08\n",
            "  3.19249876e-08 4.75732055e-07 1.77932629e-08 2.78708931e-08\n",
            "  1.50686850e-10 1.07858164e-07 1.52331203e-07 1.36284203e-09\n",
            "  4.66227437e-05 2.74268159e-11 4.36721136e-09 5.12659148e-09\n",
            "  5.66937024e-09 1.05997808e-08 2.61997067e-11 7.17497003e-10\n",
            "  2.54303353e-08 3.93376685e-08 3.00454380e-05 1.31725585e-09\n",
            "  1.42784121e-10 2.49292921e-07 1.83235958e-08 4.44366624e-12]\n",
            " [1.02876060e-08 1.57733789e-09 1.23952694e-07 5.87046002e-08\n",
            "  5.21162362e-12 2.50675344e-08 5.63639304e-12 1.58645855e-08\n",
            "  1.38116121e-12 2.68608795e-07 1.94972226e-09 1.23302899e-10\n",
            "  7.04670513e-13 8.34555471e-14 7.73407002e-09 5.07142776e-11\n",
            "  1.58220552e-09 3.95861219e-13 6.92309800e-12 5.35205751e-10\n",
            "  3.32797393e-13 2.88678214e-09 6.89438321e-11 1.03628276e-09\n",
            "  1.28201671e-09 4.71633323e-08 2.26767419e-13 1.79897442e-14\n",
            "  7.82072801e-13 9.39372146e-08 2.03185682e-10 4.86504130e-12]]\n",
            "fwd_done\n",
            "outputData =  [[4.08531339e-14 3.65760565e-13 6.07157123e-13 9.32928831e-16\n",
            "  2.06921365e-13 1.10489634e-11 6.33854140e-16 3.00808215e-13\n",
            "  7.84473786e-13 4.27990510e-12 2.58434313e-04 7.45187929e-14\n",
            "  5.18571646e-15 1.07234458e-12 1.46448803e-10 4.22200289e-11\n",
            "  2.18008118e-15 3.26293681e-14 1.36068152e-17 5.97996218e-13\n",
            "  1.97596245e-12 4.38955144e-17 1.57100510e-15 1.89788344e-12\n",
            "  2.86964304e-11 8.85648281e-12 2.56795043e-11 2.61362627e-13\n",
            "  6.19169063e-17 2.00245099e-13 6.17339236e-13 1.35242932e-12]\n",
            " [1.41058743e-08 8.76972558e-12 2.02962900e-10 1.30824579e-10\n",
            "  7.10665377e-15 5.61765251e-12 2.19283984e-08 2.09430369e-09\n",
            "  1.25411673e-09 1.10889139e-10 1.49751363e-08 1.08191838e-06\n",
            "  1.60369539e-10 4.15631792e-11 3.55969352e-10 2.94823117e-10\n",
            "  3.41127740e-08 5.65985049e-11 1.15961253e-08 4.15766288e-11\n",
            "  4.42510408e-09 3.42397975e-06 3.45939773e-07 5.76785987e-12\n",
            "  1.51678222e-13 2.99871754e-12 8.59329768e-12 4.69066513e-09\n",
            "  1.37672415e-09 9.86035838e-08 1.93261257e-10 1.63345097e-12]\n",
            " [9.36111612e-14 1.50708817e-11 2.04686251e-08 1.53230884e-10\n",
            "  2.94801407e-08 2.86887129e-11 1.18271175e-13 2.48347735e-13\n",
            "  1.55656845e-13 1.54648070e-11 2.22810516e-12 6.93054270e-15\n",
            "  2.94995164e-14 3.44578227e-10 1.93105232e-09 3.88293045e-11\n",
            "  8.57206155e-12 5.16564995e-10 1.15389649e-14 2.16424525e-09\n",
            "  6.29921187e-12 6.69527108e-15 1.68420043e-13 4.77728059e-11\n",
            "  5.50425034e-10 1.58112377e-10 6.21313381e-09 8.53458191e-13\n",
            "  1.23354850e-13 1.08631973e-14 5.76857707e-13 4.78193823e-12]\n",
            " [1.61239808e-01 5.31919026e-10 1.66430513e-11 8.08291188e-09\n",
            "  2.12883956e-07 1.11973263e-09 7.00067947e-04 3.45090587e-06\n",
            "  3.02736001e-06 6.37026056e-07 2.74100230e-11 1.86528597e-05\n",
            "  1.51037660e-04 1.85003550e-08 5.34023732e-11 1.86559062e-10\n",
            "  1.46811444e-09 8.62770380e-07 5.48035667e-01 1.31882200e-09\n",
            "  1.30460819e-04 2.82763345e-02 2.47230271e-01 3.49177657e-08\n",
            "  2.71232815e-07 3.30063651e-10 5.98231988e-12 2.06327154e-06\n",
            "  1.37961733e-02 2.57676331e-06 9.07924770e-05 3.69157825e-07]\n",
            " [6.81977149e-10 8.88905934e-11 1.22374652e-11 9.82192927e-14\n",
            "  1.23291167e-11 1.00637328e-11 3.67978670e-11 1.35822055e-09\n",
            "  2.72580704e-09 7.39008296e-09 1.26905194e-12 2.39834106e-13\n",
            "  9.61682260e-11 5.91131353e-11 3.03564355e-12 8.83610295e-11\n",
            "  4.11680051e-12 1.98554053e-11 4.01703779e-12 1.27059586e-10\n",
            "  1.54585464e-12 2.43904271e-11 5.26215223e-08 2.60673226e-13\n",
            "  3.24223816e-09 4.01824516e-10 3.06157671e-11 6.46218888e-11\n",
            "  7.05593497e-15 5.18383123e-11 8.59601030e-08 1.38750508e-09]\n",
            " [9.89160153e-09 1.18037457e-09 3.60577743e-09 8.13003941e-13\n",
            "  2.74517116e-13 1.02767892e-09 1.01034643e-11 1.87674872e-11\n",
            "  1.36462598e-10 6.89007019e-11 2.91703521e-10 9.45883298e-11\n",
            "  3.75730949e-10 4.79689285e-11 1.05284102e-08 8.37903398e-07\n",
            "  1.46851361e-11 1.31712455e-12 4.01499496e-10 8.64726070e-11\n",
            "  3.54659141e-10 1.72387448e-10 7.05169055e-11 4.56837045e-12\n",
            "  4.67225148e-11 9.67627637e-11 2.45388240e-10 1.69262199e-11\n",
            "  1.67411701e-09 2.15803709e-11 5.15490778e-10 5.44237654e-11]\n",
            " [1.24945504e-13 2.04055870e-15 5.55016304e-14 1.20658281e-14\n",
            "  3.84412356e-15 3.53779402e-14 3.77837470e-14 2.61008362e-13\n",
            "  2.58902857e-13 2.15451754e-12 2.05055534e-11 1.58960815e-12\n",
            "  2.11869853e-11 4.57839858e-13 1.23525704e-12 1.34043583e-12\n",
            "  5.06366975e-11 4.17399139e-13 5.94861552e-16 1.49752002e-12\n",
            "  1.07875123e-13 5.07248073e-15 3.29111990e-14 3.06834156e-11\n",
            "  2.89354344e-11 9.77826074e-13 4.98038228e-12 1.45620963e-10\n",
            "  1.01628514e-14 1.41898955e-12 2.82701204e-13 1.63268079e-11]\n",
            " [2.80653011e-11 5.07608222e-13 3.51365061e-13 4.06580386e-15\n",
            "  1.42365674e-11 3.33761592e-11 4.25105040e-13 1.53538211e-10\n",
            "  2.31321760e-10 1.48726454e-10 2.89701928e-12 4.62407369e-10\n",
            "  1.38572895e-12 3.08963189e-10 1.36177006e-13 3.99057342e-13\n",
            "  2.46714233e-11 6.99961934e-11 1.54913992e-11 1.15373667e-11\n",
            "  1.10551720e-09 4.94888625e-14 3.43207625e-12 1.11641052e-11\n",
            "  9.49615729e-09 1.64821237e-10 1.81512901e-12 8.97360914e-12\n",
            "  7.61854641e-13 1.28962731e-10 1.03933656e-10 7.27297643e-09]\n",
            " [9.34198889e-08 9.15641075e-11 1.50294212e-10 3.98459377e-08\n",
            "  1.82468467e-12 3.52804684e-11 6.88120660e-10 2.89731653e-06\n",
            "  3.90938977e-06 1.13900230e-08 1.73902274e-10 1.05782690e-06\n",
            "  2.78376221e-09 4.14485235e-08 2.13255486e-12 1.39248869e-13\n",
            "  4.47993901e-08 2.24587626e-09 3.06649943e-06 3.69209940e-09\n",
            "  5.77080472e-07 3.37688133e-05 4.21271379e-07 1.08902993e-08\n",
            "  1.81993997e-08 5.40026456e-09 6.80790947e-12 2.35767354e-07\n",
            "  1.04092228e-06 1.21474709e-08 1.42763318e-08 2.44831437e-08]\n",
            " [9.32750940e-08 1.78978971e-13 1.06082314e-12 1.46706341e-11\n",
            "  5.92333412e-12 4.67480927e-13 8.11042293e-11 2.07415454e-09\n",
            "  2.49224266e-09 8.65664167e-10 9.02068667e-14 1.24290030e-11\n",
            "  1.08727829e-10 2.64510452e-11 1.48597337e-11 9.78418290e-11\n",
            "  4.14529231e-12 5.45038614e-10 1.02983361e-08 3.34677884e-12\n",
            "  8.36678324e-11 6.19609365e-10 5.10492165e-06 5.81047020e-13\n",
            "  1.38438274e-09 1.35671830e-11 1.18402811e-10 6.12011443e-11\n",
            "  5.41568043e-09 3.16681948e-11 6.74875430e-08 5.04601506e-11]]\n",
            "fwd_done\n",
            "outputData =  [[5.54028412e-21 1.36067809e-14 1.44640407e-14 2.08751721e-15\n",
            "  2.33122195e-14 8.59892760e-18 3.35769117e-20 6.80308880e-16\n",
            "  3.89568624e-14 1.00603265e-11 2.98776085e-18 5.33857644e-13\n",
            "  1.93316509e-15 1.71368313e-12 1.89995674e-12 4.47125820e-12\n",
            "  1.15670706e-20 1.88817675e-15 1.34545626e-14 7.14486836e-15\n",
            "  7.47292179e-16 2.24845941e-14 3.86164865e-12 1.85755321e-14\n",
            "  2.32165194e-14 4.72242007e-15 7.63239294e-19 2.81247415e-15\n",
            "  1.95040641e-16 3.50970379e-09 2.37562648e-10 1.09153727e-16]\n",
            " [1.04521336e-10 2.45819424e-12 6.59199282e-13 4.80634154e-11\n",
            "  3.61955570e-12 2.74313193e-11 1.49649600e-11 1.23328833e-14\n",
            "  2.96317918e-09 5.82028102e-08 2.53550242e-09 1.04244144e-12\n",
            "  7.37164350e-09 2.39612429e-12 3.55123075e-09 6.08621460e-12\n",
            "  9.47460270e-09 5.93564335e-11 3.08600411e-14 1.66639246e-12\n",
            "  2.64926002e-10 2.16759257e-11 1.15713221e-12 1.21220263e-11\n",
            "  4.90502562e-14 1.62908252e-07 2.37749580e-11 3.18518976e-11\n",
            "  5.54051661e-09 1.66984926e-09 1.06289469e-09 9.10448022e-10]\n",
            " [2.98186817e-17 6.13944832e-12 1.05529143e-12 2.04678376e-15\n",
            "  6.99477114e-11 4.07871638e-12 2.90120232e-13 6.85748633e-12\n",
            "  1.61961322e-16 6.11441039e-16 2.28806497e-16 3.22029011e-11\n",
            "  6.14518144e-18 3.05316361e-11 4.87614182e-17 1.93831070e-10\n",
            "  1.37401376e-14 1.99841806e-16 1.24268644e-11 1.97805619e-10\n",
            "  1.77582636e-13 2.52711062e-15 2.80843191e-12 4.91798251e-13\n",
            "  8.87203002e-10 2.62657694e-15 2.32025128e-14 1.98140020e-15\n",
            "  6.52467432e-16 5.45732685e-13 1.32172364e-12 7.21395199e-16]\n",
            " [5.16668995e-04 6.51348637e-13 3.62554436e-11 2.63227788e-05\n",
            "  1.87293183e-10 8.23677660e-08 3.06764209e-05 2.19280776e-11\n",
            "  1.05005921e-03 5.46049897e-08 8.19330059e-03 2.14406508e-14\n",
            "  5.77406200e-03 5.12391162e-12 2.55251125e-08 2.08044573e-10\n",
            "  6.19257280e-06 3.96845543e-02 2.82951681e-07 4.33955064e-12\n",
            "  1.12844288e-06 1.65389892e-05 6.35627330e-07 2.34649914e-09\n",
            "  8.17294001e-14 2.28013919e-04 4.56400438e-06 5.21884232e-05\n",
            "  9.28230605e-01 3.23974766e-12 2.12395190e-11 1.61820491e-02]\n",
            " [1.03587459e-16 4.02072617e-13 1.14835383e-13 1.10580546e-09\n",
            "  3.58234466e-13 3.95589607e-09 1.04212882e-16 6.65682609e-12\n",
            "  1.48962101e-12 5.18738098e-13 1.32060488e-10 1.10442247e-12\n",
            "  9.75061313e-11 2.32647107e-12 3.08302947e-12 4.89441847e-12\n",
            "  5.91624107e-15 4.32381345e-13 2.31628362e-10 1.93518136e-13\n",
            "  3.85864622e-08 1.84009689e-11 1.18181046e-11 3.27745226e-13\n",
            "  1.45237537e-13 1.04381492e-13 5.55421555e-16 5.54389415e-09\n",
            "  6.31801336e-13 1.47330318e-11 5.79447912e-13 2.33640253e-10]\n",
            " [3.90239956e-11 9.45575951e-12 3.57978018e-11 6.28806260e-11\n",
            "  5.80986148e-13 1.21092766e-11 1.89972589e-08 1.24059598e-13\n",
            "  4.51320157e-14 1.09774129e-12 5.64339492e-11 8.51711504e-12\n",
            "  1.49675655e-13 7.43312411e-12 3.72838345e-15 5.82497111e-15\n",
            "  3.97405134e-13 8.21829669e-12 3.33102680e-13 9.33523083e-11\n",
            "  2.77407647e-10 1.24626889e-11 3.46196296e-12 1.28367378e-11\n",
            "  3.59579078e-11 7.83351785e-15 1.27339873e-11 6.27036885e-10\n",
            "  6.16932531e-12 3.89050172e-09 7.95256792e-12 5.11510537e-11]\n",
            " [1.25539125e-14 3.12891861e-15 2.32866905e-16 6.61259825e-18\n",
            "  4.43371903e-13 4.25730619e-16 2.80627928e-16 5.47609401e-13\n",
            "  1.05081533e-13 2.24762953e-12 1.16905951e-15 9.38846916e-16\n",
            "  3.48264597e-14 3.36299119e-12 1.42265751e-13 2.08257040e-13\n",
            "  2.02511474e-17 5.36976032e-16 3.58359957e-15 1.14095415e-15\n",
            "  5.26085326e-16 2.90099304e-13 7.34773517e-13 8.16926085e-13\n",
            "  3.48055812e-15 1.55655939e-15 4.80057380e-19 1.08467715e-14\n",
            "  2.32730000e-16 6.02843750e-08 8.42434031e-11 4.12351549e-15]\n",
            " [1.49981603e-15 2.16067755e-13 9.79942253e-14 5.89914895e-12\n",
            "  1.36724175e-11 1.61824376e-11 1.15190532e-16 4.56983940e-12\n",
            "  1.30111552e-12 4.42632512e-12 4.79790865e-14 3.40477971e-15\n",
            "  4.39880950e-14 1.36942000e-13 8.62633812e-10 1.39695723e-11\n",
            "  1.82385689e-17 3.34991478e-12 4.25112956e-11 9.83379752e-14\n",
            "  5.03039286e-12 1.66026039e-12 1.08205460e-08 1.71467814e-10\n",
            "  3.26724653e-14 6.99122222e-16 8.83159057e-15 1.23893545e-12\n",
            "  2.58962264e-15 3.07866332e-11 1.19180811e-12 4.96217246e-14]\n",
            " [3.70356218e-09 3.40595314e-14 3.78135317e-12 1.21991518e-06\n",
            "  2.29520573e-11 1.81972889e-10 6.07760761e-11 7.68104347e-13\n",
            "  2.49215486e-10 1.87144848e-10 2.50913526e-07 2.03416154e-11\n",
            "  2.64083228e-11 4.74978883e-13 1.42091015e-10 2.86567595e-10\n",
            "  7.48728993e-12 4.79804479e-08 1.16780956e-12 1.59232116e-11\n",
            "  5.15487008e-10 4.67713232e-09 1.02330986e-10 7.64509796e-11\n",
            "  2.34191384e-11 2.73405987e-11 9.35941816e-11 2.53415306e-11\n",
            "  5.67193146e-12 2.28932216e-10 2.43109694e-10 1.43110096e-08]\n",
            " [2.04800871e-11 3.88228345e-13 8.91042456e-15 1.69184111e-08\n",
            "  9.48694535e-11 3.04476140e-10 2.87661416e-10 8.47645382e-13\n",
            "  2.32332605e-12 5.88353125e-15 6.51662991e-09 1.83866780e-15\n",
            "  3.78642721e-10 5.63480147e-13 3.83891167e-13 8.01118361e-13\n",
            "  1.08633853e-11 4.87525058e-09 8.56753811e-14 1.69503592e-12\n",
            "  1.88928674e-10 9.33891263e-12 5.89017280e-13 4.00536681e-11\n",
            "  2.20768883e-14 1.05039795e-12 9.06874631e-10 3.56887228e-09\n",
            "  1.60806718e-13 4.27327061e-11 8.25443191e-12 4.63602431e-09]]\n",
            "fwd_done\n",
            "outputData =  [[1.66313632e-12 6.85781379e-11 1.95143928e-12 1.60457469e-17\n",
            "  7.89475226e-11 5.46035289e-17 1.42117542e-09 8.75019502e-13\n",
            "  1.89849463e-14 2.40326362e-14 1.49755296e-14 7.86040881e-11\n",
            "  1.80865734e-14 1.72731792e-12 5.19198798e-16 7.59817694e-13\n",
            "  1.51576418e-10 5.94444503e-12 1.40668734e-12 9.73414844e-14\n",
            "  1.40402134e-15 7.33176262e-11 4.58888068e-15 1.66688733e-11\n",
            "  3.20642864e-12 4.40027242e-11 4.90139685e-16 5.77284142e-14\n",
            "  6.33050275e-15 4.31045547e-11 1.30497655e-13 5.16186829e-11]\n",
            " [4.04506525e-10 8.40759550e-13 5.24944131e-11 3.31457082e-09\n",
            "  1.95140144e-08 1.42013541e-05 2.47051431e-09 9.21878920e-09\n",
            "  2.01900230e-08 2.88040651e-10 5.03387846e-07 1.10295550e-08\n",
            "  1.44252003e-08 1.00364665e-10 4.18320627e-05 1.44436793e-08\n",
            "  2.05119300e-09 3.11206520e-12 6.89972168e-10 1.31984973e-08\n",
            "  1.14227475e-08 1.68495264e-09 3.76336137e-09 1.04485138e-08\n",
            "  2.52432712e-09 1.56296631e-08 6.63322018e-08 4.36128298e-06\n",
            "  2.79504879e-09 1.42630828e-08 7.55368906e-09 9.08901927e-12]\n",
            " [4.16537079e-10 1.78751391e-11 5.41022886e-09 7.35514773e-13\n",
            "  1.90314304e-14 3.31913678e-13 1.06526889e-09 7.12319220e-11\n",
            "  4.65066417e-11 1.28233050e-12 7.46252899e-15 6.45262312e-11\n",
            "  1.32235317e-16 1.16880255e-08 6.00807203e-15 2.01920043e-15\n",
            "  2.20951480e-09 1.17107909e-07 1.84701931e-10 3.00332056e-12\n",
            "  8.30100827e-15 2.61095599e-13 2.31494088e-11 3.00796444e-15\n",
            "  1.28804132e-08 1.88051583e-11 1.44457913e-12 1.71321876e-11\n",
            "  8.77218120e-11 2.53134545e-11 1.23397395e-11 2.25915697e-09]\n",
            " [9.43425760e-09 1.31078066e-07 2.78046959e-05 6.41903256e-05\n",
            "  2.22726760e-04 1.21033057e-02 2.92286125e-08 9.94467882e-07\n",
            "  1.57943541e-04 5.91329251e-05 8.01282220e-02 2.70448950e-05\n",
            "  8.74477120e-04 6.22851137e-11 3.49310604e-01 3.17114406e-01\n",
            "  9.31908132e-10 1.46282987e-08 1.25343571e-04 3.11954018e-03\n",
            "  5.52838229e-02 9.46748231e-09 5.46947710e-02 3.71718878e-05\n",
            "  5.43450900e-09 1.24854112e-06 9.96985490e-03 1.12277503e-01\n",
            "  1.36286351e-04 2.24839404e-05 1.46912342e-04 6.42915385e-08]\n",
            " [2.02156246e-11 3.46560315e-09 3.18553205e-09 5.00450439e-13\n",
            "  1.86658245e-08 1.83447329e-09 1.12896260e-09 1.03965243e-10\n",
            "  1.43247374e-07 5.26626852e-12 4.60361913e-08 2.10508645e-09\n",
            "  3.87742619e-10 4.05090829e-10 4.07945518e-08 4.92973892e-11\n",
            "  5.69046039e-11 8.35746162e-13 1.54080951e-06 8.93499397e-06\n",
            "  1.84298913e-11 3.79595384e-11 3.25035424e-10 1.02283812e-07\n",
            "  1.48808161e-09 1.17604376e-09 1.17260445e-11 7.38149901e-11\n",
            "  1.60944443e-12 1.85867077e-09 3.54912429e-07 4.49119599e-09]\n",
            " [4.80444694e-09 8.04524831e-12 1.10623087e-11 2.71878883e-11\n",
            "  3.05314924e-11 1.26847520e-09 1.83404274e-11 3.00475560e-09\n",
            "  3.74483142e-10 2.97187673e-07 5.89713145e-10 2.01924586e-10\n",
            "  8.92824622e-07 7.52867671e-09 6.35598728e-10 1.08948428e-09\n",
            "  1.57749658e-08 2.44019893e-11 1.64609193e-10 1.85343544e-09\n",
            "  7.63265403e-09 5.74805970e-09 3.19853823e-08 3.38863278e-10\n",
            "  5.02459859e-13 3.75899468e-10 6.68655777e-10 3.47940783e-09\n",
            "  1.21883499e-09 1.49786399e-10 9.09580962e-09 3.07251022e-09]\n",
            " [7.32451498e-13 3.09441869e-10 3.16250036e-13 1.23930611e-13\n",
            "  2.03508650e-11 2.37593678e-13 6.84880176e-10 8.85387991e-11\n",
            "  2.50272784e-12 1.06675046e-11 5.60052395e-15 6.90902796e-11\n",
            "  3.15071465e-13 2.17223460e-13 1.10374720e-14 1.15114105e-12\n",
            "  1.17829045e-08 1.46518839e-13 1.46700991e-12 1.64590581e-13\n",
            "  5.36531833e-12 2.93612782e-10 1.84896149e-14 4.17713781e-11\n",
            "  6.17651438e-13 1.03445597e-09 3.27491185e-13 4.56045683e-14\n",
            "  1.19844196e-12 1.02526827e-09 2.46397345e-14 1.51900101e-10]\n",
            " [2.38525620e-10 1.19311576e-08 2.70818709e-11 1.90425433e-13\n",
            "  5.81499027e-10 3.78102381e-13 1.60889693e-11 1.95933920e-10\n",
            "  2.26068652e-10 9.64293563e-13 3.12215951e-09 1.41599879e-10\n",
            "  7.88501377e-10 3.02739059e-11 2.18718910e-11 8.69722918e-13\n",
            "  9.17030846e-10 3.30413922e-11 6.06312775e-09 7.84711559e-09\n",
            "  2.72068720e-10 2.78446378e-11 2.83982878e-11 1.14772255e-08\n",
            "  9.47760976e-10 1.68161521e-09 5.38958112e-11 1.31755875e-10\n",
            "  6.86542013e-13 1.11056843e-09 2.99533216e-11 2.07374275e-09]\n",
            " [1.50244634e-09 5.32128090e-07 6.73576382e-09 1.30462501e-08\n",
            "  2.62261594e-08 1.30521762e-04 4.62912302e-11 2.05510616e-09\n",
            "  1.02049556e-08 3.67369401e-05 6.83544762e-04 7.06637421e-10\n",
            "  3.14725687e-03 4.97826258e-10 8.32388683e-09 1.71184121e-07\n",
            "  1.23484611e-10 1.86016401e-12 1.52434698e-06 3.51061333e-07\n",
            "  1.18626156e-06 3.80516553e-09 3.75624319e-07 3.85697512e-06\n",
            "  8.56950673e-10 4.13030945e-09 2.27249251e-08 3.29727911e-07\n",
            "  1.88863275e-07 2.18801243e-09 5.94281147e-06 1.07806223e-07]\n",
            " [1.60585867e-11 2.44789578e-11 1.19562822e-08 3.26278601e-09\n",
            "  1.93181279e-09 3.43638696e-09 7.24415971e-09 6.46340011e-09\n",
            "  1.90127019e-09 1.94138746e-07 2.69314419e-07 6.53489014e-09\n",
            "  1.70656992e-09 7.86257671e-11 2.56897339e-08 1.57695621e-08\n",
            "  4.62425830e-11 7.57523248e-10 1.21220042e-07 9.90348961e-08\n",
            "  1.20603125e-09 1.38559875e-10 7.74930128e-08 1.05188774e-09\n",
            "  2.69730601e-09 2.44713012e-09 3.01153744e-10 4.13723002e-10\n",
            "  2.40257806e-08 6.33828214e-09 6.46056190e-06 4.30827014e-09]]\n",
            "fwd_done\n",
            "outputData =  [[7.38647045e-17 5.31951033e-14 1.20209985e-12 3.09972800e-14\n",
            "  5.33450398e-16 1.32331404e-12 2.12001895e-12 1.92866378e-16\n",
            "  1.60020107e-15 4.79111680e-18 8.13720570e-17 1.11363719e-13\n",
            "  3.41043782e-17 1.32328606e-17 5.72686768e-17 1.39519611e-20\n",
            "  2.78728443e-14 1.00567289e-13 6.24805241e-15 6.02944586e-16\n",
            "  5.52145709e-14 4.96950643e-19 3.94297631e-16 7.92684828e-14\n",
            "  5.10376404e-15 5.80709402e-19 1.39298618e-20 1.55575609e-20\n",
            "  5.61516040e-17 2.02213673e-13 6.81005804e-17 4.31440846e-12]\n",
            " [1.40866117e-09 7.46395802e-10 7.97720012e-13 3.78901884e-10\n",
            "  4.74031112e-11 5.72768001e-14 1.14049866e-11 3.23384799e-10\n",
            "  6.34120984e-13 2.05518597e-08 2.41913700e-11 3.77145839e-11\n",
            "  8.47542635e-09 2.86315420e-11 3.11149716e-12 1.47703788e-09\n",
            "  2.04423067e-12 9.09917834e-14 4.83093237e-11 3.18721989e-07\n",
            "  2.22794255e-09 1.27815638e-08 1.48898228e-12 1.33860805e-12\n",
            "  4.11296146e-07 4.90469697e-10 3.78097309e-10 3.68306823e-09\n",
            "  2.54931716e-09 1.64030376e-12 2.54922337e-10 8.01836277e-12]\n",
            " [2.30866345e-16 1.68463777e-14 1.21029944e-13 6.77009935e-15\n",
            "  3.18651752e-10 3.51845281e-12 2.01616638e-16 8.79254151e-16\n",
            "  1.34495354e-12 3.70271768e-17 7.88480132e-13 1.45717844e-12\n",
            "  3.87380971e-17 3.01349740e-12 7.42651840e-13 2.05130033e-14\n",
            "  5.43198243e-11 7.33096178e-12 1.12721907e-15 7.25514132e-17\n",
            "  3.53785285e-14 7.63265319e-16 7.84026152e-15 3.76595172e-11\n",
            "  1.94722790e-17 1.16807445e-12 1.39360218e-14 1.60029724e-15\n",
            "  9.20282985e-17 9.14386735e-11 3.95486221e-17 1.33384373e-14]\n",
            " [2.02091202e-06 5.18675962e-08 2.45026492e-11 2.03623584e-05\n",
            "  1.05913542e-06 1.69736150e-11 7.41302508e-06 1.23140649e-03\n",
            "  1.59939300e-11 1.78835679e-03 1.39127740e-07 1.05651054e-12\n",
            "  1.38589311e-01 5.10360369e-06 5.77445899e-10 2.87805451e-05\n",
            "  1.39853090e-09 1.77820815e-11 2.33775601e-06 4.05941539e-08\n",
            "  1.65243498e-09 2.31214604e-04 1.84525552e-06 5.18214772e-12\n",
            "  7.17141006e-01 1.04263751e-06 1.14283816e-02 5.71375753e-05\n",
            "  3.60308467e-02 1.25671500e-14 9.34236013e-02 1.66809063e-11]\n",
            " [2.13339087e-10 1.09399789e-12 1.82403927e-11 8.55507864e-13\n",
            "  1.56520377e-15 3.11133930e-12 7.81682598e-15 1.94930451e-11\n",
            "  6.42735499e-14 2.14663522e-08 5.19447930e-11 3.65850484e-11\n",
            "  1.38950419e-09 1.08184632e-13 1.05917201e-10 1.29498903e-13\n",
            "  1.02414170e-12 6.73121808e-12 3.02413448e-15 5.00841603e-09\n",
            "  7.57835602e-12 1.84868267e-13 1.68084600e-14 1.05013793e-11\n",
            "  2.54867292e-11 5.13929518e-12 5.07214074e-14 4.38777883e-14\n",
            "  5.98568608e-09 1.94586522e-11 2.06252797e-11 2.97727874e-10]\n",
            " [1.13655181e-11 1.14387015e-11 5.36078417e-11 2.53208132e-12\n",
            "  5.25895182e-15 2.97452731e-12 1.23854176e-11 1.31162023e-13\n",
            "  4.75893150e-11 5.68834439e-14 4.26957534e-12 4.34410227e-12\n",
            "  1.99209852e-12 2.77759948e-15 2.86600679e-12 2.16139363e-13\n",
            "  4.13131706e-11 3.05785231e-11 1.88697875e-09 6.50066501e-14\n",
            "  1.64749694e-14 1.11303597e-11 6.63183501e-11 2.37681204e-13\n",
            "  1.55600124e-13 4.19364884e-12 4.60762581e-15 1.32582218e-12\n",
            "  6.87664057e-13 5.07266045e-13 3.43395536e-10 1.12768166e-10]\n",
            " [5.27055386e-14 3.75952582e-13 1.53710309e-07 1.76453405e-14\n",
            "  8.70891797e-15 2.64141567e-13 1.14786250e-12 1.81353572e-12\n",
            "  1.70599436e-14 3.45227276e-15 2.88671275e-15 1.79817671e-13\n",
            "  7.01730359e-17 6.04667050e-12 7.72243200e-12 2.18637431e-13\n",
            "  1.44296737e-14 2.03699067e-13 1.64994275e-14 1.99102293e-12\n",
            "  1.57672566e-13 3.14492257e-14 2.59408292e-12 1.72170383e-13\n",
            "  3.76978255e-15 1.14108776e-14 2.66159783e-14 1.64322501e-14\n",
            "  1.01698026e-15 1.20128189e-13 4.79485433e-17 1.47072181e-10]\n",
            " [5.44671458e-12 1.18422086e-14 8.76482068e-13 3.52358831e-12\n",
            "  6.33478811e-13 6.36852247e-13 5.85734317e-12 2.43237392e-13\n",
            "  1.85381821e-13 2.54905455e-12 1.76515517e-12 1.04686376e-12\n",
            "  8.53083279e-11 2.04282623e-13 2.81124979e-12 3.99861865e-18\n",
            "  1.34659665e-10 7.53961416e-12 1.10776539e-11 1.40602907e-12\n",
            "  8.02014033e-12 3.96133341e-12 1.27782612e-11 3.12635719e-12\n",
            "  1.04386713e-15 4.56749180e-14 5.73760395e-15 7.79004888e-17\n",
            "  1.67867230e-12 2.06132201e-13 5.96930251e-13 1.37634538e-12]\n",
            " [2.32980640e-06 2.68251244e-10 3.64802778e-09 2.87483059e-08\n",
            "  2.65570968e-07 1.70266814e-13 1.16264678e-10 2.68605031e-10\n",
            "  1.36638689e-11 1.67146205e-06 1.83875127e-06 1.83236387e-11\n",
            "  1.21517005e-08 1.26988298e-11 3.74864464e-11 6.22720412e-09\n",
            "  7.14309956e-10 1.30334338e-13 4.82730922e-10 6.12643307e-11\n",
            "  2.93336066e-08 4.35591620e-10 2.00370028e-08 2.60470240e-12\n",
            "  6.91764312e-12 1.63433187e-09 8.11000200e-09 5.29229806e-09\n",
            "  1.19118042e-06 1.25175637e-12 1.39300679e-07 2.53422355e-09]\n",
            " [6.47186797e-11 2.38062303e-13 3.18804170e-12 3.39774888e-11\n",
            "  4.40534374e-13 4.98030708e-14 3.39411671e-12 2.60466865e-10\n",
            "  4.73982068e-13 1.86222223e-10 4.06121057e-10 5.75787011e-13\n",
            "  2.14217438e-11 6.74926228e-15 3.86221239e-11 2.49617259e-10\n",
            "  5.25452266e-13 2.02832294e-11 1.06506487e-12 1.06910999e-14\n",
            "  3.21103011e-13 6.03234812e-12 2.63026869e-10 4.33372225e-12\n",
            "  1.53325607e-12 1.08383465e-10 5.26363415e-12 4.39610100e-10\n",
            "  3.94214742e-10 2.38051783e-13 7.61008616e-12 1.23895583e-15]]\n",
            "fwd_done\n",
            "outputData =  [[7.51264111e-13 3.49255322e-15 1.23336254e-14 1.17700966e-12\n",
            "  1.78231669e-13 8.34835266e-17 2.59808777e-15 5.35044077e-12\n",
            "  3.20974590e-13 7.55430875e-15 2.33804069e-16 8.69688769e-16\n",
            "  9.49062673e-19 2.79530869e-19 1.52119191e-12 3.56705828e-16\n",
            "  2.06459556e-10 5.81522523e-16 4.24823741e-20 2.45589715e-11\n",
            "  6.30380550e-19 2.46568319e-14 2.93878784e-18 4.21639446e-09\n",
            "  4.86094779e-15 8.58412300e-19 1.34697685e-13 3.60726788e-14\n",
            "  1.86179617e-12 1.88869875e-12 1.03690788e-17 4.62668328e-12]\n",
            " [1.42679493e-09 9.70379754e-09 9.59240119e-10 1.67654428e-08\n",
            "  6.53200472e-11 1.11235030e-10 6.93057381e-09 1.58042460e-11\n",
            "  1.54415420e-09 9.99664956e-10 3.12821819e-07 5.79995885e-09\n",
            "  5.11943180e-09 3.05509572e-08 1.10644967e-08 2.84546207e-10\n",
            "  4.21804403e-09 3.69393760e-09 5.20598002e-10 8.34713543e-13\n",
            "  1.26616342e-05 7.46664502e-08 2.05635996e-08 7.37105790e-10\n",
            "  1.65245859e-11 4.24679872e-08 1.79310140e-11 8.24140306e-10\n",
            "  1.35175571e-12 1.16963980e-09 2.44227749e-08 2.36654648e-12]\n",
            " [9.76791391e-12 3.14823083e-17 2.79211444e-12 3.70292721e-14\n",
            "  2.98864379e-13 2.12223422e-13 3.76354260e-13 1.16734680e-09\n",
            "  9.30696956e-11 3.40919774e-14 5.38935919e-13 2.16707321e-12\n",
            "  1.32262188e-15 5.40145927e-16 5.51221929e-11 2.04085430e-15\n",
            "  5.57127446e-11 6.39160859e-18 1.45310660e-11 2.98207440e-10\n",
            "  3.29715941e-14 6.90068099e-16 6.56939304e-16 2.21015855e-13\n",
            "  8.28621458e-10 2.95975851e-13 1.22990001e-12 2.93172426e-13\n",
            "  1.45978319e-11 4.70042638e-10 8.26843003e-14 8.17325020e-11]\n",
            " [1.70403948e-08 9.33059326e-04 1.63918551e-07 1.67190468e-06\n",
            "  1.24398805e-05 1.23650351e-02 2.82288233e-05 2.93501043e-12\n",
            "  1.45615530e-08 5.68622461e-07 1.20417995e-06 4.13567970e-05\n",
            "  3.10899188e-01 2.01475865e-02 2.17012506e-05 1.95608383e-02\n",
            "  5.50036078e-12 3.41159507e-02 3.80844140e-02 3.61538929e-12\n",
            "  1.35655664e-02 8.40277569e-02 1.59531239e-01 6.99436786e-08\n",
            "  4.11409951e-11 1.58227486e-02 9.13588352e-08 5.50626050e-06\n",
            "  1.90636170e-09 1.01997140e-08 2.90735786e-01 7.27921750e-11]\n",
            " [8.00613863e-09 3.38100251e-10 1.28818156e-08 1.54129253e-09\n",
            "  3.11238101e-13 3.62122402e-11 6.08961264e-11 7.24004843e-11\n",
            "  1.33484584e-11 1.56200334e-07 3.07908444e-11 7.10614262e-13\n",
            "  7.72193293e-12 2.20873878e-13 3.57056002e-10 6.18319417e-15\n",
            "  1.01370221e-10 3.16945057e-09 1.61509084e-11 1.81778601e-10\n",
            "  2.97310515e-12 1.29455422e-10 2.60101226e-13 3.00129413e-11\n",
            "  7.86641496e-13 1.36460438e-13 2.15906577e-10 1.08421502e-08\n",
            "  2.60147451e-10 1.30086588e-09 1.50602740e-14 4.39690513e-13]\n",
            " [3.25945608e-13 4.27523789e-10 1.23750058e-11 1.94759823e-11\n",
            "  1.29748465e-13 1.82443071e-10 3.11423005e-11 7.95935453e-09\n",
            "  2.78756376e-09 1.97951223e-13 1.66635122e-11 4.27718304e-10\n",
            "  9.22387687e-11 5.59784823e-10 1.20446601e-12 1.53105504e-09\n",
            "  7.27998311e-12 2.17547474e-09 4.66322811e-12 6.80353010e-11\n",
            "  5.15556918e-11 5.57406752e-12 6.00125173e-11 2.16382827e-08\n",
            "  1.62760770e-10 8.68151454e-12 1.84765355e-11 5.91976424e-11\n",
            "  1.87461160e-10 1.47522390e-10 1.37827012e-07 3.67175819e-10]\n",
            " [1.32056766e-12 1.88023259e-17 1.78503352e-14 2.14958068e-12\n",
            "  1.24319181e-11 2.47533893e-13 1.24593007e-13 1.43989237e-11\n",
            "  1.69411940e-11 1.18361777e-15 3.85845802e-13 3.57160128e-14\n",
            "  9.72461749e-15 1.17547591e-13 1.59256884e-12 9.20012594e-14\n",
            "  7.01845998e-12 7.90808720e-16 4.13442159e-17 2.24946803e-11\n",
            "  1.29872645e-14 1.15455378e-12 9.84073785e-16 5.71173583e-11\n",
            "  6.43527397e-15 4.44200568e-16 1.74751337e-12 5.34781054e-14\n",
            "  6.18402827e-13 1.86394751e-10 1.91948775e-14 4.11025573e-13]\n",
            " [3.89440694e-10 9.09576978e-13 1.15427169e-09 1.56932507e-11\n",
            "  2.46845168e-11 9.00382035e-09 6.38161721e-14 8.67694296e-14\n",
            "  3.86546013e-14 2.99351059e-09 1.23205101e-13 4.05859357e-14\n",
            "  8.63405243e-12 6.78445142e-14 2.62574591e-12 4.18219408e-11\n",
            "  2.50735112e-11 3.62693230e-12 2.63875050e-12 4.02005947e-12\n",
            "  4.34872473e-14 4.73960584e-12 5.79717884e-14 1.80951603e-09\n",
            "  4.13954748e-12 6.36881360e-15 6.36001219e-11 5.87135037e-12\n",
            "  8.85742125e-09 1.26165123e-12 4.13442535e-14 2.59787750e-10]\n",
            " [4.16552767e-07 5.04056510e-05 7.19845107e-08 4.14992699e-09\n",
            "  7.98427554e-10 9.42631238e-09 4.15337952e-08 5.52797259e-13\n",
            "  3.99311756e-11 3.40912003e-06 4.69411430e-11 6.21647572e-10\n",
            "  2.40683506e-05 1.94540410e-07 7.17260102e-09 4.12063329e-07\n",
            "  3.56815333e-11 3.94880197e-06 7.87163633e-08 2.16171780e-10\n",
            "  2.39569193e-07 2.35457977e-09 4.56271803e-08 6.77951428e-08\n",
            "  1.73786248e-11 1.59240484e-09 2.20076126e-08 8.33037731e-08\n",
            "  4.60344473e-09 3.29918603e-12 5.15538663e-07 2.82812473e-11]\n",
            " [3.43606916e-09 9.48880858e-10 1.29076877e-10 2.38280647e-10\n",
            "  6.68173860e-13 1.57934867e-11 1.49177390e-08 1.95959409e-11\n",
            "  1.58009680e-13 8.50895250e-11 3.05906413e-10 6.69824165e-11\n",
            "  8.01902391e-10 3.97004860e-11 5.63867130e-10 1.09891952e-10\n",
            "  6.96472757e-11 2.58507260e-09 1.19270069e-09 3.40500220e-12\n",
            "  3.02525441e-09 2.06434496e-09 8.13909793e-10 4.52608845e-11\n",
            "  1.96186647e-13 4.23905005e-10 5.09226449e-10 4.82471785e-08\n",
            "  1.12336017e-11 8.81974541e-12 6.62831301e-09 3.69146851e-12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kARgTsK40xw3"
      },
      "source": [
        "Two things (20210310_0242):\n",
        "1. The high `exp` values were due to the fact that the inputs are pixels with values from 0 to 255. On scaling them down by 255, the outputs became more normal without even doing something about nan. Yet `np.nan_to_num()` can be helpful in case it is needed later on\n",
        "2. Why are the softmax outputs not adding up to 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMahuNP5qPkA",
        "outputId": "865932ce-b94d-4e7d-cfb7-e3afa35bb947"
      },
      "source": [
        "val_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3279.01993999, 3285.31591843, 3283.54862517, 3275.63722985,\n",
              "       3269.21427699, 3272.71498031, 3274.21236247, 3275.56751242,\n",
              "       3277.39294693])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSQ1OAMHqUuV",
        "outputId": "8fb6a47b-5c63-420e-9657-ca9084fad1e2"
      },
      "source": [
        "val_error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6LpbHL7BzQ"
      },
      "source": [
        "a = np.array([ [0,1],[1,0],[0,1] ])\r\n",
        "b = np.array( [ [0,0],[1,0],[0,0] ] )\r\n",
        "np.count_nonzero(np.all(a==b, axis=0))\r\n",
        "a[np.array([0,1])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51n0yc-CoFo-"
      },
      "source": [
        "# wandb log checks\n",
        "'''wandb.log(\n",
        "    {\"Parameter 1\": \n",
        "     [\"test string\",\"another test string\",\"yet another test\"], \n",
        "     \"Test text\": wandb.Html(\"<b><i>Working</i></b>\"), \n",
        "     \"Test table\": wandb.Table(columns=[\"I like this column\", \"Well, I like this one\"], data=[[\"The header above likes me\", \"Same here\"],[\"No idiot, he likes the whole column\", \"Good point there, neighbour\"]])})\n",
        "'''\n",
        "\n",
        "# wandb sweep checks\n",
        "def fsweeptest():\n",
        "  run = wandb.init(config={\"daIndex\":10})\n",
        "  for i in range(10):\n",
        "    wandb.log({\"x\": i, \"daMetric\": np.cos(np.pi*i*wandb.config.daIndex/80)})\n",
        "\n",
        "sweepCfg = {\"name\":\"Test sweep\", \n",
        "            \"metric\":{\n",
        "                \"name\":\"daMetric\",\n",
        "                \"goal\":\"maximise\"\n",
        "            },\n",
        "            \"method\":\"grid\", \n",
        "            \"parameters\":{\"daIndex\":{\"values\":[10,20,40,80,160]}}}\n",
        "sweepId = wandb.sweep(sweepCfg)\n",
        "wandb.agent(sweepId, function = fsweeptest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUACGeFN-Oyi"
      },
      "source": [
        "def runSweep():\n",
        "  hyp = {}\n",
        "  cfg = wandb.config\n",
        "\n",
        "  layersHidden = []\n",
        "  for i in range(cfg.numHiddenLayers):\n",
        "    layersHidden.append(cfg.hiddenLayerSize)\n",
        "  hyp[\"layerSizes\"] = [784] + layersHidden + [10]\n",
        "\n",
        "  hyp[\"batchSize\"] = cfg.batchSize\n",
        "\n",
        "  hyp[\"learningRate\"] = cfg.learningRate\n",
        "\n",
        "  hyp[\"epochs\"] = cfg.epochs\n",
        "\n",
        "  # activations?\n",
        "\n",
        "  hyp[\"lossFn\"] = cfg.loss\n",
        "\n",
        "  hyp[\"optimizer\"] = cfg.optimizer\n",
        "\n",
        "  nn = neuralNetwork(hyp)\n",
        "  ## run train functions. Also, put wandb.log statements inside after loss/err calculation\n",
        "\n",
        "'''hyp = {\n",
        "    \"layerSizes\": [len(x_train_1D[0]),len(y_train_1D[0])],\n",
        "    \"batchSize\": 32,\n",
        "    \"learningRate\": 1e-3,\n",
        "    \"epochs\": 1,\n",
        "    \"activations\": [ ACTIVATION_SOFTMAX],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\n",
        "    \"initWeightBounds\": (-1,1),\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    \"reg param\": 0.1\n",
        "}'''\n",
        "  nn = neuralNetwork(hyp)\n",
        "\n",
        "sweepCfg = {\n",
        "    \"name\":\"NN Fashion MNIST - Test Sweep\", \n",
        "    \"metric\":{\n",
        "        \"name\":\"valLoss\", \n",
        "        \"goal\":\"minimize\"\n",
        "    }, \n",
        "    \"method\": \"bayes\", \n",
        "    \"parameters\":{\n",
        "        \"epochs\":{\n",
        "          \"values\":[5,10]\n",
        "        },\n",
        "        \"numHiddenLayers\":{\n",
        "          \"values\":[3,4,5]\n",
        "        },\n",
        "        \"hiddenLayerSize\":{\n",
        "          \"values\":[32,64,128]\n",
        "        },\n",
        "        \"l2Reg\":{\n",
        "          \"values\":[0,5e-4,0.5]\n",
        "        },\n",
        "        \"learningRate\":{\n",
        "          \"values\":[1e-3, 1e-4]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "          \"values\":[GDOPT_NONE, GDOPT_MOMENTUM, GDOPT_NESTEROV, GDOPT_RMSPROP, GDOPT_ADAM, GDOPT_NADAM]\n",
        "        },\n",
        "        \"batchSize\":{\n",
        "          \"values\":[16,32,64]\n",
        "        },\n",
        "        \"weightInit\":{\n",
        "          \"values\":[WINIT_RANDOM, WINIT_XAVIER]\n",
        "        },\n",
        "        \"activationFns\":{\n",
        "          \"values\":[ACTIVATION_SIGMOID, ACTIVATION_RELU, ACTIVATION_TANH]},\n",
        "    }\n",
        "}\n",
        "\n",
        "# in wandb harness:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xktz21N-R6bt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}