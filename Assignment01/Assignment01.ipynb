{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasid99/cs6910-dl/blob/main/Assignment01/Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsh0vUPPl1gm"
      },
      "source": [
        "- change backpropagation for l2 reg\n",
        "\n",
        "- do loss calculation for validation set, not training set\n",
        "\n",
        "- change error calculation\n",
        "\n",
        "- we are assuming that 0-1 error is to be reported\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYppBN-iFAB2"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRAVTE1O9Sn"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrW6NV3sY1CG"
      },
      "source": [
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b4N8bBKd8qf"
      },
      "source": [
        "((x_train, y_train), (x_test, y_test)) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19DhbVbQehm4"
      },
      "source": [
        "classes, idx_sample_class = np.unique(y_train, return_index=True)\r\n",
        "sample_images = x_train[ idx_sample_class ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUeJZNT6f7NF"
      },
      "source": [
        "wandb.log({\"Sample_Images\": \\\n",
        "[ wandb.Image(sample_images[i], caption=\"Label:\"+str(classes[i])) \\\n",
        "for i in range(len(idx_sample_class)) ] })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRn-J2MUwg_"
      },
      "source": [
        "# function constants\n",
        "ACTIVATION_SIGMOID   = 0\n",
        "ACTIVATION_SOFTMAX   = 1\n",
        "ACTIVATION_THRESHOLD = 2\n",
        "ACTIVATION_RELU      = 3\n",
        "ACTIVATION_TANH      = 4\n",
        "\n",
        "LOSS_SQERROR         = 0\n",
        "LOSS_CROSSENTROPY    = 1\n",
        "\n",
        "GDOPT_NONE           = 0\n",
        "GDOPT_MOMENTUM       = 1\n",
        "GDOPT_NESTEROV       = 2\n",
        "GDOPT_ADAGRAD        = 3\n",
        "GDOPT_RMSPROP        = 4\n",
        "GDOPT_ADAM           = 5\n",
        "GDOPT_NADAM          = 6\n",
        "\n",
        "WINIT_RANDOM         = 0\n",
        "WINIT_XAVIER         = 1\n",
        "\n",
        "# math functions class\n",
        "class neuralNetworkMathFunctions:\n",
        "  \"\"\"\n",
        "  Helper class to handle mathematical operations of neural network passes, specifically activations and loss calculation\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize math helper class for a neural network. Hyperparameter object can be the same as given to neural network\n",
        "    \"\"\"\n",
        "    # Hyperparameters are initialized separately by the parent neural network\n",
        "\n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set mathematical hyperparameters of neural network\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "class neuralNetwork:\n",
        "  \"\"\"\n",
        "  Class for a neural network made up of multiple layers of perceptrons\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters and hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # create empty math functions object\n",
        "    self.fns = neuralNetworkMathFunctions()\n",
        "\n",
        "    # assign basic hyperparameters to neural network and math functions object\n",
        "    self.hyperparams = {}\n",
        "    self.setHyperparameters(hyperparams)\n",
        "\n",
        "    # initialize the weight and bias matrices of the NN\n",
        "    self.initModel(hyperparams)\n",
        "  \n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # change values of only the hyperparameters specified in the input variable\n",
        "    self.hyperparams.update(hp)\n",
        "    \n",
        "    # use member variables for commonly used hyperparameters\n",
        "    self.layerSizes       = self.hyperparams[\"layerSizes\"]\n",
        "    self.batchSize        = self.hyperparams[\"batchSize\"]\n",
        "    self.learningRate     = self.hyperparams[\"learningRate\"]\n",
        "    self.epochs           = self.hyperparams[\"epochs\"]\n",
        "    self.numLayers        = len(self.layerSizes) - 1\n",
        "    \n",
        "    # set math functions object hyperparameters\n",
        "    assert len(self.hyperparams[\"activations\"])==self.numLayers, \"number of layers (%d) and number of activations (%d) don't match\"%(self.numLayers,len(hp[\"activations\"]))\n",
        "    self.activations = self.hyperparams[\"activations\"]\n",
        "    self.lossFn = self.hyperparams[\"lossFn\"]\n",
        "    self.regparam = self.hyperparams[\"regparam\"]\n",
        "\n",
        "  def initModel(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters (weight and bias matrices) of neural network\n",
        "    \"\"\"\n",
        "    # checking bounds arg\n",
        "    bounds = (0,1)\n",
        "    if \"initWeightBounds\" in hyperparams.keys():\n",
        "      assert len(hyperparams[\"initWeightBounds\"])==2, \"bounds arg has to be a list/tuple of 2 numbers\"\n",
        "      bounds = hyperparams[\"initWeightBounds\"]\n",
        "\n",
        "    # create list of weight matrices and bias vectors\n",
        "    # the goal is to make the indexing same as that in lecture derivation, hence the dummy values\n",
        "    self.wmat = [np.array([1],ndmin=2)]\n",
        "    self.bias = [np.array([1],ndmin=2)]\n",
        "    \n",
        "    # create random initial parameters and append them to the above initialized lists\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      if self.hyperparams[\"initWeightMethod\"]==WINIT_XAVIER:\n",
        "        bounds = (-1/(self.layerSizes[i-1])**0.5,1/(self.layerSizes[i-1])**0.5)\n",
        "      self.wmat.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],self.layerSizes[i-1])+bounds[0])\n",
        "      self.bias.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],1)+bounds[0])\n",
        "\n",
        "  def activation(self,layerNum,x):\n",
        "    \"\"\"\n",
        "    Compute and return activation values for a given layer and its sum values\n",
        "    \"\"\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "      return 1/(1+np.exp(-x))\n",
        "    elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "      z = np.exp(x)\n",
        "      return z/np.sum(z)\n",
        "    elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "      return (x>=0)+0\n",
        "    elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "      return np.maximum(x,0)\n",
        "    elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "      return np.tanh(x)\n",
        "  \n",
        "  def activationDerivative(self,layerNum,**kwargs):\n",
        "    \"\"\"\n",
        "    Compute and return activation derivative values for a given layer and its sum or output values depending on the given argument\n",
        "    \"\"\"\n",
        "    assert ( len(kwargs.keys())==1 and np.any([_ in kwargs.keys() for _ in [\"x\",\"y\"]]) ), \"activationDerivative argument malformed. \\\n",
        "    Use activationDerivative(layerNum,x=x_val) or activationDerivative(layerNum,y=y_val)\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    \n",
        "    if \"y\" in kwargs.keys():\n",
        "      y = kwargs[\"y\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (y>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "        return 1/y\n",
        "    else:\n",
        "      x = kwargs[\"x\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        z = np.exp(x)\n",
        "        s = np.sum(z)\n",
        "        return z*(s-z)/(s**2)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (x>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return 1/np.tanh(x)\n",
        "  \n",
        "  def lossOutputDerivative(self,outputData,targetData):\n",
        "    \"\"\"\n",
        "    Compute and return loss derivatives for given output and target data\n",
        "    \"\"\"\n",
        "    print('outputData = ',outputData)\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return outputData-targetData\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      return -targetData / outputData \n",
        "  \n",
        "  def forwardPass(self, inputData):\n",
        "    \"\"\"\n",
        "    Compute output activations of all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                              # --- PSEUDOCODE ---\n",
        "    h     = inputData                              # h[0] = x\n",
        "    hData = [h]                                    #\n",
        "    datasetSize = np.shape(inputData)[1]           #\n",
        "    #                                              #\n",
        "    for i in range(1,self.numLayers+1):            # for i from 1 to L:\n",
        "      a   = self.wmat[i] @ h + self.bias[i]        #     a[i] = w[i] @ h[i-1] + b[i]\n",
        "      h   = self.activation(i,a)               #     h[i] = f(a[i])\n",
        "      hData.append(h)\n",
        "    \n",
        "    return hData\n",
        "  \n",
        "  def backwardPass(self, layerwiseOutputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute weight and bias gradients for all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                                                                        # --- PSEUDOCODE ---\n",
        "    lossData    = self.lossOutputDerivative(layerwiseOutputData[-1], targetData)             # loss_derivative = d(loss)/dh[L]\n",
        "    Delta       = lossData                                                                   # Delta[L] = loss_derivative\n",
        "    datasetSize = np.shape(targetData)[1]                                                    #\n",
        "    biasInputs  = np.array(np.ones(datasetSize),ndmin=2).T                                   #\n",
        "    gradW       = []                                                                         #\n",
        "    gradB       = []                                                                         #\n",
        "    #                                                                                        #\n",
        "    for iFwd in range(self.numLayers):                                                       # for i from L to 1:\n",
        "      i            = self.numLayers - iFwd                                                   #     // index correction\n",
        "      stocBiasCorr = self.activationDerivative(i,y=layerwiseOutputData[i]) * Delta           #     stochastic_bias_corrections = f'(a[i]) * Delta[i]\n",
        "      gW           = stocBiasCorr @ layerwiseOutputData[i-1].T + self.regparam*self.wmat[i]  #     grad(W[i]) = stochastic_bias_corrections x (h[i-1]).T\n",
        "      gB           = stocBiasCorr @ biasInputs + self.regparam*self.bias[i]                              #     grad(b[i]) = sum(stochastic_bias_corrections)\n",
        "      Delta        = self.wmat[i].T @ stocBiasCorr                                           #     Delta[i-1] = W[i] x stochastic_bias_corrections\n",
        "      \n",
        "      gradW.append(gW)\n",
        "      gradB.append(gB)\n",
        "    \n",
        "    # dummy element and order handling\n",
        "    gradW.append(np.array([0],ndmin=2))\n",
        "    gradW.reverse()\n",
        "    gradB.append(np.array([0],ndmin=2))\n",
        "    gradB.reverse()\n",
        "    \n",
        "    return (gradW,gradB)\n",
        "  \n",
        "  def infer(self,inputData,**kwargs):\n",
        "    \"\"\"\n",
        "    Perform inference on input dataset using the neural network\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    \n",
        "    # perform forward pass and return last-layer outputs\n",
        "    return self.forwardPass(inputData)[-1]\n",
        "\n",
        "  def gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex):##\n",
        "    # create data batches\n",
        "    startIndex  = batchSize * batchIndex\n",
        "    endIndex    = min(startIndex + batchSize, datasetSize)\n",
        "    inputBatch  = inputData[:,startIndex:endIndex]\n",
        "    targetBatch = targetData[:,startIndex:endIndex]\n",
        "    # perform forward and backward passes to compute gradients\n",
        "    layerwiseOutputData = self.forwardPass(inputBatch)\n",
        "    print(\"fwd_done\")\n",
        "    (gradW, gradB)      = self.backwardPass(layerwiseOutputData,targetBatch)\n",
        "    return gradW, gradB ##\n",
        "\n",
        "  def update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val):\n",
        "    y_pred_train = self.infer(inputData, colwiseData =True)\n",
        "    y_pred_val   = self.infer(x_val, colwiseData =True)\n",
        "    modW = np.sum( np.array( [ np.linalg.norm(W) for W in self.wmat ] ) )\n",
        "    modB = np.sum( np.array( [ np.linalg.norm(B) for B in self.bias ] ) )\n",
        "    modtheta_sq = (modW + modB)**2\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      loss_train = 0.5*np.sum(np.linalg.norm(y_pred_train - targetData, axis=0)**2) + 0.5*self.regparam*modtheta_sq\n",
        "      loss_val   = 0.5*np.sum(np.linalg.norm(  y_pred_val - y_val     , axis=0)**2) + 0.5*self.regparam*modtheta_sq\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      loss_train = np.sum(targetData * np.log(y_pred_train)) + 0.5*self.regparam*modtheta_sq\n",
        "      loss_val   = np.sum(     y_val * np.log(y_pred_val  )) + 0.5*self.regparam*modtheta_sq\n",
        "    error_train = 1 - np.count_nonzero( np.argmax(targetData, axis=0) == np.argmax(y_pred_train, axis=0) )/len(targetData[0])\n",
        "    error_val   = 1 - np.count_nonzero( np.argmax(     y_val, axis=0) == np.argmax(  y_pred_val, axis=0) )/len(y_val[0])\n",
        "    self.loss_train.append(loss_train)\n",
        "    self.error_train.append(error_train)\n",
        "    self.loss_val.append(loss_val)\n",
        "    self.error_val.append(error_val)\n",
        "\n",
        "  def sgd(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val)\n",
        "      for batchIndex in range(numBatches):       \n",
        "        #Get grad theta\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -self.learningRate * gradW[i]\n",
        "          self.bias[i] += -self.learningRate * gradB[i]\n",
        "    update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val)\n",
        "\n",
        "  def momentumGD(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    self.loss_lst_train = [] ##\n",
        "    self.error_lst_train = [] ##\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val, x_val, y_val)\n",
        "      for batchIndex in range(numBatches):       \n",
        "        #Get grad theta\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -update_w[i]\n",
        "          self.bias[i] += -update_b[i]\n",
        "    update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val) \n",
        "\n",
        "  def NAG(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val)\n",
        "      for batchIndex in range(numBatches):\n",
        "        # perform look ahead parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -gamma*update_w[i]\n",
        "          self.bias[i] += -gamma*update_b[i]\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -eta*gradW[i]\n",
        "          self.bias[i] += -eta*gradW[i]\n",
        "    update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val) \n",
        "\n",
        "  def rmsprop(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    beta = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #Initialise\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val)\n",
        "      for batchIndex in range(numBatches):        \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          v_w[i] = beta*v_w[i] + (1-beta)*gradW[i]**2\n",
        "          v_b[i] = beta*v_b[i] + (1-beta)*gradB[i]**2 \n",
        "          self.wmat[i] += -eta * (v_w[i] + epsilon)**-0.5 * gradW[i]\n",
        "          self.bias[i] += -eta * (v_b[i] + epsilon)**-0.5 * gradB[i]\n",
        "    update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val) \n",
        "  \n",
        "  def adam(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val)\n",
        "      for batchIndex in range(numBatches): \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = m_w[i]/(1-beta_1**t)\n",
        "          m_b_hat = m_b[i]/(1-beta_1**t)\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "    update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val) \n",
        "\n",
        "  def nadam(self, inputData, targetData, datasetSize, batchSize, numBatches, x_val, y_val):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val)\n",
        "      for batchIndex in range(numBatches): \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex) ##\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = (beta_1/(1-beta_1**(t+1)))*m_w[i] + ((1-beta_1)/(1-beta_1**t))*gradW[i]\n",
        "          m_b_hat = (beta_1/(1-beta_1**(t+1)))*m_b[i] + ((1-beta_1)/(1-beta_1**t))*gradB[i]\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "    update_val_train_loss_and_error(self, inputData, targetData, x_val, y_val) \n",
        "\n",
        "\n",
        "  def train(self, inputData, targetData, x_val, y_val, **kwargs):\n",
        "    \"\"\"\n",
        "    Train the network on the given input and target datasets\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input and target dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    targetData = np.array(targetData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "      targetData = targetData.T\n",
        "      x_val = x_val.T\n",
        "      y_val = y_val.T\n",
        "    assert np.shape(inputData)[1]==np.shape(targetData)[1], \"input and target datasets have different dataset sizes\"\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    assert np.shape(targetData)[0]==self.layerSizes[-1], \"size of target datapoint differs from size of target vector given as hyperparameter\"\n",
        "    datasetSize = np.shape(targetData)[1]\n",
        "\n",
        "    # calculate batch parameters\n",
        "    batchSize = datasetSize if self.batchSize==-1 else self.batchSize\n",
        "    numBatches = int(np.ceil(datasetSize / batchSize))\n",
        "\n",
        "    #Initialise loss and error lists\n",
        "    self.loss_train = []\n",
        "    self.error_train = []\n",
        "    self.loss_val = []\n",
        "    self.error_val = []\n",
        "\n",
        "    if self.hyperparams[\"optimizer\"] == GDOPT_NONE:\n",
        "      self.sgd(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_MOMENTUM:\n",
        "      self.momentumGD(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NESTEROV:\n",
        "      self.NAG(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_RMSPROP:\n",
        "      self.rmsprop(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_ADAM:\n",
        "      self.adam(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NADAM:\n",
        "      self.nadam(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    \n",
        "    self.loss_train = np.array(self.loss_train)\n",
        "    self.error_train = np.array(self.error_train)\n",
        "    self.loss_val = np.array(self.loss_val)\n",
        "    self.error_val = np.array(self.error_val)\n",
        "\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8x1hv7hiXy"
      },
      "source": [
        "hyp = {\n",
        "    \"layerSizes\": [2,4,1],\n",
        "    \"batchSize\": 1,\n",
        "    \"learningRate\": 1,\n",
        "    \"epochs\": 500,\n",
        "    \"activations\": [ACTIVATION_SIGMOID, ACTIVATION_SIGMOID],\n",
        "    \"lossFn\": LOSS_SQERROR,\n",
        "    \"initWeightBounds\": (-0.1,0.1),\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    \"reg param\":0.1\n",
        "}\n",
        "\n",
        "x = neuralNetwork(hyp)\n",
        "inp = np.array([[1,0.5],[-0.5,0.25],[1,2]])\n",
        "tar = np.array([[0.5],[0.75],[0.67]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nLeUVg2YFbH"
      },
      "source": [
        "x.setHyperparameters({\n",
        "    \"learningRate\":0.2,\n",
        "    \"epochs\":5000,\n",
        "    \"activations\":[ACTIVATION_RELU, ACTIVATION_RELU]\n",
        "    # square error loss will work as Hamming distance in this case\n",
        "})\n",
        "inp = np.array([[0,0],[0,1],[1,0],[1,1],[0,0],[0,1],[1,0],[1,1]])\n",
        "tar = np.array([[0],[0],[0],[1],[0],[0],[0],[1]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pswfmu-9fdLW"
      },
      "source": [
        "#Reshaping the 'x' data:\r\n",
        "len_1D = x_train.shape[1]*x_train.shape[2]\r\n",
        "x_train_1D = np.array( [x.reshape(len_1D) for x in x_train] )\r\n",
        "x_test_1D  = np.array( [x.reshape(len_1D) for x in x_test] )\r\n",
        "\r\n",
        "#Transforming 'y' data - changing scalar i to vector e(i)\r\n",
        "y_train_1D = np.zeros( (len(y_train), len(classes)) )\r\n",
        "for i in range(len(y_train)):\r\n",
        "  y_train_1D[i, y_train[i]] = 1\r\n",
        "y_test_1D = np.zeros( (len(y_test), len(classes)) )\r\n",
        "for i in range(len(y_test)):\r\n",
        "  y_test_1D[i, y_train[i]] = 1\r\n",
        "\r\n",
        "frac_val = 0.1\r\n",
        "all_idx = np.arange(len(x_train))\r\n",
        "val_idx = np.random.choice(all_idx, int(frac_val*len(x_train)), replace=False)\r\n",
        "tr2_idx = np.array([i for i in all_idx if i not in val_idx])\r\n",
        "\r\n",
        "x_train2 = x_train_1D[tr2_idx]\r\n",
        "y_train2 = y_train_1D[tr2_idx]\r\n",
        "x_val = x_train_1D[val_idx]\r\n",
        "y_val = y_train_1D[val_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnAlQs3YyE9"
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwamUIcFrJ40"
      },
      "source": [
        "hyp = {\r\n",
        "    \"layerSizes\": [len(x_train_1D[0]),len(y_train_1D[0])],\r\n",
        "    \"batchSize\": 32,\r\n",
        "    \"learningRate\": 1e-3,\r\n",
        "    \"epochs\": 1,\r\n",
        "    \"activations\": [ ACTIVATION_SOFTMAX],\r\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\r\n",
        "    \"initWeightBounds\": (-1,1),\r\n",
        "    \"optimizer\": GDOPT_ADAM,\r\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\r\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\r\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\r\n",
        "    \"regparam\": 0.1\r\n",
        "}\r\n",
        "\r\n",
        "#wandb.config.update(hyp)\r\n",
        "\r\n",
        "NN = neuralNetwork(hyp)\r\n",
        "(val_loss, val_error) = NN.train(x_train2[:256]/255, y_train2[:256]/255)\r\n",
        "#(loss_lst, error_lst) = NN.train(x_train_1D, y_train_1D)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kARgTsK40xw3"
      },
      "source": [
        "Two things (20210310_0242):\n",
        "1. The high `exp` values were due to the fact that the inputs are pixels with values from 0 to 255. On scaling them down by 255, the outputs became more normal without even doing something about nan. Yet `np.nan_to_num()` can be helpful in case it is needed later on\n",
        "2. Why are the softmax outputs not adding up to 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMahuNP5qPkA"
      },
      "source": [
        "val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSQ1OAMHqUuV"
      },
      "source": [
        "val_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6LpbHL7BzQ"
      },
      "source": [
        "a = np.array([ [0,1],[1,0],[0,1] ])\r\n",
        "b = np.array( [ [0,0],[1,0],[0,0] ] )\r\n",
        "np.count_nonzero(np.all(a==b, axis=0))\r\n",
        "a[np.array([0,1])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51n0yc-CoFo-"
      },
      "source": [
        "# wandb log checks\n",
        "'''wandb.log(\n",
        "    {\"Parameter 1\": \n",
        "     [\"test string\",\"another test string\",\"yet another test\"], \n",
        "     \"Test text\": wandb.Html(\"<b><i>Working</i></b>\"), \n",
        "     \"Test table\": wandb.Table(columns=[\"I like this column\", \"Well, I like this one\"], data=[[\"The header above likes me\", \"Same here\"],[\"No idiot, he likes the whole column\", \"Good point there, neighbour\"]])})\n",
        "'''\n",
        "\n",
        "# wandb sweep checks\n",
        "def fsweeptest():\n",
        "  run = wandb.init(config={\"daIndex\":10})\n",
        "  for i in range(10):\n",
        "    wandb.log({\"x\": i, \"daMetric\": np.cos(np.pi*i*wandb.config.daIndex/80)})\n",
        "\n",
        "sweepCfg = {\"name\":\"Test sweep\", \n",
        "            \"metric\":{\n",
        "                \"name\":\"daMetric\",\n",
        "                \"goal\":\"maximise\"\n",
        "            },\n",
        "            \"method\":\"grid\", \n",
        "            \"parameters\":{\"daIndex\":{\"values\":[10,20,40,80,160]}}}\n",
        "sweepId = wandb.sweep(sweepCfg)\n",
        "wandb.agent(sweepId, function = fsweeptest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUACGeFN-Oyi"
      },
      "source": [
        "def runSweep():\n",
        "  hyp = {}\n",
        "  cfg = wandb.config\n",
        "\n",
        "  layersHidden = []\n",
        "  for i in range(cfg.numHiddenLayers):\n",
        "    layersHidden.append(cfg.hiddenLayerSize)\n",
        "  hyp[\"layerSizes\"] = [784] + layersHidden + [10]\n",
        "\n",
        "  hyp[\"batchSize\"] = cfg.batchSize\n",
        "\n",
        "  hyp[\"learningRate\"] = cfg.learningRate\n",
        "\n",
        "  hyp[\"epochs\"] = cfg.epochs\n",
        "\n",
        "  # activations?\n",
        "\n",
        "  hyp[\"lossFn\"] = cfg.loss\n",
        "\n",
        "  hyp[\"optimizer\"] = cfg.optimizer\n",
        "\n",
        "  nn = neuralNetwork(hyp)\n",
        "  ## run train functions. Also, put wandb.log statements inside after loss/err calculation\n",
        "\n",
        "'''hyp = {\n",
        "    \"layerSizes\": [len(x_train_1D[0]),len(y_train_1D[0])],\n",
        "    \"batchSize\": 32,\n",
        "    \"learningRate\": 1e-3,\n",
        "    \"epochs\": 1,\n",
        "    \"activations\": [ ACTIVATION_SOFTMAX],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\n",
        "    \"initWeightBounds\": (-1,1),\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    \"reg param\": 0.1\n",
        "}'''\n",
        "  nn = neuralNetwork(hyp)\n",
        "\n",
        "sweepCfg = {\n",
        "    \"name\":\"NN Fashion MNIST - Test Sweep\", \n",
        "    \"metric\":{\n",
        "        \"name\":\"valLoss\", \n",
        "        \"goal\":\"minimize\"\n",
        "    }, \n",
        "    \"method\": \"bayes\", \n",
        "    \"parameters\":{\n",
        "        \"epochs\":{\n",
        "          \"values\":[5,10]\n",
        "        },\n",
        "        \"numHiddenLayers\":{\n",
        "          \"values\":[3,4,5]\n",
        "        },\n",
        "        \"hiddenLayerSize\":{\n",
        "          \"values\":[32,64,128]\n",
        "        },\n",
        "        \"l2Reg\":{\n",
        "          \"values\":[0,5e-4,0.5]\n",
        "        },\n",
        "        \"learningRate\":{\n",
        "          \"values\":[1e-3, 1e-4]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "          \"values\":[GDOPT_NONE, GDOPT_MOMENTUM, GDOPT_NESTEROV, GDOPT_RMSPROP, GDOPT_ADAM, GDOPT_NADAM]\n",
        "        },\n",
        "        \"batchSize\":{\n",
        "          \"values\":[16,32,64]\n",
        "        },\n",
        "        \"weightInit\":{\n",
        "          \"values\":[WINIT_RANDOM, WINIT_XAVIER]\n",
        "        },\n",
        "        \"activationFns\":{\n",
        "          \"values\":[ACTIVATION_SIGMOID, ACTIVATION_RELU, ACTIVATION_TANH]},\n",
        "    }\n",
        "}\n",
        "\n",
        "# in wandb harness:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xktz21N-R6bt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}