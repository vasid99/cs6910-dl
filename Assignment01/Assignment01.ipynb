{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasid99/cs6910-dl/blob/main/Assignment01/Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYppBN-iFAB2"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRAVTE1O9Sn"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gRn-J2MUwg_",
        "outputId": "847fc659-2ccc-4b47-eca7-00298e54ad2a"
      },
      "source": [
        "# function constants\n",
        "ACTIVATION_DUMMY     = -1\n",
        "ACTIVATION_SIGMOID   = 0\n",
        "ACTIVATION_SOFTMAX   = 1\n",
        "ACTIVATION_THRESHOLD = 2\n",
        "ACTIVATION_RELU      = 3\n",
        "\n",
        "LOSS_SQERROR      = 0\n",
        "LOSS_CROSSENTROPY = 1\n",
        "\n",
        "# math helper class\n",
        "class neuralNetworkMathFunctions:\n",
        "  \"\"\"\n",
        "  Helper class to handle mathematical operations of neural network passes, specifically activations and loss calculation\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Create a math helper class for the neural network. Hyperparameter object can be the same as given to neural network\n",
        "    \"\"\"\n",
        "    assert len(hyperparams[\"layerSizes\"])-1==len(hyperparams[\"activations\"]), \"number of layers and number of activations don't match\"\n",
        "    self.activations = hyperparams[\"activations\"]\n",
        "    self.activations.insert(0,ACTIVATION_DUMMY)\n",
        "    self.lossFn = hyperparams[\"lossFn\"]\n",
        "  \n",
        "  def activation(self,layerNum,x):\n",
        "    \"\"\"\n",
        "    Compute and return activation values for a given layer and its sum values\n",
        "    \"\"\"\n",
        "    if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "      return 1/(1+np.exp(-x))\n",
        "    elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "      z = np.exp(x)\n",
        "      return z/np.sum(z)\n",
        "    elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "      return (x>=0)+0\n",
        "    elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "      return np.max(x,0)\n",
        "  \n",
        "  def activationDerivative(self,layerNum,**kwargs):\n",
        "    \"\"\"\n",
        "    Compute and return activation derivative values for a given layer and its sum or output values depending on the given argument\n",
        "    \"\"\"\n",
        "    assert ( len(kwargs.keys())==1 and np.any([_ in kwargs.keys() for _ in [\"x\",\"y\"]]) ), \"activationDerivative argument malformed. \\\n",
        "    Use activationDerivative(x=x_val) or activationDerivative(y=y_val)\"\n",
        "    \n",
        "    if \"y\" in kwargs.keys():\n",
        "      y = kwargs[\"y\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (y>=0)+0\n",
        "    else:\n",
        "      x = kwargs[\"x\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        z = np.exp(x)\n",
        "        s = np.sum(z)\n",
        "        return z*(s-z)/(s**2)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (x>=0)+0\n",
        "  \n",
        "  def lossOutputDerivative(self,outputData,targetData):\n",
        "    \"\"\"\n",
        "    Compute and return loss derivatives for given output and target data\n",
        "    \"\"\"\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return outputData-targetData\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      return targetData * np.log2(outputData)\n",
        "\n",
        "class neuralNetwork:\n",
        "  \"\"\"\n",
        "  Class for a neural network made up of multiple layers of perceptrons\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    # assign basic hyperparameters\n",
        "    self.layerSizes = hyperparams[\"layerSizes\"]\n",
        "    self.batchSize = hyperparams[\"batchSize\"]\n",
        "    self.learningRate = hyperparams[\"learningRate\"]\n",
        "    self.epochs = hyperparams[\"epochs\"]\n",
        "\n",
        "    self.numLayers = len(self.layerSizes) - 1 # first layer is stand-in for inputs (can think of its weight matrix as identity)\n",
        "    \n",
        "    # create NN functions\n",
        "    self.fns = neuralNetworkMathFunctions(hyperparams)\n",
        "\n",
        "    # initialize the weight and bias matrices of the NN\n",
        "    self.initModel(hyperparams)\n",
        "\n",
        "    ## add states: forward_done, grad_calced, backward_done, train_done\n",
        "  \n",
        "  def initModel(self,hyperparams,**kwargs):\n",
        "    # argchecks\n",
        "    bounds = (0,1)\n",
        "    if \"weightBounds\" in hyperparams.keys():\n",
        "      assert len(hyperparams[\"weightBounds\"])==2, \"bounds arg has to be a list/tuple of 2 numbers\"\n",
        "      bounds = hyperparams[\"weightBounds\"]\n",
        "\n",
        "    # create list of weight matrices and bias vectors. The goal is to make the indexing same as that in class, hence the dummy values\n",
        "    self.wmat = [np.array([1],ndmin=2)]\n",
        "    self.bias = [np.array([1],ndmin=2)]\n",
        "    \n",
        "    # create random initial parameters and append them to the above initialized lists\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      self.wmat.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],self.layerSizes[i-1])+bounds[0])\n",
        "      self.bias.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],1)+bounds[0])\n",
        "  \n",
        "  def forwardPass(self, inputData):\n",
        "    h = inputData\n",
        "    hData   = [h]\n",
        "    \n",
        "    for i in range(1,self.numLayers+1):\n",
        "      a      = self.wmat[i] @ h + self.bias[i]            # a[i] = w[i] @ h[i-1] + b[i]\n",
        "      h      = self.fns.activation(i,a)                   # h[i] = g(a[i]) ## resolve layer-wise activation hyperparam\n",
        "      hData.append(h)\n",
        "    \n",
        "    return hData\n",
        "  \n",
        "  def backwardPass(self, layerwiseOutputData, targetData):\n",
        "    lossData    = self.fns.lossOutputDerivative(layerwiseOutputData[-1], targetData)\n",
        "    Delta       = lossData\n",
        "    datasetSize = np.shape(targetData)[1]\n",
        "    biasInputs  = np.array(np.ones(datasetSize),ndmin=2).T\n",
        "    gradW       = []\n",
        "    gradB       = []\n",
        "\n",
        "    for iFwd in range(self.numLayers):\n",
        "      i            = self.numLayers - iFwd\n",
        "      stocBiasCorr = self.fns.activationDerivative(i,y=layerwiseOutputData[i]) * Delta\n",
        "      gW           = stocBiasCorr @ layerwiseOutputData[i-1].T\n",
        "      gB           = stocBiasCorr @ biasInputs\n",
        "      Delta        = self.wmat[i].T @ stocBiasCorr\n",
        "      \n",
        "      gradW.append(gW)\n",
        "      gradB.append(gB)\n",
        "    \n",
        "    gradW.append(np.array([0],ndmin=2))\n",
        "    gradW.reverse()\n",
        "    gradB.append(np.array([0],ndmin=2))\n",
        "    gradB.reverse()\n",
        "    \n",
        "    return (gradW,gradB)\n",
        "  \n",
        "  def infer(self,inputData,**kwargs):\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "    return self.forwardPass(inputData)[-1]\n",
        "\n",
        "  def train(self, inputData, targetData, **kwargs):\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    targetData = np.array(targetData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "      targetData = targetData.T\n",
        "    assert np.shape(inputData)[1]==np.shape(targetData)[1], \"input and target datasets have different dataset sizes\"\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    assert np.shape(targetData)[0]==self.layerSizes[-1], \"size of target datapoint differs from size of target vector given as hyperparameter\"\n",
        "    datasetSize = np.shape(targetData)[1]\n",
        "\n",
        "    batchSize = datasetSize if self.batchSize==-1 else self.batchSize\n",
        "    numBatches = int(np.ceil(datasetSize / batchSize))\n",
        "\n",
        "    for epoch in range(self.epochs): ## epoch kwarg?\n",
        "      for batchIndex in range(numBatches):\n",
        "        startIndex  = batchSize * batchIndex\n",
        "        endIndex    = min(startIndex + batchSize, datasetSize)\n",
        "        inputBatch  = inputData[:,startIndex:endIndex]\n",
        "        targetBatch = targetData[:,startIndex:endIndex]\n",
        "        \n",
        "        layerwiseOutputData = self.forwardPass(inputBatch)\n",
        "        (gradW, gradB)      = self.backwardPass(layerwiseOutputData,targetBatch)\n",
        "        \n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -self.learningRate * gradW[i]\n",
        "          self.bias[i] += -self.learningRate * gradB[i]\n",
        "\n",
        "\n",
        "hp = {\n",
        "    \"layerSizes\": [2,4,1],\n",
        "    \"batchSize\": 1,\n",
        "    \"learningRate\": 1,\n",
        "    \"epochs\": 500,\n",
        "    \"activations\": [ACTIVATION_SIGMOID, ACTIVATION_SIGMOID],\n",
        "    \"lossFn\": LOSS_SQERROR,\n",
        "    \"weightBounds\": (-0.1,0.1)\n",
        "}\n",
        "\n",
        "x = neuralNetwork(hp)\n",
        "inp = np.array([[1,0.5],[-0.5,0.25],[1,2]])\n",
        "tar = np.array([[0.5],[0.75],[0.67]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(hp[\"epochs\"],hp[\"learningRate\"]))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target data:\n",
            "[[0.5  0.75 0.67]]\n",
            "Output before training:\n",
            "[[0.48886327 0.48852887 0.48888447]]\n",
            "Output after training for 500 epochs with learning rate of 1.00:\n",
            "[[0.52017802 0.74300063 0.66098754]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}