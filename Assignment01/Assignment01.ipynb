{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasid99/cs6910-dl/blob/main/Assignment01/Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYppBN-iFAB2"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRAVTE1O9Sn"
      },
      "source": [
        "# imports\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRn-J2MUwg_"
      },
      "source": [
        "# function constants\n",
        "ACTIVATION_SIGMOID   = 0\n",
        "ACTIVATION_SOFTMAX   = 1\n",
        "ACTIVATION_THRESHOLD = 2\n",
        "ACTIVATION_RELU      = 3\n",
        "\n",
        "LOSS_SQERROR         = 0\n",
        "LOSS_CROSSENTROPY    = 1\n",
        "\n",
        "GDOPT_NONE           = 0\n",
        "GDOPT_MOMENTUM       = 1\n",
        "GDOPT_NESTEROV       = 2\n",
        "GDOPT_ADAGRAD        = 3\n",
        "GDOPT_RMSPROP        = 4\n",
        "GDOPT_ADAM           = 5\n",
        "GDOPT_NADAM          = 6\n",
        "\n",
        "# math functions class\n",
        "class neuralNetworkMathFunctions:\n",
        "  \"\"\"\n",
        "  Helper class to handle mathematical operations of neural network passes, specifically activations and loss calculation\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize math helper class for a neural network. Hyperparameter object can be the same as given to neural network\n",
        "    \"\"\"\n",
        "    # Hyperparameters are initialized separately by the parent neural network\n",
        "\n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set mathematical hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    self.activations = hp[\"activations\"]\n",
        "    self.lossFn = hp[\"lossFn\"]\n",
        "\n",
        "  def activation(self,layerNum,x):\n",
        "    \"\"\"\n",
        "    Compute and return activation values for a given layer and its sum values\n",
        "    \"\"\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "      return 1/(1+np.exp(-x))\n",
        "    elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "      z = np.exp(x)\n",
        "      return z/np.sum(z)\n",
        "    elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "      return (x>=0)+0\n",
        "    elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "      return np.maximum(x,0)\n",
        "  \n",
        "  def activationDerivative(self,layerNum,**kwargs):\n",
        "    \"\"\"\n",
        "    Compute and return activation derivative values for a given layer and its sum or output values depending on the given argument\n",
        "    \"\"\"\n",
        "    assert ( len(kwargs.keys())==1 and np.any([_ in kwargs.keys() for _ in [\"x\",\"y\"]]) ), \"activationDerivative argument malformed. \\\n",
        "    Use activationDerivative(layerNum,x=x_val) or activationDerivative(layerNum,y=y_val)\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    \n",
        "    if \"y\" in kwargs.keys():\n",
        "      y = kwargs[\"y\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (y>=0)+0\n",
        "    else:\n",
        "      x = kwargs[\"x\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        z = np.exp(x)\n",
        "        s = np.sum(z)\n",
        "        return z*(s-z)/(s**2)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (x>=0)+0\n",
        "  \n",
        "  def lossOutputDerivative(self,outputData,targetData):\n",
        "    \"\"\"\n",
        "    Compute and return loss derivatives for given output and target data\n",
        "    \"\"\"\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return outputData-targetData\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      return - targetData / outputData \n",
        "\n",
        "      # loss fn: - targetData * np.log2(outputData) ## fix function\n",
        "      # derivative: - targetData / outputData\n",
        "\n",
        "\n",
        "class neuralNetwork:\n",
        "  \"\"\"\n",
        "  Class for a neural network made up of multiple layers of perceptrons\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters and hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # create empty math functions object\n",
        "    self.fns = neuralNetworkMathFunctions()\n",
        "\n",
        "    # assign basic hyperparameters to neural network and math functions object\n",
        "    self.hyperparams = {}\n",
        "    self.setHyperparameters(hyperparams)\n",
        "\n",
        "    # initialize the weight and bias matrices of the NN\n",
        "    self.initModel(hyperparams)\n",
        "\n",
        "  \n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # change values of only the hyperparameters specified in the input variable\n",
        "    self.hyperparams.update(hp)\n",
        "    \n",
        "    # use member variables for commonly used hyperparameters\n",
        "    self.layerSizes       = self.hyperparams[\"layerSizes\"]\n",
        "    self.batchSize        = self.hyperparams[\"batchSize\"]\n",
        "    self.learningRate     = self.hyperparams[\"learningRate\"]\n",
        "    self.epochs           = self.hyperparams[\"epochs\"]\n",
        "    self.numLayers        = len(self.layerSizes) - 1\n",
        "    \n",
        "    # set math functions object hyperparameters\n",
        "    assert len(self.hyperparams[\"activations\"])==self.numLayers, \"number of layers (%d) and number of activations (%d) don't match\"%(self.numLayers,len(hp[\"activations\"]))\n",
        "    self.fns.setHyperparameters(self.hyperparams)\n",
        "\n",
        "  def initModel(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters (weight and bias matrices) of neural network\n",
        "    \"\"\"\n",
        "    # checking bounds arg\n",
        "    bounds = (0,1)\n",
        "    if \"initWeightBounds\" in hyperparams.keys():\n",
        "      assert len(hyperparams[\"initWeightBounds\"])==2, \"bounds arg has to be a list/tuple of 2 numbers\"\n",
        "      bounds = hyperparams[\"initWeightBounds\"]\n",
        "\n",
        "    # create list of weight matrices and bias vectors\n",
        "    # the goal is to make the indexing same as that in lecture derivation, hence the dummy values\n",
        "    self.wmat = [np.array([1],ndmin=2)]\n",
        "    self.bias = [np.array([1],ndmin=2)]\n",
        "    \n",
        "    # create random initial parameters and append them to the above initialized lists\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      self.wmat.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],self.layerSizes[i-1])+bounds[0])\n",
        "      self.bias.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],1)+bounds[0])\n",
        "  \n",
        "  def forwardPass(self, inputData):\n",
        "    \"\"\"\n",
        "    Compute output activations of all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                              # --- PSEUDOCODE ---\n",
        "    h     = inputData                              # h[0] = x\n",
        "    hData = [h]                                    #\n",
        "    datasetSize = np.shape(inputData)[1]           #\n",
        "    #                                              #\n",
        "    for i in range(1,self.numLayers+1):            # for i from 1 to L:\n",
        "      a   = self.wmat[i] @ h + self.bias[i]        #     a[i] = w[i] @ h[i-1] + b[i]\n",
        "      h   = self.fns.activation(i,a)               #     h[i] = f(a[i])\n",
        "      hData.append(h)\n",
        "    \n",
        "    return hData\n",
        "  \n",
        "  def backwardPass(self, layerwiseOutputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute weight and bias gradients for all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                                                                        # --- PSEUDOCODE ---\n",
        "    lossData    = self.fns.lossOutputDerivative(layerwiseOutputData[-1], targetData)         # loss_derivative = d(loss)/dh[L]\n",
        "    Delta       = lossData                                                                   # Delta[L] = loss_derivative\n",
        "    datasetSize = np.shape(targetData)[1]                                                    #\n",
        "    biasInputs  = np.array(np.ones(datasetSize),ndmin=2).T                                   #\n",
        "    gradW       = []                                                                         #\n",
        "    gradB       = []                                                                         #\n",
        "    #                                                                                        #\n",
        "    for iFwd in range(self.numLayers):                                                       # for i from L to 1:\n",
        "      i            = self.numLayers - iFwd                                                   #     // index correction\n",
        "      stocBiasCorr = self.fns.activationDerivative(i,y=layerwiseOutputData[i]) * Delta       #     stochastic_bias_corrections = f'(a[i]) * Delta[i]\n",
        "      gW           = stocBiasCorr @ layerwiseOutputData[i-1].T                               #     grad(W[i]) = stochastic_bias_corrections x (h[i-1]).T\n",
        "      gB           = stocBiasCorr @ biasInputs                                               #     grad(b[i]) = sum(stochastic_bias_corrections)\n",
        "      Delta        = self.wmat[i].T @ stocBiasCorr                                           #     Delta[i-1] = W[i] x stochastic_bias_corrections\n",
        "      \n",
        "      gradW.append(gW)\n",
        "      gradB.append(gB)\n",
        "    \n",
        "    # dummy element and order handling\n",
        "    gradW.append(np.array([0],ndmin=2))\n",
        "    gradW.reverse()\n",
        "    gradB.append(np.array([0],ndmin=2))\n",
        "    gradB.reverse()\n",
        "    \n",
        "    return (gradW,gradB)\n",
        "  \n",
        "  def infer(self,inputData,**kwargs):\n",
        "    \"\"\"\n",
        "    Perform inference on input dataset using the neural network\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    \n",
        "    # perform forward pass and return last-layer outputs\n",
        "    return self.forwardPass(inputData)[-1]\n",
        "\n",
        "  def gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex):\n",
        "    # create data batches\n",
        "    startIndex  = batchSize * batchIndex\n",
        "    endIndex    = min(startIndex + batchSize, datasetSize)\n",
        "    inputBatch  = inputData[:,startIndex:endIndex]\n",
        "    targetBatch = targetData[:,startIndex:endIndex]\n",
        "\n",
        "    # perform forward and backward passes to compute gradients\n",
        "    layerwiseOutputData = self.forwardPass(inputBatch)\n",
        "    (gradW, gradB)      = self.backwardPass(layerwiseOutputData,targetBatch)\n",
        "\n",
        "    return gradW, gradB\n",
        "\n",
        "  def sgd(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):       \n",
        "        (gradW, gradB) = gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -self.learningRate * gradW[i]\n",
        "          self.bias[i] += -self.learningRate * gradB[i]\n",
        "\n",
        "  def momentumGD(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -update_w[i]\n",
        "          self.bias[i] += -update_b[i]\n",
        "\n",
        "  def NAG(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        # perform look ahead parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -gamma*update_w[i]\n",
        "          self.bias[i] += -gamma*update_b[i]\n",
        "        (gradW, gradB) = gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -eta*gradW[i]\n",
        "          self.bias[i] += -eta*gradW[i]\n",
        "\n",
        "  def rmsprop(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    beta = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):       \n",
        "        (gradW, gradB) = gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          v_w[i] = beta*v_w[i] + (1-beta)*gradW[i]**2\n",
        "          v_b[i] = beta*v_b[i] + (1-beta)*gradB[i]**2 \n",
        "          self.wmat[i] += -eta * (v_w[i] + epsilon)**-0.5 * gradW[i]\n",
        "          self.bias[i] += -eta * (v_b[i] + epsilon)**-0.5 * gradB[i]\n",
        "  \n",
        "  def adam(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = m_w[i]/(1-beta_1**t)\n",
        "          m_b_hat = m_b[i]/(1-beta_1**t)\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "\n",
        "    def nadam(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = (beta_1/(1-beta_1**(t+1)))*m_w[i] + ((1-beta_1)/(1-beta_1**t))*gradW[i]\n",
        "          m_b_hat = (beta_1/(1-beta_1**(t+1)))*m_b[i] + ((1-beta_1)/(1-beta_1**t))*gradB[i]\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "\n",
        "\n",
        "  def train(self, inputData, targetData, **kwargs):\n",
        "    \"\"\"\n",
        "    Train the network on the given input and target datasets\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input and target dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    targetData = np.array(targetData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "      targetData = targetData.T\n",
        "    assert np.shape(inputData)[1]==np.shape(targetData)[1], \"input and target datasets have different dataset sizes\"\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    assert np.shape(targetData)[0]==self.layerSizes[-1], \"size of target datapoint differs from size of target vector given as hyperparameter\"\n",
        "    datasetSize = np.shape(targetData)[1]\n",
        "\n",
        "    # calculate batch parameters\n",
        "    batchSize = datasetSize if self.batchSize==-1 else self.batchSize\n",
        "    numBatches = int(np.ceil(datasetSize / batchSize))\n",
        "\n",
        "    \n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs8x1hv7hiXy",
        "outputId": "c00e1f49-b478-4790-92ed-39d9e27ce411"
      },
      "source": [
        "hyp = {\n",
        "    \"layerSizes\": [2,4,1],\n",
        "    \"batchSize\": 1,\n",
        "    \"learningRate\": 1,\n",
        "    \"epochs\": 500,\n",
        "    \"activations\": [ACTIVATION_SIGMOID, ACTIVATION_SIGMOID],\n",
        "    \"lossFn\": LOSS_SQERROR,\n",
        "    \"initWeightBounds\": (-0.1,0.1)\n",
        "}\n",
        "\n",
        "x = neuralNetwork(hyp)\n",
        "inp = np.array([[1,0.5],[-0.5,0.25],[1,2]])\n",
        "tar = np.array([[0.5],[0.75],[0.67]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "Target data:\n",
            "[[0.5  0.75 0.67]]\n",
            "Output before training:\n",
            "[[0.50952111 0.51012391 0.51006613]]\n",
            "Performing training now\n",
            "Output after training for 500 epochs with learning rate of 1.00:\n",
            "[[0.51353408 0.74544424 0.66385229]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nLeUVg2YFbH",
        "outputId": "40065a16-2959-4f4b-9355-dd9f9923db1b"
      },
      "source": [
        "x.setHyperparameters({\n",
        "    \"learningRate\":0.2,\n",
        "    \"epochs\":5000,\n",
        "    \"activations\":[ACTIVATION_RELU, ACTIVATION_RELU]\n",
        "    # square error loss will work as Hamming distance in this case\n",
        "})\n",
        "inp = np.array([[0,0],[0,1],[1,0],[1,1],[0,0],[0,1],[1,0],[1,1]])\n",
        "tar = np.array([[0],[0],[0],[1],[0],[0],[0],[1]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target data:\n",
            "[[0 0 0 1 0 0 0 1]]\n",
            "Output before training:\n",
            "[[0.         0.59381962 0.         0.         0.         0.59381962\n",
            "  0.         0.        ]]\n",
            "Performing training now\n",
            "Output after training for 5000 epochs with learning rate of 0.20:\n",
            "[[0.00000000e+00 0.00000000e+00 1.11022302e-16 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.11022302e-16 1.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}