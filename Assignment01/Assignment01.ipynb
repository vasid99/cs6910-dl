{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasid99/cs6910-dl/blob/main/Assignment01/Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYppBN-iFAB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da963d83-92ae-4031-a95d-401913c49f73"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/17/b1e27f77c3d47f6915a774ecf632e3f5a7d49d9fa3991547729e7f19bedd/wandb-0.10.21-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 47.9MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=bdb6a9864000b18795fef538b4a99ecb4d6409e0ccce8fd5b987651050d29975\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=c8a3e12fec9b74b4e58f652bcda0bc412099e8cf53a9b0a295054a1caff843e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, pathtools, sentry-sdk, docker-pycreds, configparser, smmap, gitdb, GitPython, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRAVTE1O9Sn"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrW6NV3sY1CG"
      },
      "source": [
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b4N8bBKd8qf"
      },
      "source": [
        "((x_train, y_train), (x_test, y_test)) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19DhbVbQehm4"
      },
      "source": [
        "classes, idx_sample_class = np.unique(y_train, return_index=True)\r\n",
        "sample_images = x_train[ idx_sample_class ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUeJZNT6f7NF"
      },
      "source": [
        "wandb.log({\"Sample_Images\": [ wandb.Image(sample_images[i], caption=\"Label:\"+str(classes[i])) for i in range(len(idx_sample_class)) ] })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRn-J2MUwg_"
      },
      "source": [
        "# function constants\n",
        "ACTIVATION_SIGMOID   = 0\n",
        "ACTIVATION_SOFTMAX   = 1\n",
        "ACTIVATION_THRESHOLD = 2\n",
        "ACTIVATION_RELU      = 3\n",
        "\n",
        "LOSS_SQERROR         = 0\n",
        "LOSS_CROSSENTROPY    = 1\n",
        "\n",
        "GDOPT_NONE           = 0\n",
        "GDOPT_MOMENTUM       = 1\n",
        "GDOPT_NESTEROV       = 2\n",
        "GDOPT_ADAGRAD        = 3\n",
        "GDOPT_RMSPROP        = 4\n",
        "GDOPT_ADAM           = 5\n",
        "GDOPT_NADAM          = 6\n",
        "\n",
        "# math functions class\n",
        "class neuralNetworkMathFunctions:\n",
        "  \"\"\"\n",
        "  Helper class to handle mathematical operations of neural network passes, specifically activations and loss calculation\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize math helper class for a neural network. Hyperparameter object can be the same as given to neural network\n",
        "    \"\"\"\n",
        "    # Hyperparameters are initialized separately by the parent neural network\n",
        "\n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set mathematical hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    self.activations = hp[\"activations\"]\n",
        "    self.lossFn = hp[\"lossFn\"]\n",
        "\n",
        "  def activation(self,layerNum,x):\n",
        "    \"\"\"\n",
        "    Compute and return activation values for a given layer and its sum values\n",
        "    \"\"\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "      return 1/(1+np.exp(-x))\n",
        "    elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "      z = np.exp(x)\n",
        "      return z/np.sum(z)\n",
        "    elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "      return (x>=0)+0\n",
        "    elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "      return np.maximum(x,0)\n",
        "  \n",
        "  def activationDerivative(self,layerNum,**kwargs):\n",
        "    \"\"\"\n",
        "    Compute and return activation derivative values for a given layer and its sum or output values depending on the given argument\n",
        "    \"\"\"\n",
        "    assert ( len(kwargs.keys())==1 and np.any([_ in kwargs.keys() for _ in [\"x\",\"y\"]]) ), \"activationDerivative argument malformed. \\\n",
        "    Use activationDerivative(layerNum,x=x_val) or activationDerivative(layerNum,y=y_val)\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    \n",
        "    if \"y\" in kwargs.keys():\n",
        "      y = kwargs[\"y\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (y>=0)+0\n",
        "    else:\n",
        "      x = kwargs[\"x\"]\n",
        "      if self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        z = np.exp(x)\n",
        "        s = np.sum(z)\n",
        "        return z*(s-z)/(s**2)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (x>=0)+0\n",
        "  \n",
        "  def lossOutputDerivative(self,outputData,targetData):\n",
        "    \"\"\"\n",
        "    Compute and return loss derivatives for given output and target data\n",
        "    \"\"\"\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return outputData-targetData\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      return - targetData / outputData \n",
        "\n",
        "  def loss(self,outputData,targetData):\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return 0.5*np.linalg.norm(outputData-targetData)**2\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      return - targetData @ np.log(outputData)\n",
        "\n",
        "\n",
        "class neuralNetwork:\n",
        "  \"\"\"\n",
        "  Class for a neural network made up of multiple layers of perceptrons\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters and hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # create empty math functions object\n",
        "    self.fns = neuralNetworkMathFunctions()\n",
        "\n",
        "    # assign basic hyperparameters to neural network and math functions object\n",
        "    self.hyperparams = {}\n",
        "    self.setHyperparameters(hyperparams)\n",
        "\n",
        "    # initialize the weight and bias matrices of the NN\n",
        "    self.initModel(hyperparams)\n",
        "\n",
        "  \n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # change values of only the hyperparameters specified in the input variable\n",
        "    self.hyperparams.update(hp)\n",
        "    \n",
        "    # use member variables for commonly used hyperparameters\n",
        "    self.layerSizes       = self.hyperparams[\"layerSizes\"]\n",
        "    self.batchSize        = self.hyperparams[\"batchSize\"]\n",
        "    self.learningRate     = self.hyperparams[\"learningRate\"]\n",
        "    self.epochs           = self.hyperparams[\"epochs\"]\n",
        "    self.numLayers        = len(self.layerSizes) - 1\n",
        "    \n",
        "    # set math functions object hyperparameters\n",
        "    assert len(self.hyperparams[\"activations\"])==self.numLayers, \"number of layers (%d) and number of activations (%d) don't match\"%(self.numLayers,len(hp[\"activations\"]))\n",
        "    self.fns.setHyperparameters(self.hyperparams)\n",
        "\n",
        "  def initModel(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters (weight and bias matrices) of neural network\n",
        "    \"\"\"\n",
        "    # checking bounds arg\n",
        "    bounds = (0,1)\n",
        "    if \"initWeightBounds\" in hyperparams.keys():\n",
        "      assert len(hyperparams[\"initWeightBounds\"])==2, \"bounds arg has to be a list/tuple of 2 numbers\"\n",
        "      bounds = hyperparams[\"initWeightBounds\"]\n",
        "\n",
        "    # create list of weight matrices and bias vectors\n",
        "    # the goal is to make the indexing same as that in lecture derivation, hence the dummy values\n",
        "    self.wmat = [np.array([1],ndmin=2)]\n",
        "    self.bias = [np.array([1],ndmin=2)]\n",
        "    \n",
        "    # create random initial parameters and append them to the above initialized lists\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      self.wmat.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],self.layerSizes[i-1])+bounds[0])\n",
        "      self.bias.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],1)+bounds[0])\n",
        "  \n",
        "  def forwardPass(self, inputData):\n",
        "    \"\"\"\n",
        "    Compute output activations of all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                              # --- PSEUDOCODE ---\n",
        "    h     = inputData                              # h[0] = x\n",
        "    hData = [h]                                    #\n",
        "    datasetSize = np.shape(inputData)[1]           #\n",
        "    #                                              #\n",
        "    for i in range(1,self.numLayers+1):            # for i from 1 to L:\n",
        "      a   = self.wmat[i] @ h + self.bias[i]        #     a[i] = w[i] @ h[i-1] + b[i]\n",
        "      h   = self.fns.activation(i,a)               #     h[i] = f(a[i])\n",
        "      hData.append(h)\n",
        "    \n",
        "    return hData\n",
        "  \n",
        "  def backwardPass(self, layerwiseOutputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute weight and bias gradients for all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                                                                        # --- PSEUDOCODE ---\n",
        "    lossData    = self.fns.lossOutputDerivative(layerwiseOutputData[-1], targetData)         # loss_derivative = d(loss)/dh[L]\n",
        "    Delta       = lossData                                                                   # Delta[L] = loss_derivative\n",
        "    datasetSize = np.shape(targetData)[1]                                                    #\n",
        "    biasInputs  = np.array(np.ones(datasetSize),ndmin=2).T                                   #\n",
        "    gradW       = []                                                                         #\n",
        "    gradB       = []                                                                         #\n",
        "    #                                                                                        #\n",
        "    for iFwd in range(self.numLayers):                                                       # for i from L to 1:\n",
        "      i            = self.numLayers - iFwd                                                   #     // index correction\n",
        "      stocBiasCorr = self.fns.activationDerivative(i,y=layerwiseOutputData[i]) * Delta       #     stochastic_bias_corrections = f'(a[i]) * Delta[i]\n",
        "      gW           = stocBiasCorr @ layerwiseOutputData[i-1].T                               #     grad(W[i]) = stochastic_bias_corrections x (h[i-1]).T\n",
        "      gB           = stocBiasCorr @ biasInputs                                               #     grad(b[i]) = sum(stochastic_bias_corrections)\n",
        "      Delta        = self.wmat[i].T @ stocBiasCorr                                           #     Delta[i-1] = W[i] x stochastic_bias_corrections\n",
        "      \n",
        "      gradW.append(gW)\n",
        "      gradB.append(gB)\n",
        "    \n",
        "    # dummy element and order handling\n",
        "    gradW.append(np.array([0],ndmin=2))\n",
        "    gradW.reverse()\n",
        "    gradB.append(np.array([0],ndmin=2))\n",
        "    gradB.reverse()\n",
        "    \n",
        "    return (gradW,gradB)\n",
        "  \n",
        "  def infer(self,inputData,**kwargs):\n",
        "    \"\"\"\n",
        "    Perform inference on input dataset using the neural network\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    \n",
        "    # perform forward pass and return last-layer outputs\n",
        "    return self.forwardPass(inputData)[-1]\n",
        "\n",
        "  def gradtheta_for_batchindex(self, inputData, targetData, datasetSize, batchSize, numBatches, batchIndex):\n",
        "    # create data batches\n",
        "    startIndex  = batchSize * batchIndex\n",
        "    endIndex    = min(startIndex + batchSize, datasetSize)\n",
        "    inputBatch  = inputData[:,startIndex:endIndex]\n",
        "    targetBatch = targetData[:,startIndex:endIndex]\n",
        "    # perform forward and backward passes to compute gradients\n",
        "    layerwiseOutputData = self.forwardPass(inputBatch)\n",
        "    (gradW, gradB)      = self.backwardPass(layerwiseOutputData,targetBatch)\n",
        "    return gradW, gradB\n",
        "\n",
        "  def sgd(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):       \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -self.learningRate * gradW[i]\n",
        "          self.bias[i] += -self.learningRate * gradB[i]\n",
        "\n",
        "  def momentumGD(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -update_w[i]\n",
        "          self.bias[i] += -update_b[i]\n",
        "\n",
        "  def NAG(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    #initialize\n",
        "    update_w = [0]*(self.numLayers+1)\n",
        "    update_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        # perform look ahead parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          self.wmat[i] += -gamma*update_w[i]\n",
        "          self.bias[i] += -gamma*update_b[i]\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "          update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "          self.wmat[i] += -eta*gradW[i]\n",
        "          self.bias[i] += -eta*gradW[i]\n",
        "\n",
        "  def rmsprop(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    beta = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):       \n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          v_w[i] = beta*v_w[i] + (1-beta)*gradW[i]**2\n",
        "          v_b[i] = beta*v_b[i] + (1-beta)*gradB[i]**2 \n",
        "          self.wmat[i] += -eta * (v_w[i] + epsilon)**-0.5 * gradW[i]\n",
        "          self.bias[i] += -eta * (v_b[i] + epsilon)**-0.5 * gradB[i]\n",
        "  \n",
        "  def adam(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = m_w[i]/(1-beta_1**t)\n",
        "          m_b_hat = m_b[i]/(1-beta_1**t)\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "\n",
        "  def nadam(self, inputData, targetData, datasetSize, batchSize, numBatches):\n",
        "    eta = self.learningRate\n",
        "    beta_1 = self.hyperparams[\"beta_1\"]\n",
        "    beta_2 = self.hyperparams[\"beta_2\"]\n",
        "    epsilon = self.hyperparams[\"epsilon\"]\n",
        "    #initialize\n",
        "    m_w = [0]*(self.numLayers+1)\n",
        "    m_b = [0]*(self.numLayers+1)\n",
        "    v_w = [0]*(self.numLayers+1)\n",
        "    v_b = [0]*(self.numLayers+1)\n",
        "    t = 1 #tracks the iteration number\n",
        "    # run training loop\n",
        "    for epoch in range(self.epochs):\n",
        "      for batchIndex in range(numBatches):\n",
        "        (gradW, gradB) = self.gradtheta_for_batchindex(inputData, targetData, datasetSize, batchSize, numBatches, batchIndex)\n",
        "        # perform parameter update\n",
        "        for i in range(1,self.numLayers+1):\n",
        "          m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "          m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "          v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "          v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "          m_w_hat = (beta_1/(1-beta_1**(t+1)))*m_w[i] + ((1-beta_1)/(1-beta_1**t))*gradW[i]\n",
        "          m_b_hat = (beta_1/(1-beta_1**(t+1)))*m_b[i] + ((1-beta_1)/(1-beta_1**t))*gradB[i]\n",
        "          v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "          v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "          self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "          self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "        t += 1\n",
        "\n",
        "\n",
        "  def train(self, inputData, targetData, **kwargs):\n",
        "    \"\"\"\n",
        "    Train the network on the given input and target datasets\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input and target dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    targetData = np.array(targetData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "      targetData = targetData.T\n",
        "    assert np.shape(inputData)[1]==np.shape(targetData)[1], \"input and target datasets have different dataset sizes\"\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint differs from size of input vector given as hyperparameter\"\n",
        "    assert np.shape(targetData)[0]==self.layerSizes[-1], \"size of target datapoint differs from size of target vector given as hyperparameter\"\n",
        "    datasetSize = np.shape(targetData)[1]\n",
        "\n",
        "    # calculate batch parameters\n",
        "    batchSize = datasetSize if self.batchSize==-1 else self.batchSize\n",
        "    numBatches = int(np.ceil(datasetSize / batchSize))\n",
        "\n",
        "    if self.hyperparams[\"optimizer\"] == GDOPT_NONE:\n",
        "      self.sgd(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_MOMENTUM:\n",
        "      self.momentumGD(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NESTEROV:\n",
        "      self.NAG(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_RMSPROP:\n",
        "      self.rmsprop(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_ADAM:\n",
        "      self.adam(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "    elif self.hyperparams[\"optimizer\"] == GDOPT_NADAM:\n",
        "      self.nadam(inputData, targetData, datasetSize, batchSize, numBatches)\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8x1hv7hiXy"
      },
      "source": [
        "hyp = {\n",
        "    \"layerSizes\": [2,4,1],\n",
        "    \"batchSize\": 1,\n",
        "    \"learningRate\": 1,\n",
        "    \"epochs\": 500,\n",
        "    \"activations\": [ACTIVATION_SIGMOID, ACTIVATION_SIGMOID],\n",
        "    \"lossFn\": LOSS_SQERROR,\n",
        "    \"initWeightBounds\": (-0.1,0.1),\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8  # eta scaling hyperparam\n",
        "}\n",
        "\n",
        "x = neuralNetwork(hyp)\n",
        "inp = np.array([[1,0.5],[-0.5,0.25],[1,2]])\n",
        "tar = np.array([[0.5],[0.75],[0.67]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nLeUVg2YFbH"
      },
      "source": [
        "x.setHyperparameters({\n",
        "    \"learningRate\":0.2,\n",
        "    \"epochs\":5000,\n",
        "    \"activations\":[ACTIVATION_RELU, ACTIVATION_RELU]\n",
        "    # square error loss will work as Hamming distance in this case\n",
        "})\n",
        "inp = np.array([[0,0],[0,1],[1,0],[1,1],[0,0],[0,1],[1,0],[1,1]])\n",
        "tar = np.array([[0],[0],[0],[1],[0],[0],[0],[1]])\n",
        "print(\"Target data:\")\n",
        "print(tar.T)\n",
        "print(\"Output before training:\")\n",
        "print(x.infer(inp))\n",
        "print(\"Performing training now\")\n",
        "x.train(inp,tar)\n",
        "print(\"Output after training for %d epochs with learning rate of %.2f:\"%(x.epochs,x.learningRate))\n",
        "print(x.infer(inp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pswfmu-9fdLW"
      },
      "source": [
        "#Reshaping the 'x' data:\r\n",
        "len_1D = x_train.shape[1]*x_train.shape[2]\r\n",
        "x_train_1D = np.array( [x.reshape(len_1D) for x in x_train] )\r\n",
        "x_test_1D  = np.array( [x.reshape(len_1D) for x in x_test] )\r\n",
        "\r\n",
        "frac_val = 0.1\r\n",
        "all_idx = np.arange(len(x_train))\r\n",
        "val_idx = np.random.choice(all_idx, int(frac_val*len(x_train)), replace=False)\r\n",
        "tr2_idx = np.array([i for i in all_idx if i not in val_idx])\r\n",
        "\r\n",
        "x_train2 = x_train_1D(tr2_idx)\r\n",
        "y_train2 = y_train(tr2_idx)\r\n",
        "x_val = x_train_1D(val_idx)\r\n",
        "y_val = y_train(val_idx)\r\n",
        "\r\n",
        "hyp = {\r\n",
        "    \"layerSizes\": [2,4,1],\r\n",
        "    \"batchSize\": 1,\r\n",
        "    \"learningRate\": 1,\r\n",
        "    \"epochs\": 500,\r\n",
        "    \"activations\": [ACTIVATION_SIGMOID, ACTIVATION_SIGMOID],\r\n",
        "    \"lossFn\": LOSS_SQERROR,\r\n",
        "    \"initWeightBounds\": (-0.1,0.1),\r\n",
        "    \"optimizer\": GDOPT_ADAM,\r\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\r\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\r\n",
        "    \"epsilon\": 1e-8  # eta scaling hyperparam\r\n",
        "}\r\n",
        "\r\n",
        "NN = neuralNetwork(hyp)\r\n",
        "NN.fns = neuralNetworkMathFunctions()\r\n",
        "NN.train(x_train2, y_train2)\r\n",
        "y_pred_val = NN.infer(x_val)\r\n",
        "val_loss = NN.fns.loss(y_pred_val, y_val)\r\n",
        "val_err = 1 - np.count_nonzero( y_pred_val == y_val )/len(y_val)\r\n",
        "\r\n",
        "NN.train(x_train_1D, y_train)\r\n",
        "y_pred = NN.infer(x_test_1D)\r\n",
        "loss = NN.fns.loss(y_pred, y_test)\r\n",
        "err = 1 - np.count_nonzero( y_pred == y_test )/len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}