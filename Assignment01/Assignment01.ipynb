{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment01 - Krish.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "q2YwQTDLNXKp",
        "3XjIYQwtM3zs",
        "Dy6jRk4hNCtT",
        "JkW1cW6GNIas",
        "H0LyYRpjhzJF",
        "n7cSe3lwh4Am"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2YwQTDLNXKp"
      },
      "source": [
        "# 1. Library Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw3tDFmwUwnf"
      },
      "source": [
        "!pip install -q wandb"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR831kg0Pg_g"
      },
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist,mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy4UFiqLlAdS"
      },
      "source": [
        "((x_train, y_train), (x_test, y_test)) = fashion_mnist.load_data()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XjIYQwtM3zs"
      },
      "source": [
        "# 2. Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8AocNndPpp2"
      },
      "source": [
        "# function constants\n",
        "ACTIVATION_SIGMOID   = 'sigmoid'\n",
        "ACTIVATION_SOFTMAX   = 'softmax'\n",
        "ACTIVATION_THRESHOLD = 'threshold'\n",
        "ACTIVATION_RELU      = 'relu'\n",
        "ACTIVATION_TANH      = 'tanh'\n",
        "\n",
        "LOSS_SQERROR         = 'sq_error'\n",
        "LOSS_CROSSENTROPY    = 'cross_entropy'\n",
        "\n",
        "GDOPT_NONE           = 'vanilla'\n",
        "GDOPT_MOMENTUM       = 'momentum'\n",
        "GDOPT_NESTEROV       = 'nesterov'\n",
        "GDOPT_ADAGRAD        = 'adagrad'\n",
        "GDOPT_RMSPROP        = 'rmsprop'\n",
        "GDOPT_ADAM           = 'adam'\n",
        "GDOPT_NADAM          = 'nadam'\n",
        "\n",
        "WINIT_RANDOM         = 'random'\n",
        "WINIT_XAVIER         = 'xavier'\n",
        "WINIT_SAVED          = 'saved'\n",
        "\n",
        "# function limits\n",
        "EXP_INPUT_UPPER_TOL  = 300\n",
        "EXP_INPUT_LOWER_TOL  = -300\n",
        "EXP_OUTPUT_UPPER_TOL = 10**EXP_INPUT_UPPER_TOL\n",
        "EXP_OUTPUT_LOWER_TOL = 10**EXP_INPUT_LOWER_TOL\n",
        "DIVZERO_TOL          = 1e-300\n",
        "\n",
        "class neuralNetwork:\n",
        "  \"\"\"\n",
        "  Class for a neural network made up of multiple layers of perceptrons\n",
        "  \"\"\"\n",
        "  def __init__(self,hyperparams):\n",
        "    \"\"\"\n",
        "    Initialize parameters and hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # assign default hyperparameters to neural network\n",
        "    self.hyperparams = {\n",
        "        \"optmizer\": GDOPT_NONE,\n",
        "        \"initWeightMethod\": WINIT_RANDOM,\n",
        "        \"initWeightBounds\": (-0.5,0.5),\n",
        "        \"regparam\": 0,\n",
        "        \"wandb\": False\n",
        "    }\n",
        "    self.setHyperparameters(hyperparams)\n",
        "\n",
        "    # initialize the weight and bias matrices of the NN\n",
        "    self.initModel()\n",
        "  \n",
        "  def setHyperparameters(self,hp):\n",
        "    \"\"\"\n",
        "    Set hyperparameters of neural network\n",
        "    \"\"\"\n",
        "    # change values of only the hyperparameters specified in the input variable\n",
        "    self.hyperparams.update(hp)\n",
        "    \n",
        "    # use member variables for commonly used hyperparameters\n",
        "    self.layerSizes       = self.hyperparams[\"layerSizes\"]\n",
        "    self.batchSize        = self.hyperparams[\"batchSize\"]\n",
        "    self.learningRate     = self.hyperparams[\"learningRate\"]\n",
        "    self.epochs           = self.hyperparams[\"epochs\"]\n",
        "    self.numLayers        = len(self.layerSizes) - 1\n",
        "    \n",
        "    # set math functions object hyperparameters\n",
        "    assert len(self.hyperparams[\"activations\"])==self.numLayers, \"number of layers (%d) and number of activations (%d) don't match\"%(self.numLayers,len(hp[\"activations\"]))\n",
        "    self.activations = self.hyperparams[\"activations\"]\n",
        "    self.lossFn = self.hyperparams[\"lossFn\"]\n",
        "    self.regparam = self.hyperparams[\"regparam\"]\n",
        "\n",
        "  def initModel(self):\n",
        "    \"\"\"\n",
        "    Initialize parameters (weight and bias matrices) of neural network\n",
        "    \"\"\"\n",
        "    # checking bounds arg\n",
        "    assert len(self.hyperparams[\"initWeightBounds\"])==2, \"bounds arg has to be a list/tuple of 2 numbers\"\n",
        "    bounds = self.hyperparams[\"initWeightBounds\"]\n",
        "\n",
        "    # create list of weight matrices and bias vectors\n",
        "    # the goal is to make the indexing same as that in lecture derivation, hence the dummy values\n",
        "    self.wmat = [np.array([1],ndmin=2)]\n",
        "    self.bias = [np.array([1],ndmin=2)]\n",
        "    \n",
        "    # create random initial parameters and append them to the above initialized lists\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      if self.hyperparams[\"initWeightMethod\"]==WINIT_XAVIER:\n",
        "        bounds = (-1/(self.layerSizes[i-1])**0.5,1/(self.layerSizes[i-1])**0.5)\n",
        "      self.wmat.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],self.layerSizes[i-1])+bounds[0])\n",
        "      self.bias.append((bounds[1]-bounds[0])*np.random.rand(self.layerSizes[i],1)+bounds[0])\n",
        "\n",
        "  def activation(self,layerNum,x):\n",
        "    \"\"\"\n",
        "    Compute and return activation values for a given layer and its sum values\n",
        "    \"\"\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    if   self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "      return 1/(1+np.exp(-np.maximum(np.minimum(x,EXP_INPUT_UPPER_TOL),EXP_INPUT_LOWER_TOL)))\n",
        "    elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "      z = np.exp(np.maximum(x - np.amax(x, axis=0) + EXP_INPUT_UPPER_TOL, EXP_INPUT_LOWER_TOL))\n",
        "      return z/np.sum(z, axis=0)\n",
        "    elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "      return (x>=0)+0\n",
        "    elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "      return np.maximum(x,0)\n",
        "    elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "      return np.tanh(np.maximum(np.minimum(x, EXP_INPUT_UPPER_TOL), EXP_INPUT_LOWER_TOL))\n",
        "  \n",
        "  def activationDerivative(self,layerNum,**kwargs):\n",
        "    \"\"\"\n",
        "    Compute and return activation derivative values for a given layer and its partial-sum or output values depending on the given argument\n",
        "    \"\"\"\n",
        "    assert ( len(kwargs.keys())==1 and np.any([_ in kwargs.keys() for _ in [\"x\",\"y\"]]) ), \"activationDerivative argument malformed. \\\n",
        "    Use activationDerivative(layerNum,x=x_val) or activationDerivative(layerNum,y=y_val)\"\n",
        "    layerNum -= 1 # index adjustment\n",
        "    \n",
        "    if \"y\" in kwargs.keys():\n",
        "      y = kwargs[\"y\"]\n",
        "      if   self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return y*(1-y)\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (y>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_TANH:\n",
        "        return 1 - y**2\n",
        "    else:\n",
        "      x = kwargs[\"x\"]\n",
        "      if   self.activations[layerNum]==ACTIVATION_SIGMOID:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_SOFTMAX:\n",
        "        z = np.exp(x)\n",
        "        s = np.sum(z)\n",
        "        return z*(s-z)/(s**2)\n",
        "      elif self.activations[layerNum]==ACTIVATION_THRESHOLD:\n",
        "        return np.exp(-x)/(1+np.exp(-x))**2\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return (x>=0)+0\n",
        "      elif self.activations[layerNum]==ACTIVATION_RELU:\n",
        "        return 1-np.tanh(x)**2\n",
        "  \n",
        "  def loss(self, outputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute and return loss values for given output and target data\n",
        "    \"\"\"\n",
        "    # non-regularised loss\n",
        "    nonRegLoss     = 0\n",
        "    if   self.lossFn == LOSS_SQERROR:\n",
        "      nonRegLoss   = 0.5 * np.sum(np.linalg.norm(outputData - targetData, axis=0)**2)\n",
        "    elif self.lossFn == LOSS_CROSSENTROPY:\n",
        "      nonRegLoss   = - np.sum(targetData * np.log( np.maximum(outputData, EXP_OUTPUT_LOWER_TOL)) \n",
        "      + (1-targetData) * np.log( np.maximum((1-outputData), EXP_OUTPUT_LOWER_TOL)))\n",
        "    \n",
        "    # weight decay regularisation loss\n",
        "    wdRegLoss      = 0\n",
        "    if self.hyperparams[\"regparam\"]!=0:\n",
        "      modW_sq      = np.sum( np.array( [ np.linalg.norm(W) for W in self.wmat ] ) )**2\n",
        "      modB_sq      = np.sum( np.array( [ np.linalg.norm(B) for B in self.bias ] ) )**2\n",
        "      modtheta_sq  = modW_sq + modB_sq\n",
        "      wdRegLoss    = 0.5 * self.hyperparams[\"regparam\"] * modtheta_sq\n",
        "    \n",
        "    return (nonRegLoss + wdRegLoss)/len(targetData[0])\n",
        "  \n",
        "  def lossOutputDerivative(self, outputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute and return (non-regularised) loss derivatives for given output and target data\n",
        "    \"\"\"\n",
        "    if self.lossFn==LOSS_SQERROR:\n",
        "      return outputData - targetData\n",
        "    elif self.lossFn==LOSS_CROSSENTROPY:\n",
        "      dat = outputData\n",
        "      return ( - targetData/np.maximum(dat,DIVZERO_TOL) + (1-targetData)/np.maximum(1-dat,DIVZERO_TOL) ) # for no zeros/ones in output\n",
        "  \n",
        "  def lossMetrics(self, outputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute loss metrics (loss, accuracy) for given data\n",
        "    \"\"\"\n",
        "    # calculate metrics\n",
        "    loss = self.loss(outputData, targetData)\n",
        "    acc  = np.count_nonzero( np.argmax(targetData, axis=0) == np.argmax(outputData, axis=0) )/len(targetData[0])\n",
        "    return loss, acc\n",
        "  \n",
        "  def forwardPass(self, inputData):\n",
        "    \"\"\"\n",
        "    Compute output activations of all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                              # --- PSEUDOCODE ---\n",
        "    h     = inputData                              # h[0] = x\n",
        "    hData = [h]                                    #\n",
        "    datasetSize = np.shape(inputData)[1]           #\n",
        "    #                                              #\n",
        "    for i in range(1,self.numLayers+1):            # for i from 1 to L:\n",
        "      a   = self.wmat[i] @ h + self.bias[i]        #     a[i] = w[i] @ h[i-1] + b[i]\n",
        "      h   = self.activation(i,a)                   #     h[i] = f(a[i])\n",
        "      hData.append(h)\n",
        "    \n",
        "    return hData\n",
        "  \n",
        "  def backwardPass(self, layerwiseOutputData, targetData):\n",
        "    \"\"\"\n",
        "    Compute weight and bias gradients for all layers of neural network\n",
        "    Data can also be given as sets of datapoints (dimensions being layer dimension x dataset size)\n",
        "    \"\"\"\n",
        "    #                                                                                        # --- PSEUDOCODE ---\n",
        "    lossData    = self.lossOutputDerivative(layerwiseOutputData[-1], targetData)             # loss_derivative = d(loss)/dh[L]\n",
        "    Delta       = lossData                                                                   # Delta[L] = loss_derivative\n",
        "    datasetSize = np.shape(targetData)[1]                                                    #\n",
        "    biasInputs  = np.array(np.ones(datasetSize),ndmin=2).T                                   #\n",
        "    gradW       = []                                                                         #\n",
        "    gradB       = []                                                                         #\n",
        "    #                                                                                        #\n",
        "    for iFwd in range(self.numLayers):                                                       # for i from L to 1:\n",
        "      i            = self.numLayers - iFwd                                                   #     // index correction\n",
        "      stocBiasCorr = self.activationDerivative(i,y=layerwiseOutputData[i]) * Delta           #     stochastic_bias_corrections = f'(a[i]) * Delta[i]\n",
        "      gW           = (stocBiasCorr @ layerwiseOutputData[i-1].T + self.regparam*self.wmat[i])/len(targetData[0])  #     grad(W[i]) = stochastic_bias_corrections x (h[i-1]).T\n",
        "      gB           = (stocBiasCorr @ biasInputs + self.regparam*self.bias[i])/len(targetData[0])                  #     grad(b[i]) = sum(stochastic_bias_corrections)\n",
        "      Delta        = self.wmat[i].T @ stocBiasCorr                                           #     Delta[i-1] = W[i] x stochastic_bias_corrections\n",
        "      \n",
        "      gradW.append(gW)\n",
        "      gradB.append(gB)\n",
        "    \n",
        "    # dummy element and order handling\n",
        "    gradW.append(np.array([0],ndmin=2))\n",
        "    gradW.reverse()\n",
        "    gradB.append(np.array([0],ndmin=2))\n",
        "    gradB.reverse()\n",
        "    \n",
        "    return (gradW,gradB)\n",
        "\n",
        "  def initOptimizerCollector(self):\n",
        "    \"\"\"\n",
        "    Create variable for optimizer state collection\n",
        "    \"\"\"\n",
        "    optType = self.hyperparams[\"optimizer\"]\n",
        "    opt = {\n",
        "        \"t\": 1\n",
        "    }\n",
        "    \n",
        "    if   optType == GDOPT_NONE:\n",
        "      pass\n",
        "    \n",
        "    elif optType == GDOPT_MOMENTUM:\n",
        "      opt[\"update_w\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"update_b\"] = [0]*(self.numLayers+1)\n",
        "    \n",
        "    elif optType == GDOPT_NESTEROV:\n",
        "      opt[\"update_w\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"update_b\"] = [0]*(self.numLayers+1)\n",
        "    \n",
        "    elif optType == GDOPT_RMSPROP:\n",
        "      opt[\"v_w\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"v_b\"] = [0]*(self.numLayers+1)\n",
        "    \n",
        "    elif optType == GDOPT_ADAM:\n",
        "      opt[\"m_w\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"m_b\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"v_w\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"v_b\"] = [0]*(self.numLayers+1)\n",
        "    \n",
        "    elif optType == GDOPT_NADAM:\n",
        "      opt[\"m_w\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"m_b\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"v_w\"] = [0]*(self.numLayers+1)\n",
        "      opt[\"v_b\"] = [0]*(self.numLayers+1)\n",
        "    \n",
        "    return opt\n",
        "  \n",
        "  def updateParameters(self, inputData, targetData, opt):\n",
        "    \"\"\"\n",
        "    Perform parameter updates for given input and target datapoints\n",
        "    \"\"\"\n",
        "    # pre-common processing\n",
        "    optType = self.hyperparams[\"optimizer\"]\n",
        "    eta = self.hyperparams[\"learningRate\"]\n",
        "    gamma = self.hyperparams[\"beta_1\"]\n",
        "    t = opt[\"t\"]\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      if optType == GDOPT_NESTEROV:\n",
        "        update_w = opt[\"update_w\"]; update_b = opt[\"update_b\"]\n",
        "        self.wmat[i] += -gamma*update_w[i]\n",
        "        self.bias[i] += -gamma*update_b[i]\n",
        "\n",
        "    # common processing\n",
        "    layerwiseOutputData = self.forwardPass(inputData)\n",
        "    (gradW, gradB)      = self.backwardPass(layerwiseOutputData,targetData)\n",
        "    \n",
        "    # post-common processing\n",
        "    for i in range(1,self.numLayers+1):\n",
        "      if   optType == GDOPT_NONE:\n",
        "        self.wmat[i] += -eta * gradW[i]\n",
        "        self.bias[i] += -eta * gradB[i]\n",
        "      \n",
        "      elif optType == GDOPT_MOMENTUM:\n",
        "        update_w = opt[\"update_w\"]; update_b = opt[\"update_b\"]\n",
        "        update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "        update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "        self.wmat[i] += -update_w[i]\n",
        "        self.bias[i] += -update_b[i]\n",
        "      \n",
        "      elif optType == GDOPT_NESTEROV:\n",
        "        update_w = opt[\"update_w\"]; update_b = opt[\"update_b\"]\n",
        "        update_w[i] = gamma*update_w[i] + eta*gradW[i]\n",
        "        update_b[i] = gamma*update_b[i] + eta*gradB[i]\n",
        "        self.wmat[i] += -eta*gradW[i]\n",
        "        self.bias[i] += -eta*gradB[i]\n",
        "      \n",
        "      elif optType == GDOPT_RMSPROP:\n",
        "        beta = self.hyperparams[\"beta_2\"]; epsilon = self.hyperparams[\"epsilon\"]\n",
        "        v_w = opt[\"v_w\"]; v_b = opt[\"v_b\"]\n",
        "        v_w[i] = beta*v_w[i] + (1-beta)*gradW[i]**2\n",
        "        v_b[i] = beta*v_b[i] + (1-beta)*gradB[i]**2 \n",
        "        self.wmat[i] += -eta * (v_w[i] + epsilon)**-0.5 * gradW[i]\n",
        "        self.bias[i] += -eta * (v_b[i] + epsilon)**-0.5 * gradB[i]\n",
        "      \n",
        "      elif optType == GDOPT_ADAM:\n",
        "        beta_1 = gamma; beta_2 = self.hyperparams[\"beta_2\"]; epsilon = self.hyperparams[\"epsilon\"]\n",
        "        m_w = opt[\"m_w\"]; m_b = opt[\"m_b\"]\n",
        "        v_w = opt[\"v_w\"]; v_b = opt[\"v_b\"]\n",
        "        m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "        m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "        v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "        v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "        m_w_hat = m_w[i]/(1-beta_1**t)\n",
        "        m_b_hat = m_b[i]/(1-beta_1**t)\n",
        "        v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "        v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "        self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "        self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "      \n",
        "      elif optType == GDOPT_NADAM:\n",
        "        beta_1 = gamma; beta_2 = self.hyperparams[\"beta_2\"]; epsilon = self.hyperparams[\"epsilon\"]\n",
        "        m_w = opt[\"m_w\"]; m_b = opt[\"m_b\"]\n",
        "        v_w = opt[\"v_w\"]; v_b = opt[\"v_b\"]\n",
        "        m_w[i] = beta_1*m_w[i] + (1-beta_1)*gradW[i]\n",
        "        m_b[i] = beta_1*m_b[i] + (1-beta_1)*gradB[i]\n",
        "        v_w[i] = beta_2*v_w[i] + (1-beta_2)*gradW[i]**2\n",
        "        v_b[i] = beta_2*v_b[i] + (1-beta_2)*gradB[i]**2\n",
        "        m_w_hat = (beta_1/(1-beta_1**(t+1)))*m_w[i] + ((1-beta_1)/(1-beta_1**t))*gradW[i]\n",
        "        m_b_hat = (beta_1/(1-beta_1**(t+1)))*m_b[i] + ((1-beta_1)/(1-beta_1**t))*gradB[i]\n",
        "        v_w_hat = v_w[i]/(1-beta_2**t)\n",
        "        v_b_hat = v_b[i]/(1-beta_2**t)\n",
        "        self.wmat[i] += -eta * (v_w_hat + epsilon)**-0.5 * m_w_hat\n",
        "        self.bias[i] += -eta * (v_b_hat + epsilon)**-0.5 * m_b_hat\n",
        "    \n",
        "    # increment batch counter\n",
        "    opt[\"t\"] += 1\n",
        "  \n",
        "  def infer(self,inputData,**kwargs):\n",
        "    \"\"\"\n",
        "    Perform inference on input dataset using the neural network\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input dimensions\n",
        "    inputData  = np.array(inputData,ndmin=2)\n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputData  = inputData.T\n",
        "    assert np.shape(inputData)[0]==self.layerSizes[0], \"size of input datapoint (%d) differs from size of input vector given as hyperparameter (%d)\"%(np.shape(inputData)[0],self.layerSizes[0])\n",
        "    \n",
        "    # perform forward pass and return last-layer outputs and loss metrics\n",
        "    outputData = self.forwardPass(inputData)[-1]\n",
        "    if \"targetData\" in kwargs:\n",
        "      return outputData, self.lossMetrics(outputData, kwargs[\"targetData\"])\n",
        "    else:\n",
        "      return outputData\n",
        "  \n",
        "  def train(self, inputTrainingData, targetTrainingData, inputValidationData = [], targetValidationData = [], **kwargs):\n",
        "    \"\"\"\n",
        "    Train the network on the given input and target datasets\n",
        "    Unless colwiseData=True is given as an argument, data will be interpreted as being dataset size x layer dimension\n",
        "    \"\"\"\n",
        "    # resolving input and target dimensions\n",
        "    inputTrainingData    = np.array(inputTrainingData,ndmin=2)\n",
        "    targetTrainingData   = np.array(targetTrainingData,ndmin=2)\n",
        "    inputValidationData  = np.array(inputValidationData,ndmin=2)\n",
        "    targetValidationData = np.array(targetValidationData,ndmin=2)\n",
        "    \n",
        "    if \"colwiseData\" in kwargs and kwargs[\"colwiseData\"]==True:\n",
        "      pass\n",
        "    else:\n",
        "      inputTrainingData    = inputTrainingData.T\n",
        "      targetTrainingData   = targetTrainingData.T\n",
        "      inputValidationData  = inputValidationData.T\n",
        "      targetValidationData = targetValidationData.T\n",
        "    \n",
        "    assert np.shape(inputTrainingData)[1]==np.shape(targetTrainingData)[1], \"input (%d) and target (%d) training datasets have different sizes\"%(np.shape(inputTrainingData)[1],np.shape(targetTrainingData)[1])\n",
        "    assert np.shape(inputTrainingData)[0]==self.layerSizes[0], \"size of input training datapoint (%d) differs from size of input training vector given as hyperparameter (%d)\"%(np.shape(inputTrainingData)[0]==self.layerSizes[0])\n",
        "    assert np.shape(targetTrainingData)[0]==self.layerSizes[-1], \"size of target training datapoint (%d) differs from size of target training vector given as hyperparameter (%d)\"%(np.shape(targetTrainingData)[0],self.layerSizes[-1])\n",
        "    \n",
        "    assert np.shape(inputValidationData)[1]==np.shape(targetValidationData)[1], \"input (%d) and target (%d) validation datasets have different sizes\"%(np.shape(inputValidationData)[1],np.shape(targetValidationData)[1])\n",
        "    assert np.shape(inputValidationData)[0]==self.layerSizes[0], \"size of input validation datapoint differs from size of input validation vector given as hyperparameter\"%(np.shape(inputValidationData)[0]==self.layerSizes[0])\n",
        "    assert np.shape(targetValidationData)[0]==self.layerSizes[-1], \"size of target validation datapoint differs from size of target validation vector given as hyperparameter\"%(np.shape(targetValidationData)[0],self.layerSizes[-1])\n",
        "    \n",
        "    datasetSize = np.shape(targetTrainingData)[1]\n",
        "\n",
        "    # calculate batch parameters\n",
        "    batchSize = datasetSize if self.batchSize==-1 else self.batchSize\n",
        "    numBatches = int(np.ceil(datasetSize / batchSize))\n",
        "\n",
        "    # initialize optimizer state processing object\n",
        "    opt = self.initOptimizerCollector()\n",
        "\n",
        "    # run training loop\n",
        "    for epoch in range(self.hyperparams[\"epochs\"]):\n",
        "      # put in wandb's log if defined\n",
        "      if self.hyperparams[\"wandb\"]:\n",
        "        # calculate and log loss and accuracy\n",
        "        _, (loss_train, acc_train) = self.infer(inputTrainingData,   colwiseData = True, targetData = targetTrainingData)\n",
        "        _, (loss_val, acc_val)     = self.infer(inputValidationData, colwiseData = True, targetData = targetValidationData)\n",
        "\n",
        "        wandb.log({\"Epoch\": epoch, \\\n",
        "                    \"Training Loss\": loss_train, \"Training Accuracy\": acc_train, \\\n",
        "                    \"Validation Loss\": loss_val, \"Validation Accuracy\": acc_val })\n",
        "      # run batch loop\n",
        "      for batchIndex in range(numBatches):       \n",
        "        # create data batches\n",
        "        startIndex            = batchSize * batchIndex\n",
        "        endIndex              = min(startIndex + batchSize, datasetSize)\n",
        "        inputTrainingBatch    = inputTrainingData[:,startIndex:endIndex]\n",
        "        targetTrainingBatch   = targetTrainingData[:,startIndex:endIndex]\n",
        "\n",
        "        # perform parameter update\n",
        "        self.updateParameters(inputTrainingBatch, targetTrainingBatch, opt)\n",
        "        tempLD = self.lossOutputDerivative(self.infer(inputTrainingBatch,colwiseData=True),targetTrainingBatch)\n",
        "        '''f.write(\n",
        "             \"BATCH %d-%d\\n1. Outputs:\"%(startIndex+1,endIndex) \n",
        "             + str(self.infer(inputTrainingData,colwiseData = True)) \n",
        "             + '\\n2. Loss Output Derivative (Norm = %.3f)'%(np.linalg.norm(tempLD))\n",
        "             + str(tempLD)\n",
        "             + '\\n\\n\\n')\n",
        "      f.close()'''\n",
        "\n",
        "    # put in wandb's log if defined\n",
        "    if self.hyperparams[\"wandb\"]:\n",
        "      # calculate and log loss and accuracy\n",
        "      _, (loss_train, acc_train) = self.infer(inputTrainingData,   colwiseData = True, targetData = targetTrainingData)\n",
        "      _, (loss_val, acc_val)     = self.infer(inputValidationData, colwiseData = True, targetData = targetValidationData)\n",
        "\n",
        "      wandb.log({\"Epoch\": epoch+1, \\\n",
        "                  \"Training Loss\": loss_train, \"Training Accuracy\": acc_train, \\\n",
        "                  \"Validation Loss\": loss_val, \"Validation Accuracy\": acc_val })"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy6jRk4hNCtT"
      },
      "source": [
        "# 3. Dataset Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RlsrEmoU0NA"
      },
      "source": [
        "classes, idx_sample_class = np.unique(y_train, return_index=True)\n",
        "\n",
        "#Reshaping the 'x' data:\n",
        "len_1D = x_train.shape[1]*x_train.shape[2]\n",
        "x_train_1D = np.array( [x.reshape(len_1D) for x in x_train] )\n",
        "x_test_1D  = np.array( [x.reshape(len_1D) for x in x_test] )\n",
        "\n",
        "#Transforming 'y' data - changing scalar i to vector e(i)\n",
        "y_train_1D = np.zeros( (len(y_train), len(classes)) , dtype = float)\n",
        "for i in range(len(y_train)):\n",
        "  y_train_1D[i, y_train[i]] = 1.0\n",
        "y_test_1D = np.zeros( (len(y_test), len(classes)) , dtype = float)\n",
        "for i in range(len(y_test)):\n",
        "  y_test_1D[i, y_test[i]] = 1.0\n",
        "\n",
        "frac_val = 0.1\n",
        "all_idx = np.arange(len(x_train))\n",
        "val_idx = np.random.choice(all_idx, int(frac_val*len(x_train)), replace=False)\n",
        "tr2_idx = np.array([i for i in all_idx if i not in val_idx])\n",
        "\n",
        "x_train2 = x_train_1D[tr2_idx]\n",
        "y_train2 = y_train_1D[tr2_idx]\n",
        "x_val = x_train_1D[val_idx]\n",
        "y_val = y_train_1D[val_idx]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkW1cW6GNIas"
      },
      "source": [
        "# 4. Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0LyYRpjhzJF"
      },
      "source": [
        "## 4.1. Local Runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdW6LCNmVNTe"
      },
      "source": [
        "hyp = {\n",
        "    \"layerSizes\": [len(x_train_1D[0]),len(y_train_1D[0])],\n",
        "    \"batchSize\": -1,\n",
        "    \"learningRate\": 1e-2,\n",
        "    \"epochs\": 500,\n",
        "    \"activations\": [ ACTIVATION_SOFTMAX],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\n",
        "    \"initWeightBounds\": (-1,1),\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\n",
        "    \"optimizer\": GDOPT_NONE,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    \"regparam\": 0,\n",
        "    \"wandb\": False\n",
        "}\n",
        "\n",
        "NN = neuralNetwork(hyp)\n",
        "NN.train(x_train2[:128]/255, y_train2[:128], x_val[:128]/255, y_val[:128])\n",
        "# Note that x_val, y_val are given as an input just to calculate the loss and error at each epoch.\n",
        "# They are not used anywhere to train the neural network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7cSe3lwh4Am"
      },
      "source": [
        "## 4.2. W&B Runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL4Ptup5bsrD"
      },
      "source": [
        "!rm -rf outputs && mkdir outputs\n",
        "wandb.init(project= \"sample-runs\")\n",
        "hyp = {\n",
        "    \"layerSizes\": [len(x_train_1D[0]),128,128,128,128,128,len(y_train_1D[0])],\n",
        "    \"batchSize\": 32,\n",
        "    \"learningRate\": 1e-3,\n",
        "    \"epochs\": 2,\n",
        "    \"activations\": [ ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_SOFTMAX],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\n",
        "    \"initWeightBounds\": (-1,1),\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,    # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999,  # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    \"regparam\": 0.5,\n",
        "    \"wandb\": True\n",
        "}\n",
        "\n",
        "NN = neuralNetwork(hyp)\n",
        "NN.train(x_train2[:]/255, y_train2[:], x_val/255, y_val)\n",
        "# Note that x_val, y_val are given as an input just to calculate the loss and error at each epoch.\n",
        "# They are not used anywhere to train the neural network\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRrqDIofNPJQ"
      },
      "source": [
        "# 5. Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTFkOnkUYJk0"
      },
      "source": [
        "def runSweep():\n",
        "  wandb.init()\n",
        "  hyp = {}\n",
        "  hyp[\"wandb\"] = True\n",
        "  cfg = wandb.config\n",
        "\n",
        "  hyp[\"epochs\"] = cfg.epochs\n",
        "\n",
        "  layersHidden = []\n",
        "  for i in range(cfg.numHiddenLayers):\n",
        "    layersHidden.append(cfg.hiddenLayerSize)\n",
        "  hyp[\"layerSizes\"] = [len(x_train_1D[0])] + layersHidden + [len(y_train_1D[0])]\n",
        "\n",
        "  hyp[\"regparam\"] = cfg.L2Reg\n",
        "\n",
        "  hyp[\"learningRate\"] = cfg.learningRate\n",
        "\n",
        "  hyp[\"optimizer\"] = cfg.optimizer\n",
        "\n",
        "  hyp[\"batchSize\"] = cfg.batchSize\n",
        "\n",
        "  hyp[\"initWeightMethod\"] = cfg.initWeightMethod\n",
        "\n",
        "  hyp[\"activations\"] = []\n",
        "  for i in range(cfg.numHiddenLayers):\n",
        "    hyp[\"activations\"].append(cfg.activationFn)\n",
        "  hyp[\"activations\"].append(ACTIVATION_SOFTMAX)\n",
        "\n",
        "  hyp[\"lossFn\"] = cfg.loss\n",
        "\n",
        "  hyp.update({\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-8,  # eta scaling hyperparam\n",
        "    })\n",
        "\n",
        "  run_name = wandb.run.name\n",
        "  wandb.run.name = run_name+'_ep_'+str(hyp[\"epochs\"])+'_numhl_'+str(cfg.numHiddenLayers)\\\n",
        "  +'_hlsize_'+str(cfg.hiddenLayerSize)+'_l2reg_'+str(hyp[\"regparam\"])\\\n",
        "  +'_eta_'+str(hyp[\"learningRate\"])+'_'+str(hyp[\"optimizer\"])\\\n",
        "  +'_bs_'+str(hyp[\"batchSize\"])+'_'+str(hyp[\"initWeightMethod\"])+'_'+str(hyp[\"lossFn\"]+'_'+str(cfg.activationFn)) \n",
        "  wandb.run.save()\n",
        "\n",
        "  nn = neuralNetwork(hyp)\n",
        "  nn.train(x_train2/255, y_train2, x_val/255, y_val)\n",
        "\n",
        "sweepCfg = {\n",
        "    \"name\":\"NN Fashion MNIST Parameter Sweep - Cross Entropy Loss\", \n",
        "    \"metric\":{\n",
        "        \"name\":\"Validation Accuracy\",\n",
        "        \"goal\":\"maximize\"\n",
        "    }, \n",
        "    \"method\": \"bayes\", \n",
        "    \"parameters\":{\n",
        "        \"epochs\":{\n",
        "          \"values\":[25]\n",
        "        },\n",
        "        \"numHiddenLayers\":{\n",
        "          \"values\":[3,4,5]\n",
        "        },\n",
        "        \"hiddenLayerSize\":{\n",
        "          \"values\":[32,64,128]\n",
        "        },\n",
        "        \"L2Reg\":{\n",
        "          \"values\":[0,5e-4,0.5]\n",
        "        },\n",
        "        \"learningRate\":{\n",
        "          \"values\":[1e-3, 1e-5]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "          \"values\":[GDOPT_NONE, GDOPT_MOMENTUM, GDOPT_NESTEROV, GDOPT_RMSPROP, GDOPT_ADAM, GDOPT_NADAM]\n",
        "        },\n",
        "        \"batchSize\":{\n",
        "          \"values\":[32,64,128]\n",
        "        },\n",
        "        \"initWeightMethod\":{\n",
        "          \"values\":[WINIT_RANDOM, WINIT_XAVIER]\n",
        "        },\n",
        "        \"activationFn\":{\n",
        "          \"values\":[ACTIVATION_SIGMOID, ACTIVATION_RELU, ACTIVATION_TANH]\n",
        "        },\n",
        "        \"loss\":{\n",
        "          \"values\": [LOSS_CROSSENTROPY]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweepId = wandb.sweep(sweepCfg)\n",
        "wandb.agent(sweepId, function = runSweep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8W7xnoYOGeI"
      },
      "source": [
        "# 6. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3MBe8ee7prL"
      },
      "source": [
        "wandb.init(project=\"Test run - 1\")\r\n",
        "hyp = {\r\n",
        "    \"layerSizes\": [len(x_train_1D[0]), 128, 128, 128, 128, 128,len(y_train_1D[0])],\r\n",
        "    \"batchSize\": 64,\r\n",
        "    \"learningRate\": 1e-3,\r\n",
        "    \"epochs\": 1,\r\n",
        "    \"activations\": [ ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_SOFTMAX ],\r\n",
        "    \"lossFn\": LOSS_SQERROR, #LOSS_CROSSENTROPY LOSS_SQERROR\r\n",
        "    \"initWeightBounds\": (-1,1),\r\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\r\n",
        "    \"optimizer\": GDOPT_ADAM,\r\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\r\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\r\n",
        "    \"epsilon\": 1e-3,  # eta scaling hyperparam\r\n",
        "    \"regparam\": 5e-4,\r\n",
        "    \"wandb\": True,\r\n",
        "}\r\n",
        "\r\n",
        "run_name = wandb.run.name\r\n",
        "wandb.run.name = run_name + '_ep_' + str(hyp[\"epochs\"]) + '_numhl_' + str(5)\\\r\n",
        "+'_hlsize_'+str(128)+'_l2reg_'+str(hyp[\"regparam\"])\\\r\n",
        "+'_eta_'+str(hyp[\"learningRate\"])+'_'+str(hyp[\"optimizer\"])\\\r\n",
        "+'_bs_'+str(hyp[\"batchSize\"])+'_'+str(hyp[\"initWeightMethod\"])+'_'+str(hyp[\"lossFn\"])+'_tanh'\r\n",
        "wandb.run.save()\r\n",
        "\r\n",
        "#wandb.config.update(hyp)\r\n",
        "\r\n",
        "NN = neuralNetwork(hyp)\r\n",
        "NN.train(x_train_1D/(255), y_train_1D, x_test_1D/(255), y_test_1D)\r\n",
        "# Note that x_val, y_val are given as an input just to calculate the loss and error at each epoch.\r\n",
        "# They are not used anywhere to train the neural network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4NvnzOURnNv"
      },
      "source": [
        "y_pred_1D = NN.infer(x_test_1D/(255))\r\n",
        "y_pred = np.argmax(y_pred_1D, axis=0)\r\n",
        "conf_mat = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZnFVeUoW22B"
      },
      "source": [
        "X, Y = np.meshgrid( np.arange(len(classes))-0.3, np.arange(len(classes))-0.3 )\r\n",
        "x_vec = X.ravel()\r\n",
        "y_vec = Y.ravel()\r\n",
        "z_vec = 0\r\n",
        "\r\n",
        "dx = 0.6*np.ones_like(x_vec)\r\n",
        "dy = 0.6*np.ones_like(y_vec)\r\n",
        "dz = conf_mat.ravel()\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(15,10))\r\n",
        "ax = fig.add_subplot(111, projection='3d')\r\n",
        "\r\n",
        "minima = np.amin(conf_mat)\r\n",
        "maxima = np.amax(conf_mat)\r\n",
        "\r\n",
        "norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\r\n",
        "mapper = cm.ScalarMappable(norm=norm, cmap=cm.viridis)\r\n",
        "\r\n",
        "color_vec = [ mapper.to_rgba(v) for v_vec in conf_mat for v in v_vec]\r\n",
        "\r\n",
        "bar3d = ax.bar3d(x_vec, y_vec, z_vec, dx, dy, dz,\r\n",
        "         zsort='average', color=color_vec, shade=True,alpha=0.4, edgecolor='black')\r\n",
        "ax.set_xlabel('Predicted Class')\r\n",
        "ax.set_ylabel('True Class')\r\n",
        "ax.set_xticks(np.arange(10))\r\n",
        "ax.set_yticks(np.arange(10))\r\n",
        "ax.set_title('Confusion Matrix')\r\n",
        "ax.view_init(50,-100)\r\n",
        "\r\n",
        "fig.colorbar(bar3d, boundaries=np.linspace(np.amin(dz), np.amax(dz), 20 ))\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkALZ7w4ZfUr"
      },
      "source": [
        "wandb.log({\"Confusion Matrix\": wandb.Image(fig)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwmmReO94bsc"
      },
      "source": [
        "# 7. Three Best Runs from Fashion-MNIST on MNIST Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOkAyblj8Yx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f2d4d1-8a4a-4983-cae8-e389d4ce4dad"
      },
      "source": [
        "((x_train, y_train), (x_test, y_test)) = mnist.load_data()\n",
        "classes, idx_sample_class = np.unique(y_train, return_index=True)\n",
        "sample_images = x_train[ idx_sample_class ]\n",
        "\n",
        "#Reshaping the 'x' data:\n",
        "len_1D = x_train.shape[1]*x_train.shape[2]\n",
        "x_train_1D = np.array( [x.reshape(len_1D) for x in x_train] )\n",
        "x_test_1D  = np.array( [x.reshape(len_1D) for x in x_test] )\n",
        "\n",
        "#Transforming 'y' data - changing scalar i to vector e(i)\n",
        "y_train_1D = np.zeros( (len(y_train), len(classes)) )\n",
        "for i in range(len(y_train)):\n",
        "  y_train_1D[i, y_train[i]] = 1\n",
        "y_test_1D = np.zeros( (len(y_test), len(classes)) )\n",
        "for i in range(len(y_test)):\n",
        "  y_test_1D[i, y_test[i]] = 1\n",
        "\n",
        "y_train_1D = y_train_1D.astype(float)\n",
        "y_test_1D = y_test_1D.astype(float)\n",
        "\n",
        "frac_val = 0.1\n",
        "all_idx = np.arange(len(x_train))\n",
        "val_idx = np.random.choice(all_idx, int(frac_val*len(x_train)), replace=False)\n",
        "tr2_idx = np.array([i for i in all_idx if i not in val_idx])\n",
        "\n",
        "x_train2 = x_train_1D[tr2_idx]\n",
        "y_train2 = y_train_1D[tr2_idx]\n",
        "x_val = x_train_1D[val_idx]\n",
        "y_val = y_train_1D[val_idx]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ-1NbEppk3Q"
      },
      "source": [
        "def fullMNISTHarness(hyp):\n",
        "  wandb.init(project=\"mnist-training-using-top-fmnist-xentropy-runs\")\n",
        "\n",
        "  NN = neuralNetwork(hyp)\n",
        "  NN.train(x_train_1D/(255), y_train_1D, x_test_1D/(255), y_test_1D)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjbICaAatGs9"
      },
      "source": [
        "hyps = []\n",
        "\n",
        "# Best of top 3\n",
        "hyps.append({\n",
        "    \"layerSizes\": [len(x_train_1D[0]),64, 64, 64,len(y_train_1D[0])],\n",
        "    \"batchSize\": 128,\n",
        "    \"learningRate\": 1e-3,\n",
        "    \"epochs\": 25,\n",
        "    \"activations\": [ ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_SOFTMAX ],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-3,  # eta scaling hyperparam\n",
        "    \"regparam\": 0,\n",
        "    \"wandb\":True,\n",
        "})\n",
        "\n",
        "# Second best of top 3\n",
        "hyps.append({\n",
        "    \"layerSizes\": [len(x_train_1D[0]),32, 32, 32, 32,len(y_train_1D[0])],\n",
        "    \"batchSize\": 128,\n",
        "    \"learningRate\": 1e-3,\n",
        "    \"epochs\": 25,\n",
        "    \"activations\": [ ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_SOFTMAX ],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\n",
        "    \"optimizer\": GDOPT_NADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-3,  # eta scaling hyperparam\n",
        "    \"regparam\": 0,\n",
        "    \"wandb\":True,\n",
        "})\n",
        "\n",
        "# Third best of top 3\n",
        "hyps.append({\n",
        "    \"layerSizes\": [len(x_train_1D[0]),32, 32, 32, 32,len(y_train_1D[0])],\n",
        "    \"batchSize\": 128,\n",
        "    \"learningRate\": 1e-3,\n",
        "    \"epochs\": 25,\n",
        "    \"activations\": [ ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_TANH, ACTIVATION_SOFTMAX ],\n",
        "    \"lossFn\": LOSS_CROSSENTROPY,\n",
        "    \"initWeightMethod\": WINIT_XAVIER,\n",
        "    \"optimizer\": GDOPT_ADAM,\n",
        "    \"beta_1\": 0.9,   # momentum scaling hyperparam\n",
        "    \"beta_2\": 0.999, # eta scaling hyperparam\n",
        "    \"epsilon\": 1e-3,  # eta scaling hyperparam\n",
        "    \"regparam\": 0,\n",
        "    \"wandb\":True,\n",
        "})\n",
        "\n",
        "# run top 3 configs\n",
        "for hyp in hyps:\n",
        "  fullMNISTHarness(hyp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}